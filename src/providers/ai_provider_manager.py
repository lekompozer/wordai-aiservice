from typing import List, Dict, AsyncGenerator, Optional
from src.clients.chatgpt_client import ChatGPTClient
from src.clients.gemini_client import GeminiClient
from src.clients.cerebras_client import CerebrasClient
from src.utils.logger import setup_logger
import requests
import json
import asyncio

logger = setup_logger()


class AIProviderManager:
    def __init__(
        self,
        deepseek_api_key: str,
        chatgpt_api_key: str,
        gemini_api_key: str = None,
        cerebras_api_key: str = None,
    ):
        self.deepseek_api_key = deepseek_api_key
        self.chatgpt_api_key = chatgpt_api_key  # Store for later use
        self.gemini_api_key = gemini_api_key  # Store for later use
        self.cerebras_api_key = cerebras_api_key  # Store for later use
        self.deepseek_api_url = "https://api.deepseek.com/v1/chat/completions"

        # Initialize ChatGPT client vá»›i reasoning support
        self.chatgpt_client = (
            ChatGPTClient(chatgpt_api_key) if chatgpt_api_key else None
        )

        # Initialize Gemini client
        self.gemini_client = GeminiClient(gemini_api_key) if gemini_api_key else None

        # Initialize Cerebras client
        self.cerebras_client = (
            CerebrasClient(cerebras_api_key) if cerebras_api_key else None
        )

        self.logger = logger

    async def chat_completion_stream_with_reasoning(
        self,
        messages: List[Dict],
        provider: str = "deepseek",
        use_reasoning: bool = False,
    ) -> AsyncGenerator[str, None]:
        """
        Unified streaming vá»›i reasoning support
        """
        try:
            if provider == "chatgpt" and self.chatgpt_client:
                # ChatGPT vá»›i reasoning
                async for (
                    chunk
                ) in self.chatgpt_client.chat_completion_stream_with_reasoning(
                    messages, use_reasoning
                ):
                    yield chunk
            elif provider == "gemini" and self.gemini_client:
                # Gemini vá»›i reasoning
                async for (
                    chunk
                ) in self.gemini_client.chat_completion_stream_with_reasoning(
                    messages, use_reasoning
                ):
                    yield chunk
            elif provider == "cerebras" and self.cerebras_client:
                # Cerebras vá»›i reasoning
                async for (
                    chunk
                ) in self.cerebras_client.chat_completion_stream_with_reasoning(
                    messages, use_reasoning
                ):
                    yield chunk
            elif provider == "deepseek":
                # DeepSeek vá»›i reasoning prompt enhancement
                if use_reasoning:
                    messages = self._enhance_messages_for_reasoning(messages)

                # Truncate messages cho DeepSeek Ä‘á»ƒ trÃ¡nh vÆ°á»£t quÃ¡ token limit
                messages = self._truncate_messages_for_deepseek(messages)

                async for chunk in self._deepseek_completion_stream_async(messages):
                    yield chunk
            else:
                yield f"Provider {provider} khÃ´ng Ä‘Æ°á»£c há»— trá»£."

        except Exception as e:
            self.logger.error(f"Error in chat_completion_stream_with_reasoning: {e}")
            yield f"Lá»—i {provider}: {str(e)}"

    def _truncate_messages_for_deepseek(self, messages: List[Dict]) -> List[Dict]:
        """Truncate messages Ä‘á»ƒ phÃ¹ há»£p vá»›i DeepSeek token limit"""
        # Æ¯á»›c tÃ­nh: 1 token â‰ˆ 4 characters (tiáº¿ng Viá»‡t)
        MAX_TOKENS = 50000  # Äá»ƒ láº¡i buffer cho completion
        MAX_CHARS = MAX_TOKENS * 4

        truncated_messages = []
        total_chars = 0

        for msg in messages:
            content = msg.get("content", "")

            if msg["role"] == "system":
                # RÃºt gá»n system prompt nhÆ°ng giá»¯ thÃ´ng tin quan trá»ng
                if len(content) > 2000:
                    content = (
                        content[:2000]
                        + "\n\nHÃ£y phÃ¢n tÃ­ch báº¥t Ä‘á»™ng sáº£n vÃ  Ä‘Æ°a ra Ä‘á»‹nh giÃ¡ cÃ¹ng kháº£ nÄƒng tháº¿ cháº¥p."
                    )

            elif msg["role"] == "user":
                # RÃºt gá»n user content náº¿u quÃ¡ dÃ i
                if len(content) > MAX_CHARS - total_chars - 1000:  # Äá»ƒ láº¡i buffer
                    available_chars = MAX_CHARS - total_chars - 1000
                    if available_chars > 1000:
                        content = (
                            content[:available_chars]
                            + "\n\n... (ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c rÃºt gá»n)"
                        )
                    else:
                        # Náº¿u khÃ´ng Ä‘á»§ chá»—, chá»‰ giá»¯ pháº§n Ä‘áº§u quan trá»ng
                        content = content[:1000] + "\n\nHÃ£y Ä‘á»‹nh giÃ¡ báº¥t Ä‘á»™ng sáº£n nÃ y."

            total_chars += len(content)
            truncated_messages.append({"role": msg["role"], "content": content})

            # Break náº¿u Ä‘Ã£ gáº§n Ä‘áº¿n giá»›i háº¡n
            if total_chars > MAX_CHARS - 1000:
                break

        self.logger.info(f"DeepSeek messages truncated - Total chars: {total_chars}")
        return truncated_messages

    async def loan_assessment_completion_non_stream(
        self, messages: List[Dict], provider: str = "deepseek"
    ) -> str:
        """
        âœ… DEDICATED non-streaming method for loan assessment - Returns complete response
        """
        try:
            self.logger.info(
                f"ðŸ¦ Loan Assessment: Starting {provider} call with {len(messages)} messages"
            )

            if provider == "chatgpt" and self.chatgpt_client:
                # ChatGPT non-streaming vá»›i loan assessment enhancement
                enhanced_messages = self._enhance_messages_for_loan_assessment(messages)
                return await self.chatgpt_client.chat_completion(enhanced_messages)
            elif provider == "cerebras" and self.cerebras_client:
                # Cerebras non-streaming vá»›i loan assessment enhancement
                enhanced_messages = self._enhance_messages_for_loan_assessment(messages)
                return await self.cerebras_client.chat_completion(
                    enhanced_messages,
                    temperature=0.1,  # Low temperature for consistent assessment
                    max_tokens=3000,
                )
            elif provider == "deepseek":
                # âœ… ENHANCE MESSAGES FOR LOAN ASSESSMENT
                enhanced_messages = self._enhance_messages_for_loan_assessment(messages)

                # âœ… TRUNCATE FOR DEEPSEEK
                truncated_messages = self._truncate_messages_for_deepseek(
                    enhanced_messages
                )

                self.logger.info(
                    f"ðŸ¦ Loan Assessment: Messages prepared - Enhanced: {len(enhanced_messages)}, Truncated: {len(truncated_messages)}"
                )

                # âœ… CALL DEEPSEEK NON-STREAMING
                validated_messages = self._validate_deepseek_messages(
                    truncated_messages
                )

                headers = {
                    "Authorization": f"Bearer {self.deepseek_api_key}",
                    "Content-Type": "application/json",
                }

                payload = {
                    "model": "deepseek-chat",
                    "messages": validated_messages,
                    "temperature": 0.1,  # Low temperature for consistent loan assessment
                    "max_tokens": 3000,  # Enough for detailed assessment JSON
                    "stream": False,  # Non-streaming for complete response
                }

                self.logger.info(f"ðŸ¦ Loan Assessment: Sending request to DeepSeek")

                # âœ… SYNCHRONOUS CALL
                import requests

                response = requests.post(
                    self.deepseek_api_url, headers=headers, json=payload, timeout=120
                )

                if response.status_code != 200:
                    error_text = response.text
                    self.logger.error(
                        f"ðŸ¦ Loan Assessment: DeepSeek API error {response.status_code}: {error_text}"
                    )
                    raise Exception(
                        f"DeepSeek API error {response.status_code}: {error_text}"
                    )

                response.raise_for_status()
                result = response.json()

                # âœ… EXTRACT CONTENT
                if "choices" in result and len(result["choices"]) > 0:
                    content = result["choices"][0]["message"]["content"]
                    self.logger.info(
                        f"ðŸ¦ Loan Assessment: DeepSeek response received - {len(content)} characters"
                    )
                    return content
                else:
                    raise Exception("No content in DeepSeek response")
            else:
                raise Exception(
                    f"Provider {provider} khÃ´ng Ä‘Æ°á»£c há»— trá»£ cho loan assessment"
                )

        except Exception as e:
            self.logger.error(
                f"ðŸ¦ Loan Assessment: Error in loan_assessment_completion_non_stream - {e}"
            )
            raise e

    def _enhance_messages_for_loan_assessment(self, messages: List[Dict]) -> List[Dict]:
        """
        âœ… Enhance messages cho DeepSeek loan assessment mode - VRB Bank Standards
        """
        enhanced_messages = []

        for msg in messages:
            if msg["role"] == "system":
                # âœ… COMPACT LOAN ASSESSMENT REASONING PROMPT
                enhanced_content = (
                    msg["content"]
                    + """

    ðŸ¦ **PHÆ¯Æ NG PHÃP THáº¨M Äá»ŠNH TÃN Dá»¤NG VRB:**

    **BÆ¯á»šC 1 - PHÃ‚N TÃCH KHáº¢ NÄ‚NG TÃ€I CHÃNH:**
    - DTI (Debt-to-Income): Tá»· lá»‡ ná»£/thu nháº­p â‰¤ 50%
    - DÃ²ng tiá»n hÃ ng thÃ¡ng sau tráº£ ná»£ â‰¥ 15 triá»‡u VNÄ
    - Thu nháº­p á»•n Ä‘á»‹nh â‰¥ 12 thÃ¡ng

    **BÆ¯á»šC 2 - ÄÃNH GIÃ TÃ€I Sáº¢N Äáº¢M Báº¢O:**
    - LTV (Loan-to-Value): Tá»· lá»‡ cho vay/giÃ¡ trá»‹ tÃ i sáº£n â‰¤ 80%
    - Äá»‹nh giÃ¡ tÃ i sáº£n theo thá»‹ trÆ°á»ng hiá»‡n táº¡i
    - TÃ­nh thanh khoáº£n vÃ  rá»§i ro phÃ¡p lÃ½

    **BÆ¯á»šC 3 - Rá»¦I RO TÃN Dá»¤NG:**
    - CIC Score: NhÃ³m 1-2 (tá»‘t), NhÃ³m 3+ (cáº£nh bÃ¡o)
    - Lá»‹ch sá»­ tráº£ ná»£ vÃ  cam káº¿t tÃ i chÃ­nh
    - Äá»™ á»•n Ä‘á»‹nh cÃ´ng viá»‡c vÃ  thu nháº­p

    **BÆ¯á»šC 4 - QUYáº¾T Äá»ŠNH PHÃ‚N LOáº I:**
    - APPROVED: DTI â‰¤ 40%, LTV â‰¤ 70%, CIC NhÃ³m 1-2
    - NEEDS_REVIEW: DTI 40-50%, LTV 70-80%, cáº§n bá»• sung
    - REJECTED: DTI > 50%, LTV > 80%, CIC NhÃ³m 3+

    **BÆ¯á»šC 5 - ÄIá»€U KIá»†N & KHUYáº¾N NGHá»Š:**
    - LÃ£i suáº¥t dá»±a trÃªn profile rá»§i ro
    - Äiá»u kiá»‡n bá»• sung (náº¿u cáº§n)
    - Timeline vÃ  yÃªu cáº§u phÃ¡p lÃ½

    ðŸ“Š **YÃŠU Cáº¦U JSON RESPONSE:**
    Pháº£i cÃ³ Ä‘áº§y Ä‘á»§: status, confidence, creditScore, reasoning (â‰¥200 tá»«),
    riskFactors, recommendations, approvedAmount, interestRate, conditions.

    ðŸŽ¯ **TRÃŒNH BÃ€Y:** Logic tá»«ng bÆ°á»›c, cÄƒn cá»© sá»‘ liá»‡u, quyáº¿t Ä‘á»‹nh rÃµ rÃ ng.
    """
                )
                enhanced_messages.append(
                    {"role": "system", "content": enhanced_content}
                )
            else:
                enhanced_messages.append(msg)

        return enhanced_messages

    def _enhance_messages_for_reasoning(self, messages: List[Dict]) -> List[Dict]:
        """Enhance messages cho DeepSeek reasoning mode - COMPACT VERSION"""
        enhanced_messages = []

        for msg in messages:
            if msg["role"] == "system":
                # Compact reasoning prompt
                enhanced_content = (
                    msg["content"]
                    + """

                PHÆ¯Æ NG PHÃP REASONING:
                1. PhÃ¢n tÃ­ch thÃ´ng tin: diá»‡n tÃ­ch, vá»‹ trÃ­, phÃ¡p lÃ½
                2. Äá»‹nh giÃ¡ thá»‹ trÆ°á»ng 2025
                3. TÃ­nh kháº£ nÄƒng tháº¿ cháº¥p (LTV 70-80%)
                4. ÄÃ¡nh giÃ¡ rá»§i ro
                5. Khuyáº¿n nghá»‹ cá»¥ thá»ƒ

                TrÃ¬nh bÃ y tá»«ng bÆ°á»›c vá»›i sá»‘ liá»‡u cá»¥ thá»ƒ.
                """
                )
                enhanced_messages.append(
                    {"role": "system", "content": enhanced_content}
                )
            else:
                enhanced_messages.append(msg)

        return enhanced_messages

    def _validate_deepseek_messages(self, messages: List[Dict]) -> List[Dict]:
        """Validate vÃ  clean messages cho DeepSeek API"""
        validated_messages = []

        for msg in messages:
            # Äáº£m báº£o role há»£p lá»‡
            if msg["role"] not in ["system", "user", "assistant"]:
                continue

            # Äáº£m báº£o content khÃ´ng None vÃ  cÃ³ dáº¡ng string cho DeepSeek
            content = msg.get("content", "")
            if isinstance(content, list):
                # DeepSeek khÃ´ng há»— trá»£ multimodal, extract text only
                text_content = ""
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_content += item.get("text", "")
                content = text_content

            if not content or not isinstance(content, str):
                content = "Ná»™i dung trá»‘ng"

            validated_messages.append({"role": msg["role"], "content": content})

        return validated_messages

    async def _deepseek_completion_stream_async(
        self, messages: List[Dict]
    ) -> AsyncGenerator[str, None]:
        """DeepSeek streaming async version"""

        def sync_generator():
            yield from self._deepseek_completion_stream_sync(messages)

        loop = asyncio.get_event_loop()
        generator = await loop.run_in_executor(None, lambda: list(sync_generator()))
        for chunk in generator:
            yield chunk

    def _deepseek_completion_stream_sync(self, messages: List[Dict]):
        """DeepSeek streaming completion - SYNC generator"""
        # Validate messages trÆ°á»›c khi gá»­i
        validated_messages = self._validate_deepseek_messages(messages)

        headers = {
            "Authorization": f"Bearer {self.deepseek_api_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": "deepseek-chat",
            "messages": validated_messages,
            "temperature": 0.2,
            "max_tokens": 2000,  # GIáº¢M Tá»ª 4000 XUá»NG 2000
            "stream": True,
        }

        try:
            self.logger.info(f"DeepSeek request - Messages: {len(validated_messages)}")

            with requests.post(
                self.deepseek_api_url,
                headers=headers,
                json=payload,
                timeout=120,
                stream=True,
            ) as response:

                if response.status_code != 200:
                    error_text = response.text
                    self.logger.error(
                        f"DeepSeek API error {response.status_code}: {error_text}"
                    )
                    yield f"DeepSeek API Error: {error_text}"
                    return

                response.raise_for_status()

                for line in response.iter_lines():
                    if line:
                        line = line.decode("utf-8")
                        if line.startswith("data: "):
                            line = line[6:]
                            if line == "[DONE]":
                                break
                            try:
                                chunk_data = json.loads(line)
                                if (
                                    "choices" in chunk_data
                                    and len(chunk_data["choices"]) > 0
                                ):
                                    delta = chunk_data["choices"][0].get("delta", {})
                                    if "content" in delta:
                                        yield delta["content"]
                            except json.JSONDecodeError:
                                continue

        except requests.exceptions.RequestException as e:
            self.logger.error(f"DeepSeek streaming error: {e}")
            yield "Xin lá»—i, tÃ´i Ä‘ang gáº·p sá»± cá»‘ khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n."
        except Exception as e:
            self.logger.error(f"DeepSeek unexpected error: {e}")
            yield "Xin lá»—i, tÃ´i Ä‘ang gáº·p sá»± cá»‘ khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n."

    # THÃŠM CÃC METHODS CÅ¨ Äá»‚ BACKWARD COMPATIBILITY
    async def chat_completion(
        self, messages: List[Dict], provider: str = "deepseek"
    ) -> str:
        """Non-streaming chat completion - USES PROVEN STREAMING METHOD"""
        try:
            if provider == "chatgpt" and self.chatgpt_client:
                return await self.chatgpt_client.chat_completion(messages)
            elif provider == "gemini" and self.gemini_client:
                return await self.gemini_client.chat_completion(messages)
            else:
                # Use the proven streaming method that works well in other routes
                # Collect all chunks into a single response
                chunks = []
                async for chunk in self.chat_completion_stream_with_reasoning(
                    messages, provider, use_reasoning=False
                ):
                    chunks.append(chunk)

                raw_response = "".join(chunks)
                logger.info(
                    f"âœ… {provider} chat_completion: Response length: {len(raw_response)} chars"
                )
                return raw_response
        except Exception as e:
            logger.error(f"âŒ Error in chat_completion: {e}")
            raise e

    async def chat_completion_stream(
        self, messages: List[Dict], provider: str = "deepseek"
    ) -> AsyncGenerator[str, None]:
        """Original streaming method - backward compatibility"""
        async for chunk in self.chat_completion_stream_with_reasoning(
            messages, provider, use_reasoning=False
        ):
            yield chunk

    def _deepseek_completion_sync(self, messages: List[Dict]) -> str:
        """
        Synchronous DeepSeek completion
        """
        headers = {
            "Authorization": f"Bearer {self.deepseek_api_key}",
            "Content-Type": "application/json",
        }

        data = {
            "model": "deepseek-chat",
            "messages": messages,
            "temperature": 0.3,
            "max_tokens": 4000,
            "stream": False,
        }

        try:
            response = requests.post(
                self.deepseek_api_url, headers=headers, json=data, timeout=60
            )

            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                return f"DeepSeek API Error: {response.status_code} - {response.text}"

        except Exception as e:
            return f"DeepSeek request failed: {str(e)}"

    async def _deepseek_completion_async(self, messages: List[Dict]) -> str:
        """
        Asynchronous DeepSeek completion (non-streaming) - ENHANCED DEBUG VERSION
        """
        import aiohttp

        headers = {
            "Authorization": f"Bearer {self.deepseek_api_key}",
            "Content-Type": "application/json",
        }

        data = {
            "model": "deepseek-chat",
            "messages": messages,
            "temperature": 0.1,  # Lower temperature for more consistent results
            "max_tokens": 2000,  # Reduced from 4000 to prevent timeout
            "stream": False,
        }

        self.logger.info(
            f"DeepSeek async call - {len(messages)} messages, total chars: {sum(len(str(m.get('content', ''))) for m in messages)}"
        )

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.deepseek_api_url,
                    headers=headers,
                    json=data,
                    timeout=aiohttp.ClientTimeout(
                        total=120
                    ),  # Increased timeout to 120s
                ) as response:
                    self.logger.info(f"DeepSeek response status: {response.status}")

                    if response.status == 200:
                        result = await response.json()
                        content = result["choices"][0]["message"]["content"]
                        self.logger.info(
                            f"DeepSeek success - response length: {len(content)} chars"
                        )
                        return content
                    else:
                        error_text = await response.text()
                        self.logger.error(
                            f"DeepSeek API Error: {response.status} - {error_text}"
                        )
                        return f"DeepSeek API Error: {response.status} - {error_text}"

        except Exception as e:
            self.logger.error(f"DeepSeek async exception: {str(e)}")
            import traceback

            self.logger.error(f"DeepSeek traceback: {traceback.format_exc()}")
            return f"DeepSeek request failed: {str(e)}"

    # ===== ADDITIONAL METHODS FOR API COMPATIBILITY =====

    async def get_available_providers(self) -> List[str]:
        """
        âœ… Get list of available AI providers
        """
        providers = ["deepseek"]
        if self.chatgpt_client:
            providers.append("chatgpt")
        if self.gemini_client:
            providers.append("gemini")
        if self.cerebras_client:
            providers.append("cerebras")
        return providers

    def get_current_provider(self) -> str:
        """
        âœ… Get current default provider
        """
        return "deepseek"  # Default provider

    async def get_response(
        self,
        question: str,
        user_id: Optional[str] = None,
        provider: str = "deepseek",
    ) -> str:
        """
        âœ… Get single response from AI provider (non-streaming)
        """
        try:
            messages = [{"role": "user", "content": question}]

            if provider == "chatgpt" and self.chatgpt_client:
                return await self.chatgpt_client.get_completion(messages)
            elif provider == "cerebras" and self.cerebras_client:
                return await self.cerebras_client.chat_completion(messages)
            elif provider == "gemini" and self.gemini_client:
                # Note: Gemini client would need a non-streaming method
                return await self.gemini_client.get_completion(messages)
            else:
                # Use DeepSeek
                return await self._deepseek_completion_async(messages)

        except Exception as e:
            self.logger.error(f"Error in get_response: {e}")
            return f"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra: {str(e)}"

    async def stream_response(
        self,
        question: str,
        user_id: Optional[str] = None,
        provider: str = "deepseek",
    ) -> AsyncGenerator[str, None]:
        """
        âœ… Stream response from AI provider
        """
        try:
            messages = [{"role": "user", "content": question}]

            async for chunk in self.chat_completion_stream_with_reasoning(
                messages, provider
            ):
                yield chunk

        except Exception as e:
            self.logger.error(f"Error in stream_response: {e}")
            yield f"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra: {str(e)}"

    async def clear_history(self, user_id: Optional[str] = None):
        """
        âœ… Clear chat history for user (simplified without session_id)
        """
        self.logger.info(f"History cleared for user: {user_id or 'anonymous'}")
        # TODO: Implement actual history storage and clearing based on user_id
        return True

    # ===== END ADDITIONAL METHODS =====

    async def chat_with_file_stream(
        self,
        messages: List[Dict],
        file_content: bytes,
        file_name: str,
        provider: str = "gemini",
        temperature: float = 0.7,
    ) -> AsyncGenerator[str, None]:
        """
        Stream chat with file upload support.
        Currently only supports Gemini provider.
        """
        try:
            if provider == "gemini" and self.gemini_client:
                async for chunk in self.gemini_client.chat_with_file_stream(
                    messages, file_content, file_name, temperature
                ):
                    yield chunk
            elif provider == "chatgpt" and self.chatgpt_client:
                # ChatGPT doesn't have direct file upload, could implement URL-based approach
                yield f"ChatGPT file upload chÆ°a Ä‘Æ°á»£c há»— trá»£ trong phÆ°Æ¡ng thá»©c nÃ y."
            else:
                yield f"Provider {provider} khÃ´ng há»— trá»£ chat vá»›i file upload."

        except Exception as e:
            self.logger.error(f"âŒ Error in chat_with_file_stream: {e}")
            yield f"Lá»—i {provider} file processing: {str(e)}"

    async def generate_embedding(self, text: str) -> List[float]:
        """
        Generate embedding vector for text using sentence-transformers (768 dimensions)
        Táº¡o vector embedding cho text sá»­ dá»¥ng sentence-transformers (768 dimensions)
        """
        try:
            # Import EmbeddingService to use 768-dimension model
            from src.services.embedding_service import get_embedding_service

            # Use singleton embedding service instance
            embedding_service = get_embedding_service()

            # Generate embedding using sentence-transformers
            embedding = await embedding_service.generate_embedding(text)

            self.logger.info(f"âœ… Generated embedding with {len(embedding)} dimensions")
            return embedding

        except Exception as e:
            self.logger.error(f"Failed to generate embedding: {e}")
            return []

    async def generate_embeddings_batch(
        self, texts: List[str], max_batch_size: int = 20, timeout_seconds: int = 300
    ) -> List[List[float]]:
        """
        Generate embedding vectors for multiple texts efficiently using batch processing
        Táº¡o vector embedding cho nhiá»u text hiá»‡u quáº£ sá»­ dá»¥ng batch processing
        """
        try:
            # Import EmbeddingService to use 768-dimension model
            from src.services.embedding_service import get_embedding_service

            # Use singleton embedding service instance
            embedding_service = get_embedding_service()

            # Generate embeddings using batch processing with timeout and size limits
            embeddings = await embedding_service.generate_embeddings_batch(
                texts=texts,
                max_batch_size=max_batch_size,
                timeout_seconds=timeout_seconds,
            )

            self.logger.info(f"âœ… Generated {len(embeddings)} embeddings in batch mode")
            return embeddings

        except Exception as e:
            self.logger.error(f"Failed to generate batch embeddings: {e}")
            # Return zero vectors as fallback
            return [[0.0] * 768] * len(texts)

    async def stream_chat_completion(
        self,
        messages: List[Dict],
        provider: str = "deepseek",
        use_reasoning: bool = False,
    ) -> AsyncGenerator[str, None]:
        """
        Alias method for backward compatibility.
        Delegates to chat_completion_stream_with_reasoning.
        """
        async for chunk in self.chat_completion_stream_with_reasoning(
            messages=messages, provider=provider, use_reasoning=use_reasoning
        ):
            yield chunk
