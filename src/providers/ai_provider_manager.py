from typing import List, Dict, AsyncGenerator, Optional
from src.clients.chatgpt_client import ChatGPTClient
from src.clients.gemini_client import GeminiClient
from src.clients.cerebras_client import CerebrasClient
from src.utils.logger import setup_logger
import requests
import json
import asyncio

logger = setup_logger()


class AIProviderManager:
    def __init__(
        self,
        deepseek_api_key: str,
        chatgpt_api_key: str,
        gemini_api_key: str = None,
        cerebras_api_key: str = None,
    ):
        self.deepseek_api_key = deepseek_api_key
        self.chatgpt_api_key = chatgpt_api_key  # Store for later use
        self.gemini_api_key = gemini_api_key  # Store for later use
        self.cerebras_api_key = cerebras_api_key  # Store for later use
        self.deepseek_api_url = "https://api.deepseek.com/v1/chat/completions"

        # Initialize ChatGPT client v·ªõi reasoning support
        self.chatgpt_client = (
            ChatGPTClient(chatgpt_api_key) if chatgpt_api_key else None
        )

        # Initialize Gemini client
        self.gemini_client = GeminiClient(gemini_api_key) if gemini_api_key else None

        # Initialize Cerebras client
        self.cerebras_client = (
            CerebrasClient(cerebras_api_key) if cerebras_api_key else None
        )

        self.logger = logger

    async def chat_completion_stream_with_reasoning(
        self,
        messages: List[Dict],
        provider: str = "deepseek",
        use_reasoning: bool = False,
    ) -> AsyncGenerator[str, None]:
        """
        Unified streaming v·ªõi reasoning support
        """
        try:
            if provider == "chatgpt" and self.chatgpt_client:
                # ChatGPT v·ªõi reasoning
                async for (
                    chunk
                ) in self.chatgpt_client.chat_completion_stream_with_reasoning(
                    messages, use_reasoning
                ):
                    yield chunk
            elif provider == "gemini" and self.gemini_client:
                # Gemini v·ªõi reasoning
                async for (
                    chunk
                ) in self.gemini_client.chat_completion_stream_with_reasoning(
                    messages, use_reasoning
                ):
                    yield chunk
            elif provider == "cerebras" and self.cerebras_client:
                # Cerebras v·ªõi reasoning
                async for (
                    chunk
                ) in self.cerebras_client.chat_completion_stream_with_reasoning(
                    messages, use_reasoning
                ):
                    yield chunk
            elif provider == "deepseek":
                # DeepSeek v·ªõi reasoning prompt enhancement
                if use_reasoning:
                    messages = self._enhance_messages_for_reasoning(messages)

                # Truncate messages cho DeepSeek ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° token limit
                messages = self._truncate_messages_for_deepseek(messages)

                async for chunk in self._deepseek_completion_stream_async(messages):
                    yield chunk
            else:
                yield f"Provider {provider} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£."

        except Exception as e:
            self.logger.error(f"Error in chat_completion_stream_with_reasoning: {e}")
            yield f"L·ªói {provider}: {str(e)}"

    def _truncate_messages_for_deepseek(self, messages: List[Dict]) -> List[Dict]:
        """Truncate messages ƒë·ªÉ ph√π h·ª£p v·ªõi DeepSeek token limit"""
        # ∆Ø·ªõc t√≠nh: 1 token ‚âà 4 characters (ti·∫øng Vi·ªát)
        MAX_TOKENS = 50000  # ƒê·ªÉ l·∫°i buffer cho completion
        MAX_CHARS = MAX_TOKENS * 4

        truncated_messages = []
        total_chars = 0

        for msg in messages:
            content = msg.get("content", "")

            if msg["role"] == "system":
                # R√∫t g·ªçn system prompt nh∆∞ng gi·ªØ th√¥ng tin quan tr·ªçng
                if len(content) > 2000:
                    content = (
                        content[:2000]
                        + "\n\nH√£y ph√¢n t√≠ch b·∫•t ƒë·ªông s·∫£n v√† ƒë∆∞a ra ƒë·ªãnh gi√° c√πng kh·∫£ nƒÉng th·∫ø ch·∫•p."
                    )

            elif msg["role"] == "user":
                # R√∫t g·ªçn user content n·∫øu qu√° d√†i
                if len(content) > MAX_CHARS - total_chars - 1000:  # ƒê·ªÉ l·∫°i buffer
                    available_chars = MAX_CHARS - total_chars - 1000
                    if available_chars > 1000:
                        content = (
                            content[:available_chars]
                            + "\n\n... (n·ªôi dung ƒë√£ ƒë∆∞·ª£c r√∫t g·ªçn)"
                        )
                    else:
                        # N·∫øu kh√¥ng ƒë·ªß ch·ªó, ch·ªâ gi·ªØ ph·∫ßn ƒë·∫ßu quan tr·ªçng
                        content = content[:1000] + "\n\nH√£y ƒë·ªãnh gi√° b·∫•t ƒë·ªông s·∫£n n√†y."

            total_chars += len(content)
            truncated_messages.append({"role": msg["role"], "content": content})

            # Break n·∫øu ƒë√£ g·∫ßn ƒë·∫øn gi·ªõi h·∫°n
            if total_chars > MAX_CHARS - 1000:
                break

        self.logger.info(f"DeepSeek messages truncated - Total chars: {total_chars}")
        return truncated_messages

    async def loan_assessment_completion_non_stream(
        self, messages: List[Dict], provider: str = "deepseek"
    ) -> str:
        """
        ‚úÖ DEDICATED non-streaming method for loan assessment - Returns complete response
        """
        try:
            self.logger.info(
                f"üè¶ Loan Assessment: Starting {provider} call with {len(messages)} messages"
            )

            if provider == "chatgpt" and self.chatgpt_client:
                # ChatGPT non-streaming v·ªõi loan assessment enhancement
                enhanced_messages = self._enhance_messages_for_loan_assessment(messages)
                return await self.chatgpt_client.chat_completion(enhanced_messages)
            elif provider == "cerebras" and self.cerebras_client:
                # Cerebras non-streaming v·ªõi loan assessment enhancement
                enhanced_messages = self._enhance_messages_for_loan_assessment(messages)
                return await self.cerebras_client.chat_completion(
                    enhanced_messages,
                    temperature=0.1,  # Low temperature for consistent assessment
                    max_tokens=3000,
                )
            elif provider == "deepseek":
                # ‚úÖ ENHANCE MESSAGES FOR LOAN ASSESSMENT
                enhanced_messages = self._enhance_messages_for_loan_assessment(messages)

                # ‚úÖ TRUNCATE FOR DEEPSEEK
                truncated_messages = self._truncate_messages_for_deepseek(
                    enhanced_messages
                )

                self.logger.info(
                    f"üè¶ Loan Assessment: Messages prepared - Enhanced: {len(enhanced_messages)}, Truncated: {len(truncated_messages)}"
                )

                # ‚úÖ CALL DEEPSEEK NON-STREAMING
                validated_messages = self._validate_deepseek_messages(
                    truncated_messages
                )

                headers = {
                    "Authorization": f"Bearer {self.deepseek_api_key}",
                    "Content-Type": "application/json",
                }

                payload = {
                    "model": "deepseek-chat",
                    "messages": validated_messages,
                    "temperature": 0.1,  # Low temperature for consistent loan assessment
                    "max_tokens": 3000,  # Enough for detailed assessment JSON
                    "stream": False,  # Non-streaming for complete response
                }

                self.logger.info(f"üè¶ Loan Assessment: Sending request to DeepSeek")

                # ‚úÖ SYNCHRONOUS CALL
                import requests

                response = requests.post(
                    self.deepseek_api_url, headers=headers, json=payload, timeout=120
                )

                if response.status_code != 200:
                    error_text = response.text
                    self.logger.error(
                        f"üè¶ Loan Assessment: DeepSeek API error {response.status_code}: {error_text}"
                    )
                    raise Exception(
                        f"DeepSeek API error {response.status_code}: {error_text}"
                    )

                response.raise_for_status()
                result = response.json()

                # ‚úÖ EXTRACT CONTENT
                if "choices" in result and len(result["choices"]) > 0:
                    content = result["choices"][0]["message"]["content"]
                    self.logger.info(
                        f"üè¶ Loan Assessment: DeepSeek response received - {len(content)} characters"
                    )
                    return content
                else:
                    raise Exception("No content in DeepSeek response")
            else:
                raise Exception(
                    f"Provider {provider} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ cho loan assessment"
                )

        except Exception as e:
            self.logger.error(
                f"üè¶ Loan Assessment: Error in loan_assessment_completion_non_stream - {e}"
            )
            raise e

    def _enhance_messages_for_loan_assessment(self, messages: List[Dict]) -> List[Dict]:
        """
        ‚úÖ Enhance messages cho DeepSeek loan assessment mode - VRB Bank Standards
        """
        enhanced_messages = []

        for msg in messages:
            if msg["role"] == "system":
                # ‚úÖ COMPACT LOAN ASSESSMENT REASONING PROMPT
                enhanced_content = (
                    msg["content"]
                    + """

    üè¶ **PH∆Ø∆†NG PH√ÅP TH·∫®M ƒê·ªäNH T√çN D·ª§NG VRB:**

    **B∆Ø·ªöC 1 - PH√ÇN T√çCH KH·∫¢ NƒÇNG T√ÄI CH√çNH:**
    - DTI (Debt-to-Income): T·ª∑ l·ªá n·ª£/thu nh·∫≠p ‚â§ 50%
    - D√≤ng ti·ªÅn h√†ng th√°ng sau tr·∫£ n·ª£ ‚â• 15 tri·ªáu VNƒê
    - Thu nh·∫≠p ·ªïn ƒë·ªãnh ‚â• 12 th√°ng

    **B∆Ø·ªöC 2 - ƒê√ÅNH GI√Å T√ÄI S·∫¢N ƒê·∫¢M B·∫¢O:**
    - LTV (Loan-to-Value): T·ª∑ l·ªá cho vay/gi√° tr·ªã t√†i s·∫£n ‚â§ 80%
    - ƒê·ªãnh gi√° t√†i s·∫£n theo th·ªã tr∆∞·ªùng hi·ªán t·∫°i
    - T√≠nh thanh kho·∫£n v√† r·ªßi ro ph√°p l√Ω

    **B∆Ø·ªöC 3 - R·ª¶I RO T√çN D·ª§NG:**
    - CIC Score: Nh√≥m 1-2 (t·ªët), Nh√≥m 3+ (c·∫£nh b√°o)
    - L·ªãch s·ª≠ tr·∫£ n·ª£ v√† cam k·∫øt t√†i ch√≠nh
    - ƒê·ªô ·ªïn ƒë·ªãnh c√¥ng vi·ªác v√† thu nh·∫≠p

    **B∆Ø·ªöC 4 - QUY·∫æT ƒê·ªäNH PH√ÇN LO·∫†I:**
    - APPROVED: DTI ‚â§ 40%, LTV ‚â§ 70%, CIC Nh√≥m 1-2
    - NEEDS_REVIEW: DTI 40-50%, LTV 70-80%, c·∫ßn b·ªï sung
    - REJECTED: DTI > 50%, LTV > 80%, CIC Nh√≥m 3+

    **B∆Ø·ªöC 5 - ƒêI·ªÄU KI·ªÜN & KHUY·∫æN NGH·ªä:**
    - L√£i su·∫•t d·ª±a tr√™n profile r·ªßi ro
    - ƒêi·ªÅu ki·ªán b·ªï sung (n·∫øu c·∫ßn)
    - Timeline v√† y√™u c·∫ßu ph√°p l√Ω

    üìä **Y√äU C·∫¶U JSON RESPONSE:**
    Ph·∫£i c√≥ ƒë·∫ßy ƒë·ªß: status, confidence, creditScore, reasoning (‚â•200 t·ª´),
    riskFactors, recommendations, approvedAmount, interestRate, conditions.

    üéØ **TR√åNH B√ÄY:** Logic t·ª´ng b∆∞·ªõc, cƒÉn c·ª© s·ªë li·ªáu, quy·∫øt ƒë·ªãnh r√µ r√†ng.
    """
                )
                enhanced_messages.append(
                    {"role": "system", "content": enhanced_content}
                )
            else:
                enhanced_messages.append(msg)

        return enhanced_messages

    def _enhance_messages_for_reasoning(self, messages: List[Dict]) -> List[Dict]:
        """Enhance messages cho DeepSeek reasoning mode - COMPACT VERSION"""
        enhanced_messages = []

        for msg in messages:
            if msg["role"] == "system":
                # Compact reasoning prompt
                enhanced_content = (
                    msg["content"]
                    + """

                PH∆Ø∆†NG PH√ÅP REASONING:
                1. Ph√¢n t√≠ch th√¥ng tin: di·ªán t√≠ch, v·ªã tr√≠, ph√°p l√Ω
                2. ƒê·ªãnh gi√° th·ªã tr∆∞·ªùng 2025
                3. T√≠nh kh·∫£ nƒÉng th·∫ø ch·∫•p (LTV 70-80%)
                4. ƒê√°nh gi√° r·ªßi ro
                5. Khuy·∫øn ngh·ªã c·ª• th·ªÉ

                Tr√¨nh b√†y t·ª´ng b∆∞·ªõc v·ªõi s·ªë li·ªáu c·ª• th·ªÉ.
                """
                )
                enhanced_messages.append(
                    {"role": "system", "content": enhanced_content}
                )
            else:
                enhanced_messages.append(msg)

        return enhanced_messages

    def _validate_deepseek_messages(self, messages: List[Dict]) -> List[Dict]:
        """Validate v√† clean messages cho DeepSeek API"""
        validated_messages = []

        for msg in messages:
            # ƒê·∫£m b·∫£o role h·ª£p l·ªá
            if msg["role"] not in ["system", "user", "assistant"]:
                continue

            # ƒê·∫£m b·∫£o content kh√¥ng None v√† c√≥ d·∫°ng string cho DeepSeek
            content = msg.get("content", "")
            if isinstance(content, list):
                # DeepSeek kh√¥ng h·ªó tr·ª£ multimodal, extract text only
                text_content = ""
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_content += item.get("text", "")
                content = text_content

            if not content or not isinstance(content, str):
                content = "N·ªôi dung tr·ªëng"

            validated_messages.append({"role": msg["role"], "content": content})

        return validated_messages

    async def _deepseek_completion_stream_async(
        self, messages: List[Dict]
    ) -> AsyncGenerator[str, None]:
        """DeepSeek streaming async version"""

        def sync_generator():
            yield from self._deepseek_completion_stream_sync(messages)

        loop = asyncio.get_event_loop()
        generator = await loop.run_in_executor(None, lambda: list(sync_generator()))
        for chunk in generator:
            yield chunk

    def _deepseek_completion_stream_sync(self, messages: List[Dict]):
        """DeepSeek streaming completion - SYNC generator"""
        # Validate messages tr∆∞·ªõc khi g·ª≠i
        validated_messages = self._validate_deepseek_messages(messages)

        headers = {
            "Authorization": f"Bearer {self.deepseek_api_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": "deepseek-chat",
            "messages": validated_messages,
            "temperature": 0.2,
            "max_tokens": 2000,  # GI·∫¢M T·ª™ 4000 XU·ªêNG 2000
            "stream": True,
        }

        try:
            self.logger.info(f"DeepSeek request - Messages: {len(validated_messages)}")

            with requests.post(
                self.deepseek_api_url,
                headers=headers,
                json=payload,
                timeout=120,
                stream=True,
            ) as response:

                if response.status_code != 200:
                    error_text = response.text
                    self.logger.error(
                        f"DeepSeek API error {response.status_code}: {error_text}"
                    )
                    yield f"DeepSeek API Error: {error_text}"
                    return

                response.raise_for_status()

                for line in response.iter_lines():
                    if line:
                        line = line.decode("utf-8")
                        if line.startswith("data: "):
                            line = line[6:]
                            if line == "[DONE]":
                                break
                            try:
                                chunk_data = json.loads(line)
                                if (
                                    "choices" in chunk_data
                                    and len(chunk_data["choices"]) > 0
                                ):
                                    delta = chunk_data["choices"][0].get("delta", {})
                                    if "content" in delta:
                                        yield delta["content"]
                            except json.JSONDecodeError:
                                continue

        except requests.exceptions.RequestException as e:
            self.logger.error(f"DeepSeek streaming error: {e}")
            yield "Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n."
        except Exception as e:
            self.logger.error(f"DeepSeek unexpected error: {e}")
            yield "Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n."

    # TH√äM C√ÅC METHODS C≈® ƒê·ªÇ BACKWARD COMPATIBILITY
    async def chat_completion(
        self, messages: List[Dict], provider: str = "deepseek"
    ) -> str:
        """Non-streaming chat completion - USES PROVEN STREAMING METHOD"""
        try:
            if provider == "chatgpt" and self.chatgpt_client:
                return await self.chatgpt_client.chat_completion(messages)
            elif provider == "gemini" and self.gemini_client:
                return await self.gemini_client.chat_completion(messages)
            else:
                # Use the proven streaming method that works well in other routes
                # Collect all chunks into a single response
                chunks = []
                async for chunk in self.chat_completion_stream_with_reasoning(
                    messages, provider, use_reasoning=False
                ):
                    chunks.append(chunk)

                raw_response = "".join(chunks)
                logger.info(
                    f"‚úÖ {provider} chat_completion: Response length: {len(raw_response)} chars"
                )
                return raw_response
        except Exception as e:
            logger.error(f"‚ùå Error in chat_completion: {e}")
            raise e

    async def chat_completion_stream(
        self, messages: List[Dict], provider: str = "deepseek"
    ) -> AsyncGenerator[str, None]:
        """Original streaming method - backward compatibility"""
        async for chunk in self.chat_completion_stream_with_reasoning(
            messages, provider, use_reasoning=False
        ):
            yield chunk

    def _deepseek_completion_sync(self, messages: List[Dict]) -> str:
        """
        Synchronous DeepSeek completion
        """
        headers = {
            "Authorization": f"Bearer {self.deepseek_api_key}",
            "Content-Type": "application/json",
        }

        data = {
            "model": "deepseek-chat",
            "messages": messages,
            "temperature": 0.3,
            "max_tokens": 4000,
            "stream": False,
        }

        try:
            response = requests.post(
                self.deepseek_api_url, headers=headers, json=data, timeout=60
            )

            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                return f"DeepSeek API Error: {response.status_code} - {response.text}"

        except Exception as e:
            return f"DeepSeek request failed: {str(e)}"

    async def _deepseek_completion_async(self, messages: List[Dict]) -> str:
        """
        Asynchronous DeepSeek completion (non-streaming) - ENHANCED DEBUG VERSION
        """
        import aiohttp

        headers = {
            "Authorization": f"Bearer {self.deepseek_api_key}",
            "Content-Type": "application/json",
        }

        data = {
            "model": "deepseek-chat",
            "messages": messages,
            "temperature": 0.1,  # Lower temperature for more consistent results
            "max_tokens": 2000,  # Reduced from 4000 to prevent timeout
            "stream": False,
        }

        self.logger.info(
            f"DeepSeek async call - {len(messages)} messages, total chars: {sum(len(str(m.get('content', ''))) for m in messages)}"
        )

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.deepseek_api_url,
                    headers=headers,
                    json=data,
                    timeout=aiohttp.ClientTimeout(
                        total=120
                    ),  # Increased timeout to 120s
                ) as response:
                    self.logger.info(f"DeepSeek response status: {response.status}")

                    if response.status == 200:
                        result = await response.json()
                        content = result["choices"][0]["message"]["content"]
                        self.logger.info(
                            f"DeepSeek success - response length: {len(content)} chars"
                        )
                        return content
                    else:
                        error_text = await response.text()
                        self.logger.error(
                            f"DeepSeek API Error: {response.status} - {error_text}"
                        )
                        return f"DeepSeek API Error: {response.status} - {error_text}"

        except Exception as e:
            self.logger.error(f"DeepSeek async exception: {str(e)}")
            import traceback

            self.logger.error(f"DeepSeek traceback: {traceback.format_exc()}")
            return f"DeepSeek request failed: {str(e)}"

    # ===== ADDITIONAL METHODS FOR API COMPATIBILITY =====

    async def get_available_providers(self) -> List[str]:
        """
        ‚úÖ Get list of available AI providers
        """
        providers = ["deepseek"]
        if self.chatgpt_client:
            providers.append("chatgpt")
        if self.gemini_client:
            providers.append("gemini")
        if self.cerebras_client:
            providers.append("cerebras")
        return providers

    def get_current_provider(self) -> str:
        """
        ‚úÖ Get current default provider
        """
        return "deepseek"  # Default provider

    async def get_response(
        self,
        question: str,
        user_id: Optional[str] = None,
        provider: str = "deepseek",
    ) -> str:
        """
        ‚úÖ Get single response from AI provider (non-streaming)
        """
        try:
            messages = [{"role": "user", "content": question}]

            if provider == "chatgpt" and self.chatgpt_client:
                return await self.chatgpt_client.get_completion(messages)
            elif provider == "cerebras" and self.cerebras_client:
                return await self.cerebras_client.chat_completion(messages)
            elif provider == "gemini" and self.gemini_client:
                # Note: Gemini client would need a non-streaming method
                return await self.gemini_client.get_completion(messages)
            else:
                # Use DeepSeek
                return await self._deepseek_completion_async(messages)

        except Exception as e:
            self.logger.error(f"Error in get_response: {e}")
            return f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}"

    async def stream_response(
        self,
        question: str,
        user_id: Optional[str] = None,
        provider: str = "deepseek",
    ) -> AsyncGenerator[str, None]:
        """
        ‚úÖ Stream response from AI provider
        """
        try:
            messages = [{"role": "user", "content": question}]

            async for chunk in self.chat_completion_stream_with_reasoning(
                messages, provider
            ):
                yield chunk

        except Exception as e:
            self.logger.error(f"Error in stream_response: {e}")
            yield f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}"

    async def clear_history(self, user_id: Optional[str] = None):
        """
        ‚úÖ Clear chat history for user (simplified without session_id)
        """
        self.logger.info(f"History cleared for user: {user_id or 'anonymous'}")
        # TODO: Implement actual history storage and clearing based on user_id
        return True

    # ===== END ADDITIONAL METHODS =====

    async def chat_with_file_stream(
        self,
        messages: List[Dict],
        file_content: bytes,
        file_name: str,
        provider: str = "gemini",
        temperature: float = 0.7,
    ) -> AsyncGenerator[str, None]:
        """
        Stream chat with file upload support.
        Currently only supports Gemini provider.
        """
        try:
            if provider == "gemini" and self.gemini_client:
                async for chunk in self.gemini_client.chat_with_file_stream(
                    messages, file_content, file_name, temperature
                ):
                    yield chunk
            elif provider == "chatgpt" and self.chatgpt_client:
                # ChatGPT doesn't have direct file upload, could implement URL-based approach
                yield f"ChatGPT file upload ch∆∞a ƒë∆∞·ª£c h·ªó tr·ª£ trong ph∆∞∆°ng th·ª©c n√†y."
            else:
                yield f"Provider {provider} kh√¥ng h·ªó tr·ª£ chat v·ªõi file upload."

        except Exception as e:
            self.logger.error(f"‚ùå Error in chat_with_file_stream: {e}")
            yield f"L·ªói {provider} file processing: {str(e)}"

    async def generate_embedding(self, text: str) -> List[float]:
        """
        Generate embedding vector for text using sentence-transformers (768 dimensions)
        T·∫°o vector embedding cho text s·ª≠ d·ª•ng sentence-transformers (768 dimensions)
        """
        try:
            # Import EmbeddingService to use 768-dimension model
            from src.services.embedding_service import get_embedding_service

            # Use singleton embedding service instance
            embedding_service = get_embedding_service()

            # Generate embedding using sentence-transformers
            embedding = await embedding_service.generate_embedding(text)

            self.logger.info(f"‚úÖ Generated embedding with {len(embedding)} dimensions")
            return embedding

        except Exception as e:
            self.logger.error(f"Failed to generate embedding: {e}")
            return []

    async def generate_embeddings_batch(
        self, texts: List[str], max_batch_size: int = 20, timeout_seconds: int = 300
    ) -> List[List[float]]:
        """
        Generate embedding vectors for multiple texts efficiently using batch processing
        T·∫°o vector embedding cho nhi·ªÅu text hi·ªáu qu·∫£ s·ª≠ d·ª•ng batch processing
        """
        try:
            # Import EmbeddingService to use 768-dimension model
            from src.services.embedding_service import get_embedding_service

            # Use singleton embedding service instance
            embedding_service = get_embedding_service()

            # Generate embeddings using batch processing with timeout and size limits
            embeddings = await embedding_service.generate_embeddings_batch(
                texts=texts,
                max_batch_size=max_batch_size,
                timeout_seconds=timeout_seconds,
            )

            self.logger.info(f"‚úÖ Generated {len(embeddings)} embeddings in batch mode")
            return embeddings

        except Exception as e:
            self.logger.error(f"Failed to generate batch embeddings: {e}")
            # Return zero vectors as fallback
            return [[0.0] * 768] * len(texts)
