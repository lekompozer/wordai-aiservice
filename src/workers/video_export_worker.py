"""
Video Export Worker
Processes video export tasks from Redis queue using Playwright for screenshots
"""

import os
import sys
import asyncio
import logging
import signal
import time
import shutil
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from pathlib import Path
from dotenv import load_dotenv

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

# Load environment variables
env_var = os.getenv("ENVIRONMENT", os.getenv("ENV", "production"))
env_file = "development.env" if env_var == "development" else ".env"
load_dotenv(env_file)

from bson import ObjectId

from src.queue.queue_manager import QueueManager, set_job_status
from src.models.ai_queue_tasks import VideoExportTask
from src.database.db_manager import DBManager
from src.utils.logger import setup_logger

logger = setup_logger()


class VideoExportWorker:
    """Worker that processes video export tasks from Redis queue"""

    def __init__(
        self,
        worker_id: Optional[str] = None,
        redis_url: Optional[str] = None,
        batch_size: int = 1,
        max_retries: int = 2,
    ):
        self.worker_id = (
            worker_id or f"video_export_worker_{int(time.time())}_{os.getpid()}"
        )
        self.redis_url = redis_url or os.getenv(
            "REDIS_URL", "redis://redis-server:6379"
        )
        self.batch_size = batch_size
        self.max_retries = max_retries
        self.running = False

        # Initialize components
        self.queue_manager = QueueManager(
            redis_url=self.redis_url, queue_name="video_export"
        )
        self.db_manager = DBManager()
        self.db = self.db_manager.db

        # Storage services (lazy init)
        self.r2_service = None
        self.library_manager = None

        # Frontend URL for loading presentations
        self.frontend_url = os.getenv("FRONTEND_URL", "https://wordai.pro")

        logger.info(f"üîß Video Export Worker {self.worker_id} initialized")
        logger.info(f"   üì° Redis: {self.redis_url}")
        logger.info(f"   üåê Frontend: {self.frontend_url}")

    async def initialize(self):
        """Initialize worker components"""
        try:
            await self.queue_manager.connect()
            logger.info(
                f"‚úÖ Worker {self.worker_id}: Connected to Redis video_export queue"
            )

            # Check if Playwright is installed
            try:
                from playwright.async_api import async_playwright

                logger.info("‚úÖ Playwright available")
            except ImportError:
                logger.error(
                    "‚ùå Playwright not installed. Run: pip install playwright && playwright install chromium"
                )
                raise

        except Exception as e:
            logger.error(f"‚ùå Worker {self.worker_id}: Initialization failed: {e}")
            raise

    async def shutdown(self):
        """Gracefully shutdown worker"""
        logger.info(f"üõë Worker {self.worker_id}: Shutting down...")
        self.running = False

        if self.queue_manager:
            await self.queue_manager.disconnect()

        logger.info(f"‚úÖ Worker {self.worker_id}: Shutdown complete")

    async def capture_screenshots_optimized(
        self,
        public_token: str,
        slide_count: int,
        output_dir: Path,
        job_id: str,
    ) -> List[Path]:
        """
        Capture 1 screenshot per slide at 6s mark (after animations)

        Args:
            public_token: Public presentation token
            slide_count: Total number of slides
            output_dir: Directory to save screenshots
            job_id: Job ID for progress updates

        Returns:
            List of screenshot paths
        """
        from playwright.async_api import async_playwright

        logger.info(f"üì∏ Optimized mode: Capturing {slide_count} screenshots...")

        screenshot_paths = []
        presentation_url = f"{self.frontend_url}/public/presentations/{public_token}"

        async with async_playwright() as p:
            # Launch browser
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    "--no-sandbox",
                    "--disable-setuid-sandbox",
                    "--disable-dev-shm-usage",
                ],
            )

            page = await browser.new_page(
                viewport={"width": 1920, "height": 1080},
                device_scale_factor=1,
            )

            # Load presentation
            logger.info(f"   üåê Loading: {presentation_url}")
            await page.goto(presentation_url, wait_until="networkidle")
            await asyncio.sleep(2)  # Wait for initial render

            # Capture each slide
            for slide_idx in range(slide_count):
                try:
                    # Navigate to slide
                    await page.evaluate(f"window.goToSlide({slide_idx})")

                    # Wait 6 seconds for animations to complete
                    await asyncio.sleep(6)

                    # Take screenshot
                    screenshot_path = output_dir / f"slide_{slide_idx:03d}.png"
                    await page.screenshot(
                        path=str(screenshot_path),
                        type="png",
                        full_page=False,
                    )

                    screenshot_paths.append(screenshot_path)
                    logger.info(
                        f"   ‚úÖ Slide {slide_idx + 1}/{slide_count}: {screenshot_path.name}"
                    )

                    # Update progress: screenshots are 50% of total work
                    progress = int(((slide_idx + 1) / slide_count) * 50)
                    await set_job_status(
                        redis_client=self.queue_manager.redis_client,
                        job_id=job_id,
                        status="processing",
                        progress=progress,
                        current_phase="screenshot",
                    )

                except Exception as e:
                    logger.error(f"   ‚ùå Failed to capture slide {slide_idx}: {e}")
                    # Continue with next slide
                    continue

            await browser.close()

        logger.info(f"‚úÖ Captured {len(screenshot_paths)} screenshots")
        return screenshot_paths

    async def capture_screenshots_animated(
        self,
        public_token: str,
        slide_count: int,
        output_dir: Path,
        job_id: str,
    ) -> Dict[int, List[Path]]:
        """
        Capture 5s animation (150 frames @ 30 FPS) per slide

        Args:
            public_token: Public presentation token
            slide_count: Total number of slides
            output_dir: Directory to save screenshots
            job_id: Job ID for progress updates

        Returns:
            Dict mapping slide_index to list of frame paths
        """
        from playwright.async_api import async_playwright

        logger.info(f"üé¨ Animated mode: Capturing {slide_count} slides √ó 150 frames...")

        slide_frames = {}
        presentation_url = f"{self.frontend_url}/public/presentations/{public_token}"
        fps = 30
        animation_duration = 5  # seconds
        frames_per_slide = fps * animation_duration  # 150 frames

        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    "--no-sandbox",
                    "--disable-setuid-sandbox",
                    "--disable-dev-shm-usage",
                ],
            )

            page = await browser.new_page(
                viewport={"width": 1920, "height": 1080},
                device_scale_factor=1,
            )

            # Load presentation
            logger.info(f"   üåê Loading: {presentation_url}")
            await page.goto(presentation_url, wait_until="networkidle")
            await asyncio.sleep(2)

            # Capture each slide
            for slide_idx in range(slide_count):
                try:
                    # Navigate to slide
                    await page.evaluate(f"window.goToSlide({slide_idx})")
                    await asyncio.sleep(0.5)  # Brief settle time

                    # Create slide directory
                    slide_dir = output_dir / f"slide_{slide_idx:03d}"
                    slide_dir.mkdir(exist_ok=True)

                    frames = []

                    # Capture frames at 30 FPS for 5 seconds
                    frame_interval = 1 / fps  # ~33ms per frame
                    for frame_idx in range(frames_per_slide):
                        frame_path = slide_dir / f"frame_{frame_idx:04d}.png"

                        await page.screenshot(
                            path=str(frame_path),
                            type="png",
                            full_page=False,
                        )

                        frames.append(frame_path)

                        # Sleep for next frame (30 FPS = 33.3ms per frame)
                        await asyncio.sleep(frame_interval)

                    slide_frames[slide_idx] = frames

                    logger.info(
                        f"   ‚úÖ Slide {slide_idx + 1}/{slide_count}: {len(frames)} frames"
                    )

                    # Update progress: screenshots are 50% of total work
                    progress = int(((slide_idx + 1) / slide_count) * 50)
                    await set_job_status(
                        redis_client=self.queue_manager.redis_client,
                        job_id=job_id,
                        status="processing",
                        progress=progress,
                        current_phase="screenshot",
                    )

                except Exception as e:
                    logger.error(f"   ‚ùå Failed to capture slide {slide_idx}: {e}")
                    slide_frames[slide_idx] = []
                    continue

            await browser.close()

        total_frames = sum(len(frames) for frames in slide_frames.values())
        logger.info(
            f"‚úÖ Captured {total_frames} frames across {len(slide_frames)} slides"
        )
        return slide_frames

    async def download_audio(
        self, audio_url: str, output_path: Path, job_id: str
    ) -> Path:
        """
        Download merged audio file

        Args:
            audio_url: URL to download audio from
            output_path: Path to save audio file
            job_id: Job ID for progress updates

        Returns:
            Path to downloaded audio file
        """
        import aiohttp

        logger.info(f"üéµ Downloading audio from: {audio_url}")

        async with aiohttp.ClientSession() as session:
            async with session.get(audio_url) as response:
                if response.status != 200:
                    raise ValueError(
                        f"Failed to download audio: HTTP {response.status}"
                    )

                # Stream to file
                with open(output_path, "wb") as f:
                    async for chunk in response.content.iter_chunked(8192):
                        f.write(chunk)

        file_size_mb = output_path.stat().st_size / (1024 * 1024)
        logger.info(f"‚úÖ Audio downloaded: {output_path.name} ({file_size_mb:.1f} MB)")
        return output_path

    async def encode_video_optimized(
        self,
        screenshot_paths: List[Path],
        slide_timestamps: List[Dict[str, float]],
        audio_path: Path,
        output_path: Path,
        settings: Dict[str, Any],
        job_id: str,
    ) -> Path:
        """
        Encode video in optimized mode using FFmpeg slideshow

        Args:
            screenshot_paths: List of screenshot image paths
            slide_timestamps: List of slide timestamp dicts with start_time, end_time
            audio_path: Path to audio file
            output_path: Path to save final MP4
            settings: Video export settings (resolution, fps, crf, etc.)
            job_id: Job ID for progress updates

        Returns:
            Path to encoded video file
        """
        logger.info(f"üé¨ Encoding optimized video...")

        temp_dir = screenshot_paths[0].parent

        # 1. Create durations.txt with slide timestamps
        durations_file = temp_dir / "durations.txt"
        with open(durations_file, "w") as f:
            for idx, screenshot_path in enumerate(screenshot_paths):
                # Get duration from timestamps
                if idx < len(slide_timestamps):
                    duration = (
                        slide_timestamps[idx]["end_time"]
                        - slide_timestamps[idx]["start_time"]
                    )
                else:
                    duration = 5.0  # Default 5 seconds

                f.write(f"file '{screenshot_path.name}'\n")
                f.write(f"duration {duration}\n")

            # FFmpeg requires last file repeated without duration
            if screenshot_paths:
                f.write(f"file '{screenshot_paths[-1].name}'\n")

        logger.info(f"   Created durations.txt with {len(screenshot_paths)} slides")

        # 2. Get encoding settings
        crf = settings.get("crf", 28)
        fps = settings.get("fps", 24)
        resolution = settings.get("resolution", "1080p")
        width, height = (1920, 1080) if resolution == "1080p" else (1280, 720)

        # 3. Encode video with FFmpeg concat demuxer
        video_only_path = temp_dir / "video_only.mp4"

        ffmpeg_cmd = [
            "ffmpeg",
            "-f",
            "concat",
            "-safe",
            "0",
            "-i",
            str(durations_file),
            "-vf",
            f"scale={width}:{height}:force_original_aspect_ratio=decrease,pad={width}:{height}:(ow-iw)/2:(oh-ih)/2,fps={fps}",
            "-c:v",
            "libx264",
            "-crf",
            str(crf),
            "-preset",
            "medium",
            "-pix_fmt",
            "yuv420p",
            "-movflags",
            "+faststart",
            "-y",
            str(video_only_path),
        ]

        logger.info(f"   FFmpeg command: {' '.join(ffmpeg_cmd)}")

        # Run FFmpeg
        process = await asyncio.create_subprocess_exec(
            *ffmpeg_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=str(temp_dir),
        )

        # Update progress: encoding (50-80%)
        await set_job_status(
            redis_client=self.queue_manager.redis_client,
            job_id=job_id,
            status="processing",
            progress=65,
            current_phase="encode",
        )

        stdout, stderr = await process.communicate()

        if process.returncode != 0:
            error_msg = stderr.decode() if stderr else "FFmpeg encoding failed"
            logger.error(f"   ‚ùå FFmpeg error: {error_msg}")
            raise RuntimeError(f"FFmpeg encoding failed: {error_msg}")

        logger.info(f"   ‚úÖ Video encoded: {video_only_path.name}")

        # 4. Merge with audio
        await set_job_status(
            redis_client=self.queue_manager.redis_client,
            job_id=job_id,
            status="processing",
            progress=80,
            current_phase="merge",
        )

        merge_cmd = [
            "ffmpeg",
            "-i",
            str(video_only_path),
            "-i",
            str(audio_path),
            "-c:v",
            "copy",
            "-c:a",
            "aac",
            "-b:a",
            "128k",
            "-shortest",
            "-movflags",
            "+faststart",
            "-y",
            str(output_path),
        ]

        logger.info(f"   Merging audio: {' '.join(merge_cmd)}")

        process = await asyncio.create_subprocess_exec(
            *merge_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        stdout, stderr = await process.communicate()

        if process.returncode != 0:
            error_msg = stderr.decode() if stderr else "Audio merge failed"
            logger.error(f"   ‚ùå Merge error: {error_msg}")
            raise RuntimeError(f"Audio merge failed: {error_msg}")

        file_size_mb = output_path.stat().st_size / (1024 * 1024)
        logger.info(f"‚úÖ Final video: {output_path.name} ({file_size_mb:.1f} MB)")

        return output_path

    async def encode_video_animated(
        self,
        slide_frames: Dict[int, List[Path]],
        slide_timestamps: List[Dict[str, float]],
        audio_path: Path,
        output_path: Path,
        settings: Dict[str, Any],
        job_id: str,
    ) -> Path:
        """
        Encode video in animated mode using FFmpeg image2pipe

        Args:
            slide_frames: Dict mapping slide_index to list of frame paths
            slide_timestamps: List of slide timestamp dicts
            audio_path: Path to audio file
            output_path: Path to save final MP4
            settings: Video export settings
            job_id: Job ID for progress updates

        Returns:
            Path to encoded video file
        """
        logger.info(f"üé¨ Encoding animated video...")

        # Check if we have any frames
        if not slide_frames or all(
            len(frames) == 0 for frames in slide_frames.values()
        ):
            raise ValueError(
                "No frames captured. Frontend must implement window.goToSlide(slideIndex) function for Playwright navigation."
            )

        temp_dir = list(slide_frames.values())[0][0].parent.parent

        # 1. Create concat file listing all frame sequences
        concat_file = temp_dir / "concat.txt"
        with open(concat_file, "w") as f:
            for slide_idx in sorted(slide_frames.keys()):
                frames = slide_frames[slide_idx]

                # Animation frames (5 seconds @ 30 FPS = 150 frames)
                for frame_path in frames:
                    f.write(f"file '{frame_path.relative_to(temp_dir)}'\n")
                    f.write(f"duration {1/30}\n")  # 30 FPS

                # Freeze last frame until next slide
                if slide_idx < len(slide_timestamps) - 1:
                    freeze_duration = (
                        slide_timestamps[slide_idx]["end_time"]
                        - slide_timestamps[slide_idx]["start_time"]
                        - 5
                    )
                    if freeze_duration > 0:
                        last_frame = frames[-1]
                        f.write(f"file '{last_frame.relative_to(temp_dir)}'\n")
                        f.write(f"duration {freeze_duration}\n")

            # FFmpeg requires last file repeated
            if slide_frames:
                last_slide = max(slide_frames.keys())
                last_frame = slide_frames[last_slide][-1]
                f.write(f"file '{last_frame.relative_to(temp_dir)}'\n")

        logger.info(f"   Created concat file for {len(slide_frames)} slides")

        # 2. Get encoding settings
        crf = settings.get("crf", 25)
        fps = settings.get("fps", 30)
        resolution = settings.get("resolution", "1080p")
        width, height = (1920, 1080) if resolution == "1080p" else (1280, 720)

        # 3. Encode video
        video_only_path = temp_dir / "video_only.mp4"

        ffmpeg_cmd = [
            "ffmpeg",
            "-f",
            "concat",
            "-safe",
            "0",
            "-i",
            str(concat_file),
            "-vf",
            f"scale={width}:{height}:force_original_aspect_ratio=decrease,pad={width}:{height}:(ow-iw)/2:(oh-ih)/2,fps={fps}",
            "-c:v",
            "libx264",
            "-crf",
            str(crf),
            "-preset",
            "medium",
            "-pix_fmt",
            "yuv420p",
            "-movflags",
            "+faststart",
            "-y",
            str(video_only_path),
        ]

        logger.info(f"   FFmpeg command: {' '.join(ffmpeg_cmd)}")

        await set_job_status(
            redis_client=self.queue_manager.redis_client,
            job_id=job_id,
            status="processing",
            progress=65,
            current_phase="encode",
        )

        process = await asyncio.create_subprocess_exec(
            *ffmpeg_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        stdout, stderr = await process.communicate()

        if process.returncode != 0:
            error_msg = stderr.decode() if stderr else "FFmpeg encoding failed"
            logger.error(f"   ‚ùå FFmpeg error: {error_msg}")
            raise RuntimeError(f"FFmpeg encoding failed: {error_msg}")

        logger.info(f"   ‚úÖ Video encoded: {video_only_path.name}")

        # 4. Merge with audio
        await set_job_status(
            redis_client=self.queue_manager.redis_client,
            job_id=job_id,
            status="processing",
            progress=80,
            current_phase="merge",
        )

        merge_cmd = [
            "ffmpeg",
            "-i",
            str(video_only_path),
            "-i",
            str(audio_path),
            "-c:v",
            "copy",
            "-c:a",
            "aac",
            "-b:a",
            "128k",
            "-shortest",
            "-movflags",
            "+faststart",
            "-y",
            str(output_path),
        ]

        logger.info(f"   Merging audio...")

        process = await asyncio.create_subprocess_exec(
            *merge_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        stdout, stderr = await process.communicate()

        if process.returncode != 0:
            error_msg = stderr.decode() if stderr else "Audio merge failed"
            logger.error(f"   ‚ùå Merge error: {error_msg}")
            raise RuntimeError(f"Audio merge failed: {error_msg}")

        file_size_mb = output_path.stat().st_size / (1024 * 1024)
        logger.info(f"‚úÖ Final video: {output_path.name} ({file_size_mb:.1f} MB)")

        return output_path

    async def upload_to_r2_and_library(
        self,
        video_path: Path,
        user_id: str,
        presentation_id: str,
        job_id: str,
        export_mode: str,
        settings: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Upload final video to R2 and save to library_videos

        Args:
            video_path: Path to final.mp4 file
            user_id: User ID
            presentation_id: Presentation ID
            job_id: Export job ID
            export_mode: optimized or animated
            settings: Video export settings

        Returns:
            Dict with r2_url, library_video_id, etc.
        """
        logger.info("‚òÅÔ∏è  Uploading video to R2...")

        # Initialize services (lazy init)
        if not self.r2_service:
            from src.services.r2_storage_service import R2StorageService

            self.r2_service = R2StorageService()

        if not self.library_manager:
            from src.services.library_manager import LibraryManager

            self.library_manager = LibraryManager(db=self.db)

        # Read video file
        with open(video_path, "rb") as f:
            video_bytes = f.read()

        file_size = len(video_bytes)
        file_size_mb = file_size / (1024 * 1024)

        # Generate R2 key
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        file_name = f"export_{job_id}_{export_mode}.mp4"
        r2_key = f"videos/exports/{user_id}/{presentation_id}/{timestamp}_{job_id}.mp4"

        # Upload to R2
        upload_result = await self.r2_service.upload_file(
            file_content=video_bytes,
            r2_key=r2_key,
            content_type="video/mp4",
        )

        r2_url = upload_result["public_url"]
        logger.info(f"   ‚úÖ Uploaded to R2: {r2_url}")

        # Get video metadata
        resolution = settings.get("resolution", "1080p")
        width, height = (1920, 1080) if resolution == "1080p" else (1280, 720)
        fps = settings.get("fps", 24)
        crf = settings.get("crf", 28)

        # Get video duration from video file (using ffprobe)
        try:
            ffprobe_cmd = [
                "ffprobe",
                "-v",
                "error",
                "-show_entries",
                "format=duration",
                "-of",
                "default=noprint_wrappers=1:nokey=1",
                str(video_path),
            ]
            process = await asyncio.create_subprocess_exec(
                *ffprobe_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await process.communicate()
            duration = float(stdout.decode().strip()) if stdout else 0
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è  Could not get video duration: {e}")
            duration = 0

        # Save to library_videos
        library_record = self.library_manager.save_library_file(
            user_id=user_id,
            filename=file_name,
            file_type="video",
            category="video",
            r2_url=r2_url,
            r2_key=r2_key,
            file_size=file_size,
            mime_type="video/mp4",
            metadata={
                "source_type": "presentation_export",
                "presentation_id": presentation_id,
                "export_job_id": job_id,
                "export_mode": export_mode,
                "resolution": resolution,
                "width": width,
                "height": height,
                "fps": fps,
                "crf": crf,
                "duration_seconds": duration,
                "codec": "h264",
                "audio_codec": "aac",
            },
        )

        library_video_id = library_record.get(
            "library_id", library_record.get("file_id")
        )

        logger.info(f"   ‚úÖ Saved to library_videos: {library_video_id}")
        logger.info(
            f"   üìä File size: {file_size_mb:.1f} MB, Duration: {duration:.1f}s"
        )

        return {
            "r2_url": r2_url,
            "r2_key": r2_key,
            "library_video_id": library_video_id,
            "file_size": file_size,
            "file_size_mb": file_size_mb,
            "duration": duration,
        }

    async def process_task(self, task: VideoExportTask) -> bool:
        """
        Process a single video export task

        Args:
            task: VideoExportTask from Redis queue

        Returns:
            bool: True if processing successful
        """
        job_id = task.job_id
        start_time = datetime.utcnow()

        # Temporary directory for this job
        temp_dir = Path(f"/tmp/export_{job_id}")
        temp_dir.mkdir(parents=True, exist_ok=True)

        try:
            logger.info(f"üé¨ Processing video export job {job_id}")
            logger.info(f"   Presentation: {task.presentation_id}")
            logger.info(f"   Language: {task.language}")
            logger.info(f"   Mode: {task.export_mode}")

            # Update status to processing
            await set_job_status(
                redis_client=self.queue_manager.redis_client,
                job_id=job_id,
                status="processing",
                user_id=task.user_id,
                presentation_id=task.presentation_id,
                language=task.language,
                export_mode=task.export_mode,
                started_at=start_time.isoformat(),
                progress=0,
                current_phase="load",
            )

            # 1. Load presentation data
            logger.info("üìÑ Loading presentation data...")
            presentation = self.db.documents.find_one(
                {"document_id": task.presentation_id}
            )
            if not presentation:
                raise ValueError("Presentation not found")

            # Get public token
            sharing_config = self.db.presentation_sharing_config.find_one(
                {"presentation_id": task.presentation_id}
            )
            if not sharing_config or not sharing_config.get("public_token"):
                raise ValueError(
                    "Presentation not shared publicly. Generate public link first."
                )

            public_token = sharing_config["public_token"]

            # 2. Load subtitle and audio data
            subtitle = self.db.presentation_subtitles.find_one(
                {"_id": ObjectId(task.subtitle_id)}
            )
            if not subtitle:
                raise ValueError("Subtitle not found")

            audio = self.db.presentation_audio.find_one(
                {"_id": ObjectId(task.audio_id)}
            )
            if not audio or not audio.get("audio_url"):
                raise ValueError("Audio not found")

            # Get slide timestamps for durations
            slide_timestamps = audio.get("slide_timestamps", [])
            audio_duration = audio.get("audio_metadata", {}).get("duration_seconds", 0)

            # ‚úÖ CRITICAL: Get slide count from audio timestamps (NOT slide_backgrounds)
            # Presentations use outline_version ‚Üí slide_backgrounds may be empty/incomplete
            slide_count = len(slide_timestamps)

            if slide_count == 0:
                raise ValueError(
                    "No slide timestamps found in audio. Please regenerate audio."
                )

            logger.info(f"   Slides: {slide_count} (from audio timestamps)")
            logger.info(f"   Token: {public_token}")
            logger.info(f"   Audio duration: {audio_duration}s")

            # Update progress: loaded data
            await set_job_status(
                redis_client=self.queue_manager.redis_client,
                job_id=job_id,
                status="processing",
                progress=10,
                current_phase="screenshot",
            )

            # 3. Capture screenshots based on mode
            if task.export_mode == "optimized":
                screenshot_paths = await self.capture_screenshots_optimized(
                    public_token=public_token,
                    slide_count=slide_count,
                    output_dir=temp_dir,
                    job_id=job_id,
                )

                # Store for next phase (FFmpeg)
                screenshot_data = {
                    "mode": "optimized",
                    "screenshots": [str(p) for p in screenshot_paths],
                    "slide_timestamps": slide_timestamps,
                }

            else:  # animated
                slide_frames = await self.capture_screenshots_animated(
                    public_token=public_token,
                    slide_count=slide_count,
                    output_dir=temp_dir,
                    job_id=job_id,
                )

                # Store for next phase (FFmpeg)
                screenshot_data = {
                    "mode": "animated",
                    "slide_frames": {
                        idx: [str(p) for p in frames]
                        for idx, frames in slide_frames.items()
                    },
                    "slide_timestamps": slide_timestamps,
                }

            # Save screenshot metadata for Phase 3 (FFmpeg)
            import json

            metadata_path = temp_dir / "screenshot_metadata.json"
            with open(metadata_path, "w") as f:
                json.dump(screenshot_data, f, indent=2)

            logger.info(f"‚úÖ Screenshot phase complete: {metadata_path}")

            # Update progress: screenshots done (50%)
            await set_job_status(
                redis_client=self.queue_manager.redis_client,
                job_id=job_id,
                status="processing",
                progress=50,
                current_phase="encode",
                screenshot_metadata_path=str(metadata_path),
            )

            # 4. Download audio
            logger.info("üéµ Downloading audio...")
            audio_path = temp_dir / "audio.mp3"
            await self.download_audio(
                audio_url=audio["audio_url"],
                output_path=audio_path,
                job_id=job_id,
            )

            # 5. Encode video based on mode
            final_video_path = temp_dir / "final.mp4"

            if task.export_mode == "optimized":
                await self.encode_video_optimized(
                    screenshot_paths=screenshot_paths,
                    slide_timestamps=slide_timestamps,
                    audio_path=audio_path,
                    output_path=final_video_path,
                    settings=task.settings,
                    job_id=job_id,
                )
            else:  # animated
                await self.encode_video_animated(
                    slide_frames=slide_frames,
                    slide_timestamps=slide_timestamps,
                    audio_path=audio_path,
                    output_path=final_video_path,
                    settings=task.settings,
                    job_id=job_id,
                )

            # 6. Upload to R2 and save to library_videos
            logger.info("‚òÅÔ∏è  Phase 4: Uploading to R2 storage...")

            await set_job_status(
                redis_client=self.queue_manager.redis_client,
                job_id=job_id,
                status="processing",
                progress=90,
                current_phase="upload",
            )

            upload_result = await self.upload_to_r2_and_library(
                video_path=final_video_path,
                user_id=task.user_id,
                presentation_id=task.presentation_id,
                job_id=job_id,
                export_mode=task.export_mode,
                settings=task.settings,
            )

            r2_url = upload_result["r2_url"]
            library_video_id = upload_result["library_video_id"]
            file_size_mb = upload_result["file_size_mb"]
            duration = upload_result["duration"]

            # Mark as completed
            await set_job_status(
                redis_client=self.queue_manager.redis_client,
                job_id=job_id,
                status="completed",
                progress=100,
                current_phase="completed",
                output_url=r2_url,
                library_video_id=library_video_id,
                file_size_mb=file_size_mb,
                duration_seconds=duration,
            )

            # Cleanup temp directory
            try:
                shutil.rmtree(temp_dir)
                logger.info(f"   üóëÔ∏è  Cleaned up temp directory: {temp_dir}")
            except Exception as cleanup_error:
                logger.warning(f"   ‚ö†Ô∏è  Failed to cleanup {temp_dir}: {cleanup_error}")

            elapsed = (datetime.utcnow() - start_time).total_seconds()
            logger.info(f"‚úÖ Video export job {job_id} completed in {elapsed:.1f}s")
            logger.info(f"   üìπ Video URL: {r2_url}")
            logger.info(f"   üìö Library ID: {library_video_id}")
            logger.info(f"   üìä File size: {file_size_mb:.1f} MB")
            logger.info(f"   ‚è±Ô∏è  Duration: {duration:.1f}s")

            return True

        except Exception as e:
            error_msg = str(e)
            logger.error(
                f"‚ùå Video export job {job_id} failed: {error_msg}", exc_info=True
            )

            # Update status to failed
            await set_job_status(
                redis_client=self.queue_manager.redis_client,
                job_id=job_id,
                status="failed",
                user_id=task.user_id,
                error=error_msg,
                failed_at=datetime.utcnow().isoformat(),
            )

            # Cleanup temp directory on failure
            if temp_dir.exists():
                try:
                    shutil.rmtree(temp_dir)
                    logger.info(f"   üóëÔ∏è  Cleaned up temp directory: {temp_dir}")
                except Exception as cleanup_error:
                    logger.warning(
                        f"   ‚ö†Ô∏è  Failed to cleanup {temp_dir}: {cleanup_error}"
                    )

            return False

    async def run(self):
        """Main worker loop"""
        self.running = True
        logger.info(f"üöÄ Worker {self.worker_id} started")

        while self.running:
            try:
                # Process tasks from queue
                task_dict = await self.queue_manager.dequeue_generic_task(
                    worker_id=self.worker_id
                )

                if task_dict:
                    # Parse task
                    task = VideoExportTask(**task_dict)
                    logger.info(f"üì• Dequeued task: {task.job_id}")

                    # Process task
                    success = await self.process_task(task)

                    if success:
                        logger.info(f"‚úÖ Task {task.job_id} completed successfully")
                    else:
                        logger.error(f"‚ùå Task {task.job_id} failed")

                else:
                    # No tasks available, sleep briefly
                    await asyncio.sleep(1)

            except asyncio.CancelledError:
                logger.info("üõë Worker cancelled, shutting down...")
                break
            except Exception as e:
                logger.error(f"‚ùå Worker error: {e}", exc_info=True)
                await asyncio.sleep(5)  # Back off on error

        logger.info(f"‚úÖ Worker {self.worker_id} stopped")


async def main():
    """Main entry point"""
    worker = VideoExportWorker()

    # Setup signal handlers for graceful shutdown
    def signal_handler(sig, frame):
        logger.info(f"üõë Received signal {sig}, shutting down...")
        asyncio.create_task(worker.shutdown())

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    try:
        await worker.initialize()
        await worker.run()
    except KeyboardInterrupt:
        logger.info("üõë Keyboard interrupt received")
    finally:
        await worker.shutdown()


if __name__ == "__main__":
    asyncio.run(main())
