"""
Ingestion Worker for processing document ingestion tasks from the Redis queue.
Downloads files from R2 storage and processes them through the ingestion pipeline.

This worker connects to Redis, pulls tasks from the queue, and processes them
using the existing AIDocumentProcessor.
"""

import os
import os
import sys
import asyncio
import logging
import signal
import time
import traceback
from typing import Optional
from dotenv import load_dotenv

# Add src to path to enable imports
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

# Load environment variables
env_var = os.getenv("ENVIRONMENT", os.getenv("ENV", "production"))
env_file = "development.env" if env_var == "development" else ".env"
load_dotenv(env_file)

from src.workers.document_processor import AIDocumentProcessor
from src.queue.queue_manager import QueueManager, IngestionTask, TaskStatus
from src.utils.logger import setup_logger

# ==============================================================================
# WORKER CONFIGURATION
# ==============================================================================

logger = setup_logger()


class DocumentIngestionWorker:
    """
    Worker that processes document ingestion tasks from Redis queue.
    """

    def __init__(
        self,
        worker_id: str = None,
        redis_url: str = None,
        batch_size: int = 1,
        max_retries: int = 3,
    ):
        self.worker_id = worker_id or f"worker_{int(time.time())}_{os.getpid()}"
        self.redis_url = redis_url or os.getenv(
            "REDIS_URL", "redis://redis-server:6379"
        )
        self.batch_size = batch_size
        self.max_retries = max_retries
        self.running = False

        # Initialize components
        self.queue_manager = QueueManager(redis_url=self.redis_url)
        self.doc_processor = None

        logger.info(f"ğŸ”§ Worker {self.worker_id} initialized")
        logger.info(f"   ğŸ“¡ Redis: {self.redis_url}")
        logger.info(f"   ğŸ“¦ Batch size: {self.batch_size}")

        # Debug environment variables
        logger.info(f"ğŸ”§ Environment Debug:")
        logger.info(f"   ENV: {os.getenv('ENV', 'NOT_SET')}")
        logger.info(f"   R2_BUCKET_NAME: {os.getenv('R2_BUCKET_NAME', 'NOT_SET')}")
        logger.info(
            f"   R2_ACCOUNT_ID: {os.getenv('R2_ACCOUNT_ID', 'NOT_SET')[:10]}..."
        )
        logger.info(
            f"   R2_ACCESS_KEY_ID: {os.getenv('R2_ACCESS_KEY_ID', 'NOT_SET')[:10]}..."
        )
        logger.info(f"   Environment file loaded: {env_file}")

    async def initialize(self):
        """Initialize worker components"""
        try:
            # Connect to Redis
            await self.queue_manager.connect()
            logger.info(f"âœ… Worker {self.worker_id}: Connected to Redis")

            # Initialize document processor
            self.doc_processor = AIDocumentProcessor()
            logger.info(f"âœ… Worker {self.worker_id}: Document processor ready")

        except Exception as e:
            logger.error(f"âŒ Worker {self.worker_id}: Initialization failed: {e}")
            raise

    async def shutdown(self):
        """Gracefully shutdown worker"""
        logger.info(f"ğŸ›‘ Worker {self.worker_id}: Shutting down...")
        self.running = False

        if self.queue_manager:
            await self.queue_manager.disconnect()

        logger.info(f"âœ… Worker {self.worker_id}: Shutdown complete")

    async def process_task(self, task: IngestionTask) -> bool:
        """
        Process a single ingestion task.

        Args:
            task: IngestionTask to process

        Returns:
            True if successful, False if failed
        """
        start_time = time.time()

        logger.info(f"ğŸš€ Worker {self.worker_id}: Processing task {task.task_id}")
        logger.info(f"   ğŸ“„ Document: {task.document_id}")
        logger.info(f"   ğŸ‘¤ User: {task.user_id}")
        logger.info(f"   ğŸ“ File: {task.filename}")
        logger.info(f"   ğŸ—‚ï¸ R2 Key: {task.file_path}")

        try:
            # Step 1: Download file from R2
            logger.info(f"ğŸ“¥ Worker {self.worker_id}: Downloading from R2...")
            bucket_name = os.getenv("R2_BUCKET_NAME", "studynotes")
            logger.info(f"ğŸª£ Worker {self.worker_id}: Using bucket: '{bucket_name}'")
            logger.info(f"ğŸ”‘ Worker {self.worker_id}: R2 Key: '{task.file_path}'")
            file_content = await self.doc_processor.download_from_r2(
                bucket_name, task.file_path
            )
            logger.info(
                f"âœ… Worker {self.worker_id}: Downloaded {len(file_content)} bytes"
            )

            # Step 2: Extract text content
            logger.info(f"ğŸ” Worker {self.worker_id}: Extracting text content...")
            text_content = await self.doc_processor.extract_text_content(
                file_content, task.file_type, task.filename
            )

            if not text_content or len(text_content.strip()) < 50:
                raise Exception("Document appears to be empty or too short")

            logger.info(
                f"âœ… Worker {self.worker_id}: Extracted {len(text_content)} characters"
            )

            # Step 3: Chunk document
            logger.info(f"ğŸ”ª Worker {self.worker_id}: Chunking document...")
            chunks = self.doc_processor.chunk_document(text_content)
            logger.info(f"âœ… Worker {self.worker_id}: Created {len(chunks)} chunks")

            # Step 4: Generate embeddings
            logger.info(f"ğŸ§  Worker {self.worker_id}: Generating embeddings...")
            embeddings = await self.doc_processor.generate_embeddings(chunks)
            logger.info(
                f"âœ… Worker {self.worker_id}: Generated {len(embeddings)} embeddings"
            )

            # Step 5: Store in Qdrant
            collection_name = f"user_{task.user_id}_documents"
            logger.info(
                f"ğŸ’¾ Worker {self.worker_id}: Storing in Qdrant collection: {collection_name}"
            )

            await self.doc_processor.store_in_qdrant(
                collection_name=collection_name,
                chunks=chunks,
                embeddings=embeddings,
                task_data={
                    "userId": task.user_id,
                    "uploadId": task.document_id,
                    "taskId": task.task_id,
                    "fileName": task.filename,
                    "contentType": task.file_type,
                    "metadata": task.additional_metadata,
                },
            )

            processing_time = time.time() - start_time

            # Step 6: Send SUCCESS callback to backend
            if task.callback_url:
                logger.info(f"ğŸ“ Worker {self.worker_id}: Sending success callback...")
                callback_success = await self.doc_processor.send_callback(
                    task_data={
                        "callbackUrl": task.callback_url,
                        "taskId": task.task_id,
                        "uploadId": task.document_id,
                        "userId": task.user_id,
                    },
                    status="completed",
                    result={
                        "message": "Document processing completed successfully",
                        "chunksProcessed": len(chunks),
                        "collectionName": collection_name,
                        "documentLength": len(text_content),
                        "processingTime": processing_time,
                    },
                    processing_time=processing_time,
                    chunks_processed=len(chunks),
                    collection_name=collection_name,
                )
                logger.info(
                    f"ğŸ“ Worker {self.worker_id}: Callback: {'âœ…' if callback_success else 'âŒ'}"
                )

            # Mark task as completed
            await self.queue_manager.complete_task(task.task_id, success=True)

            logger.info(
                f"âœ… Worker {self.worker_id}: Task {task.task_id} completed successfully"
            )
            logger.info(f"   ğŸ“Š Chunks: {len(chunks)}")
            logger.info(f"   ğŸ’¾ Collection: {collection_name}")
            logger.info(f"   â±ï¸ Time: {processing_time:.2f}s")

            return True

        except Exception as e:
            error_msg = str(e)
            processing_time = time.time() - start_time

            logger.error(
                f"âŒ Worker {self.worker_id}: Task {task.task_id} failed: {error_msg}"
            )
            logger.error(
                f"ğŸ” Worker {self.worker_id}: Traceback: {traceback.format_exc()}"
            )

            # Send FAILED callback to backend
            if task.callback_url:
                await self.doc_processor.send_callback(
                    task_data={
                        "callbackUrl": task.callback_url,
                        "taskId": task.task_id,
                        "uploadId": task.document_id,
                        "userId": task.user_id,
                    },
                    status="failed",
                    error=error_msg,
                    processing_time=processing_time,
                )

            # Mark task as failed
            await self.queue_manager.complete_task(
                task.task_id, success=False, error_message=error_msg
            )

            return False

    async def run(self):
        """
        Main worker loop - continuously process tasks from queue.
        """
        logger.info(f"ğŸ¬ Worker {self.worker_id}: Starting main loop")
        self.running = True

        consecutive_empty_polls = 0
        max_empty_polls = 5  # Wait longer after multiple empty polls

        while self.running:
            try:
                # Get next task from queue (with timeout)
                task = await self.queue_manager.dequeue_task(
                    worker_id=self.worker_id, timeout=10  # 10 second timeout
                )

                if task:
                    consecutive_empty_polls = 0

                    # Process the task
                    success = await self.process_task(task)

                    if success:
                        logger.info(
                            f"âœ… Worker {self.worker_id}: Task processed successfully"
                        )
                    else:
                        logger.error(
                            f"âŒ Worker {self.worker_id}: Task processing failed"
                        )

                else:
                    # No task available - implement backoff
                    consecutive_empty_polls += 1

                    if consecutive_empty_polls >= max_empty_polls:
                        sleep_time = min(
                            30, consecutive_empty_polls * 2
                        )  # Max 30 seconds
                        logger.debug(
                            f"ğŸ’¤ Worker {self.worker_id}: No tasks, sleeping {sleep_time}s"
                        )
                        await asyncio.sleep(sleep_time)
                    else:
                        await asyncio.sleep(1)  # Short wait

            except asyncio.CancelledError:
                logger.info(f"ğŸ›‘ Worker {self.worker_id}: Received cancellation signal")
                break

            except Exception as e:
                logger.error(
                    f"âŒ Worker {self.worker_id}: Unexpected error in main loop: {e}"
                )
                logger.error(
                    f"ğŸ” Worker {self.worker_id}: Traceback: {traceback.format_exc()}"
                )

                # Sleep before retrying to avoid tight error loop
                await asyncio.sleep(5)

        logger.info(f"ğŸ Worker {self.worker_id}: Main loop ended")


# ==============================================================================
# MAIN WORKER SCRIPT
# ==============================================================================


async def main():
    """Main worker entry point"""
    # Handle shutdown signals
    shutdown_event = asyncio.Event()

    def signal_handler(signum, frame):
        logger.info(f"ğŸ“¡ Received signal {signum}, initiating shutdown...")
        shutdown_event.set()

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # Create and initialize worker
    worker = DocumentIngestionWorker()

    try:
        await worker.initialize()

        # Run worker until shutdown signal
        worker_task = asyncio.create_task(worker.run())
        shutdown_task = asyncio.create_task(shutdown_event.wait())

        # Wait for either worker completion or shutdown signal
        done, pending = await asyncio.wait(
            [worker_task, shutdown_task], return_when=asyncio.FIRST_COMPLETED
        )

        # Cancel remaining tasks
        for task in pending:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass

    except Exception as e:
        logger.error(f"âŒ Worker failed to start: {e}")

    finally:
        await worker.shutdown()


if __name__ == "__main__":
    logger.info("ğŸš€ Starting Document Ingestion Worker")
    asyncio.run(main())
