"""
Company Data Management Service
D·ªãch v·ª• qu·∫£n l√Ω d·ªØ li·ªáu c√¥ng ty
"""

import os
import uuid
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from pathlib import Path
import json

from src.models.unified_models import (
    CompanyConfig,
    CompanyDataFile,
    FileType,
    IndustryDataType,
    DataExtractionStatus,
    DataExtractionRequest,
    CompanyDataStats,
    Industry,
    Language,
    QdrantDocumentChunk,
)
from src.services.ai_extraction_service import get_ai_service
from src.services.qdrant_company_service import get_qdrant_service
from src.utils.logger import setup_logger

logger = setup_logger()


class CompanyDataManager:
    """
    Main service for managing company data lifecycle
    D·ªãch v·ª• ch√≠nh ƒë·ªÉ qu·∫£n l√Ω v√≤ng ƒë·ªùi d·ªØ li·ªáu c√¥ng ty
    """

    def __init__(self, storage_base_path: str = "./data/companies"):
        self.storage_base_path = Path(storage_base_path)
        self.storage_base_path.mkdir(parents=True, exist_ok=True)

        self.ai_extraction_service = get_ai_service()
        self.qdrant_service = get_qdrant_service()
        self.logger = logger

        # MongoDB storage for company persistence
        # L∆∞u tr·ªØ MongoDB cho t√≠nh b·ªÅn v·ªØng c·ªßa d·ªØ li·ªáu c√¥ng ty
        from src.database.company_db_service import get_company_db_service

        self.company_db = get_company_db_service()

        # In-memory cache for fast access (loaded from MongoDB)
        # Cache trong memory ƒë·ªÉ truy c·∫≠p nhanh (ƒë∆∞·ª£c load t·ª´ MongoDB)
        self.companies: Dict[str, CompanyConfig] = {}
        self.files: Dict[str, CompanyDataFile] = {}

        # Load existing companies from MongoDB on startup
        # Load c√°c c√¥ng ty ƒë√£ c√≥ t·ª´ MongoDB khi kh·ªüi ƒë·ªông
        self._load_companies_from_db()

        # File type mappings / √Ånh x·∫° lo·∫°i file
        self.file_type_mappings = {
            ".jpg": FileType.IMAGE,
            ".jpeg": FileType.IMAGE,
            ".png": FileType.IMAGE,
            ".gif": FileType.IMAGE,
            ".webp": FileType.IMAGE,
            ".pdf": FileType.PDF,
            ".xlsx": FileType.EXCEL,
            ".xls": FileType.EXCEL,
            ".docx": FileType.WORD,
            ".doc": FileType.WORD,
            ".txt": FileType.TEXT,
            ".json": FileType.JSON,
            ".csv": FileType.CSV,
        }

        # Backend DataType mapping to IndustryDataType (unified for all industries)
        # √Ånh x·∫° DataType t·ª´ backend sang IndustryDataType (th·ªëng nh·∫•t cho t·∫•t c·∫£ ng√†nh)
        self.backend_data_type_mapping = {
            "company_info": IndustryDataType.COMPANY_INFO,
            "products": IndustryDataType.PRODUCTS,  # Unified products for all industries
            "services": IndustryDataType.SERVICES,  # Unified services for all industries
            "policies": IndustryDataType.POLICIES,
            "faq": IndustryDataType.FAQ,
            "knowledge_base": IndustryDataType.KNOWLEDGE_BASE,
            "other": IndustryDataType.OTHER,
        }

        # Industry data type suggestions for UI (same data types, different templates)
        # G·ª£i √Ω lo·∫°i d·ªØ li·ªáu theo ng√†nh cho UI (c√πng data types, kh√°c template)
        self.industry_data_suggestions = {
            Industry.RESTAURANT: [
                IndustryDataType.COMPANY_INFO,
                IndustryDataType.PRODUCTS,  # Menu items for restaurant
                IndustryDataType.SERVICES,  # Restaurant services
                IndustryDataType.POLICIES,
                IndustryDataType.FAQ,
                IndustryDataType.KNOWLEDGE_BASE,
            ],
            Industry.HOTEL: [
                IndustryDataType.COMPANY_INFO,
                IndustryDataType.PRODUCTS,  # Room types for hotel
                IndustryDataType.SERVICES,  # Hotel services
                IndustryDataType.POLICIES,
                IndustryDataType.FAQ,
                IndustryDataType.KNOWLEDGE_BASE,
            ],
            Industry.BANKING: [
                IndustryDataType.COMPANY_INFO,
                IndustryDataType.PRODUCTS,  # Loan products for banking
                IndustryDataType.SERVICES,  # Banking services
                IndustryDataType.POLICIES,
                IndustryDataType.FAQ,
                IndustryDataType.KNOWLEDGE_BASE,
            ],
            Industry.INSURANCE: [
                IndustryDataType.COMPANY_INFO,
                IndustryDataType.PRODUCTS,  # Insurance products
                IndustryDataType.SERVICES,  # Insurance services
                IndustryDataType.POLICIES,
                IndustryDataType.FAQ,
                IndustryDataType.KNOWLEDGE_BASE,
            ],
            Industry.EDUCATION: [
                IndustryDataType.COMPANY_INFO,
                IndustryDataType.PRODUCTS,  # Courses for education
                IndustryDataType.SERVICES,  # Education services
                IndustryDataType.POLICIES,
                IndustryDataType.FAQ,
                IndustryDataType.KNOWLEDGE_BASE,
            ],
            # Default for all other industries
            "default": [
                IndustryDataType.COMPANY_INFO,
                IndustryDataType.PRODUCTS,
                IndustryDataType.SERVICES,
                IndustryDataType.POLICIES,
                IndustryDataType.FAQ,
                IndustryDataType.KNOWLEDGE_BASE,
            ],
        }

    async def register_company(self, company_data: Dict[str, Any]) -> CompanyConfig:
        """
        Register a new company for data management
        ƒêƒÉng k√Ω c√¥ng ty m·ªõi ƒë·ªÉ qu·∫£n l√Ω d·ªØ li·ªáu
        """
        try:
            # Extract metadata for company info indexing / Tr√≠ch xu·∫•t metadata ƒë·ªÉ index th√¥ng tin c√¥ng ty
            metadata = company_data.get("metadata", {})

            # Create company config / T·∫°o c·∫•u h√¨nh c√¥ng ty
            company_config = CompanyConfig(
                company_id=company_data.get("company_id", str(uuid.uuid4())),
                company_name=company_data["company_name"],
                industry=Industry(company_data["industry"]),
                languages=company_data.get("languages", [Language.VIETNAMESE]),
                qdrant_collection=f"company_{company_data.get('company_id', str(uuid.uuid4()))}_data",
                data_sources=company_data.get("data_sources", {}),
                ai_config=company_data.get("ai_config", {}),
                industry_config=company_data.get("industry_config", {}),
                business_hours=metadata.get("business_info", {}).get("business_hours"),
                contact_info={
                    "email": metadata.get("email"),
                    "phone": metadata.get("phone"),
                    "website": metadata.get("website"),
                    "address": metadata.get("location", {}).get("address"),
                    "social_links": metadata.get("social_links", {}),
                },
            )

            # Initialize Qdrant collection / Kh·ªüi t·∫°o collection Qdrant
            collection_name = await self.qdrant_service.initialize_company_collection(
                company_config
            )
            company_config.qdrant_collection = collection_name

            # Index company basic info to Qdrant / Index th√¥ng tin c∆° b·∫£n c√¥ng ty v√†o Qdrant
            await self._index_company_basic_info(company_config, metadata)

            # Create storage directory / T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ
            company_storage_path = self.storage_base_path / company_config.company_id
            company_storage_path.mkdir(parents=True, exist_ok=True)

            # Save to MongoDB / L∆∞u v√†o MongoDB
            success = self.company_db.save_company(company_config)
            if not success:
                self.logger.warning(
                    f"‚ö†Ô∏è Failed to save company to MongoDB: {company_config.company_id}"
                )
                # Continue anyway, we still have it in memory

            # Store company config in memory cache / L∆∞u c·∫•u h√¨nh c√¥ng ty v√†o cache memory
            self.companies[company_config.company_id] = company_config

            self.logger.info(
                f"‚úÖ Registered company: {company_config.company_name} ({company_config.company_id})"
            )

            return company_config

        except Exception as e:
            self.logger.error(f"‚ùå Failed to register company: {e}")
            raise e

    async def upload_company_file(
        self,
        company_id: str,
        file_name: str,
        file_content: bytes,
        backend_data_type: str,  # Changed from IndustryDataType to string
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Upload and store a company data file
        Upload v√† l∆∞u tr·ªØ file d·ªØ li·ªáu c√¥ng ty
        """
        try:
            # Validate company exists / Ki·ªÉm tra c√¥ng ty c√≥ t·ªìn t·∫°i
            if company_id not in self.companies:
                raise ValueError(f"Company {company_id} not found")

            company_config = self.companies[company_id]

            # Map backend data type to industry data type / √Ånh x·∫° data type t·ª´ backend sang industry data type
            industry_data_type = self.map_backend_data_type(
                backend_data_type, company_config.industry
            )

            # Generate file ID / T·∫°o ID file
            file_id = str(uuid.uuid4())

            # Determine file type / X√°c ƒë·ªãnh lo·∫°i file
            file_extension = Path(file_name).suffix.lower()
            file_type = self.file_type_mappings.get(file_extension, FileType.TEXT)

            # Create storage path / T·∫°o ƒë∆∞·ªùng d·∫´n l∆∞u tr·ªØ
            company_storage_path = self.storage_base_path / company_id
            file_storage_path = company_storage_path / f"{file_id}_{file_name}"

            # Save file to storage / L∆∞u file v√†o storage
            with open(file_storage_path, "wb") as f:
                f.write(file_content)

            # Extract file_name, description, tags from metadata / Tr√≠ch xu·∫•t file_name, description, tags t·ª´ metadata
            display_name = metadata.get("file_name") if metadata else None
            if not display_name:
                display_name = file_name

            # Create file record / T·∫°o record file
            file_data = CompanyDataFile(
                file_id=file_id,
                company_id=company_id,
                file_name=display_name,  # Use display name from metadata
                file_type=file_type,
                file_size=len(file_content),
                file_path=str(file_storage_path),
                industry=company_config.industry,
                data_type=industry_data_type,  # Use mapped industry data type
                extraction_status=DataExtractionStatus.PENDING,
                metadata={
                    **(metadata or {}),
                    "original_file_name": file_name,  # Keep original file name
                    "backend_data_type": backend_data_type,  # Keep backend data type for reference
                    "mapped_data_type": industry_data_type.value,  # Store mapped type
                    "tags": metadata.get("tags", []) if metadata else [],
                },
            )

            # Store file record / L∆∞u record file
            self.files[file_id] = file_data

            self.logger.info(
                f"üìÅ Uploaded file: {display_name} ({backend_data_type} -> {industry_data_type.value}) for company {company_id}"
            )

            # Return file info for response / Tr·∫£ v·ªÅ th√¥ng tin file cho response
            return {
                "file_id": file_id,
                "file_name": display_name,
                "original_name": file_name,
                "backend_data_type": backend_data_type,
                "mapped_data_type": industry_data_type.value,
                "file_size": len(file_content),
                "extraction_status": DataExtractionStatus.PENDING.value,
                "tags": metadata.get("tags", []) if metadata else [],
                "extracted_items": 0,  # Will be updated after processing
                "chunks_created": 0,  # Will be updated after processing
                "processing_time": 0,  # Will be updated after processing
            }

        except Exception as e:
            self.logger.error(f"‚ùå Failed to upload file {file_name}: {e}")
            raise e

    async def process_data_extraction(
        self, extraction_request: DataExtractionRequest
    ) -> Dict[str, Any]:
        """
        Process data extraction for uploaded files
        X·ª≠ l√Ω tr√≠ch xu·∫•t d·ªØ li·ªáu cho c√°c file ƒë√£ upload
        """
        try:
            results = []

            for file_id in extraction_request.file_ids:
                # Get file data / L·∫•y d·ªØ li·ªáu file
                if file_id not in self.files:
                    results.append(
                        {
                            "file_id": file_id,
                            "status": "error",
                            "error": "File not found",
                        }
                    )
                    continue

                file_data = self.files[file_id]

                # Update status to processing / C·∫≠p nh·∫≠t tr·∫°ng th√°i th√†nh ƒëang x·ª≠ l√Ω
                file_data.extraction_status = DataExtractionStatus.PROCESSING

                try:
                    # Read file content / ƒê·ªçc n·ªôi dung file
                    with open(file_data.file_path, "rb") as f:
                        file_content = f.read()

                    # Note: Data extraction is now handled directly in admin_routes.py using ai_extraction_service
                    # This legacy method is preserved for compatibility but not actively used
                    # Ghi ch√∫: Tr√≠ch xu·∫•t d·ªØ li·ªáu gi·ªù ƒë∆∞·ª£c x·ª≠ l√Ω tr·ª±c ti·∫øp trong admin_routes.py s·ª≠ d·ª•ng ai_extraction_service
                    # Method c≈© n√†y ƒë∆∞·ª£c gi·ªØ l·∫°i ƒë·ªÉ t∆∞∆°ng th√≠ch nh∆∞ng kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng t√≠ch c·ª±c

                    self.logger.warning(
                        f"‚ö†Ô∏è Legacy extraction method called for file {file_id}. Consider using ai_extraction_service directly."
                    )

                    # For now, mark as completed and skip actual processing
                    # Hi·ªán t·∫°i, ƒë√°nh d·∫•u l√† ho√†n th√†nh v√† b·ªè qua x·ª≠ l√Ω th·ª±c t·∫ø
                    file_data.extraction_status = DataExtractionStatus.COMPLETED
                    file_data.processed_at = datetime.now()

                    results.append(
                        {
                            "file_id": file_id,
                            "status": "skipped_legacy",
                            "message": "Use ai_extraction_service for actual processing",
                        }
                    )

                except Exception as e:
                    # Processing failed / X·ª≠ l√Ω th·∫•t b·∫°i
                    file_data.extraction_status = DataExtractionStatus.FAILED
                    file_data.extraction_error = str(e)

                    results.append(
                        {"file_id": file_id, "status": "error", "error": str(e)}
                    )

            self.logger.info(
                f"üîÑ Processed extraction for {len(extraction_request.file_ids)} files"
            )

            return {
                "status": "completed",
                "total_files": len(extraction_request.file_ids),
                "results": results,
                "processed_at": datetime.now().isoformat(),
            }

        except Exception as e:
            self.logger.error(f"‚ùå Data extraction processing failed: {e}")
            return {
                "status": "error",
                "error": str(e),
                "processed_at": datetime.now().isoformat(),
            }

    async def search_company_data(
        self,
        company_id: str,
        query: str,
        content_types: Optional[
            List[str]
        ] = None,  # Changed from IndustryDataType to str
        language: str = "vi",  # Changed from Language enum to string
        limit: int = 10,
        filter_tags: Optional[List[str]] = None,  # Added tags filtering
    ) -> List[Dict[str, Any]]:
        """
        Search company data using RAG
        T√¨m ki·∫øm d·ªØ li·ªáu c√¥ng ty s·ª≠ d·ª•ng RAG
        """
        try:
            # Validate company exists / Ki·ªÉm tra c√¥ng ty c√≥ t·ªìn t·∫°i
            if company_id not in self.companies:
                raise ValueError(f"Company {company_id} not found")

            company_config = self.companies[company_id]

            # Map backend content types to industry data types if provided
            mapped_content_types = None
            if content_types:
                mapped_content_types = []
                for content_type in content_types:
                    mapped_type = self.map_backend_data_type(
                        content_type, company_config.industry
                    )
                    mapped_content_types.append(mapped_type)

            # Convert language string to enum if needed
            language_enum = (
                Language.VIETNAMESE if language == "vi" else Language.ENGLISH
            )

            # Search in Qdrant / T√¨m ki·∫øm trong Qdrant
            search_results = await self.qdrant_service.search_company_data(
                company_id=company_id,
                query=query,
                industry=company_config.industry,
                content_types=mapped_content_types,
                language=language_enum,
                limit=limit,
                filter_tags=filter_tags,
            )

            return search_results

        except Exception as e:
            self.logger.error(f"‚ùå Company data search failed: {e}")
            return []

    async def get_company_stats(self, company_id: str) -> Dict[str, Any]:
        """
        Get comprehensive statistics for company data
        L·∫•y th·ªëng k√™ to√†n di·ªán cho d·ªØ li·ªáu c√¥ng ty
        """
        try:
            # Validate company exists / Ki·ªÉm tra c√¥ng ty c√≥ t·ªìn t·∫°i
            if company_id not in self.companies:
                raise ValueError(f"Company {company_id} not found")

            company_config = self.companies[company_id]

            # Get file statistics / L·∫•y th·ªëng k√™ file
            company_files = [
                f for f in self.files.values() if f.company_id == company_id
            ]

            file_stats = {
                "total_files": len(company_files),
                "processed_files": len(
                    [
                        f
                        for f in company_files
                        if f.extraction_status == DataExtractionStatus.COMPLETED
                    ]
                ),
                "failed_files": len(
                    [
                        f
                        for f in company_files
                        if f.extraction_status == DataExtractionStatus.FAILED
                    ]
                ),
                "total_file_size": sum(f.file_size for f in company_files),
                "pending_files": len(
                    [
                        f
                        for f in company_files
                        if f.extraction_status == DataExtractionStatus.PENDING
                    ]
                ),
                "processing_files": len(
                    [
                        f
                        for f in company_files
                        if f.extraction_status == DataExtractionStatus.PROCESSING
                    ]
                ),
            }

            # Get Qdrant statistics / L·∫•y th·ªëng k√™ Qdrant
            try:
                qdrant_stats = await self.qdrant_service.get_company_data_stats(
                    company_id
                )
                file_stats.update(
                    {
                        "total_chunks": qdrant_stats.get("total_chunks", 0),
                        "vector_points": qdrant_stats.get("total_chunks", 0),
                        "storage_used_mb": round(
                            file_stats["total_file_size"] / (1024 * 1024), 2
                        ),
                    }
                )
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Could not get Qdrant stats: {e}")
                file_stats.update(
                    {
                        "total_chunks": 0,
                        "vector_points": 0,
                        "storage_used_mb": round(
                            file_stats["total_file_size"] / (1024 * 1024), 2
                        ),
                    }
                )

            # Get data types distribution / Ph√¢n b·ªë lo·∫°i d·ªØ li·ªáu
            data_types = {}
            for file in company_files:
                backend_type = file.metadata.get("backend_data_type", "other")
                data_types[backend_type] = data_types.get(backend_type, 0) + 1

            # Get tags statistics / Th·ªëng k√™ tags
            all_tags = {}
            for file in company_files:
                file_tags = file.metadata.get("tags", [])
                for tag in file_tags:
                    if tag not in all_tags:
                        all_tags[tag] = {
                            "name": tag,
                            "file_count": 0,
                            "created_at": file.created_at.isoformat(),
                            "last_used": (
                                file.updated_at.isoformat()
                                if file.updated_at
                                else file.created_at.isoformat()
                            ),
                        }
                    all_tags[tag]["file_count"] += 1

            # Sort tags by usage / S·∫Øp x·∫øp tags theo m·ª©c s·ª≠ d·ª•ng
            most_used_tags = sorted(
                [
                    {"name": tag, "file_count": info["file_count"]}
                    for tag, info in all_tags.items()
                ],
                key=lambda x: x["file_count"],
                reverse=True,
            )[:3]

            # Get company info status / Tr·∫°ng th√°i th√¥ng tin c√¥ng ty
            company_info_indexed = any(
                f.metadata.get("backend_data_type") == "company_info"
                for f in company_files
            ) or company_config.contact_info.get(
                "email"
            )  # Basic info from registration

            file_stats.update(
                {
                    "last_activity": max(
                        [f.updated_at or f.created_at for f in company_files],
                        default=company_config.created_at,
                    ).isoformat(),
                    "data_types": data_types,
                    "total_tags": len(all_tags),
                    "most_used_tags": most_used_tags,
                    "company_info_indexed": company_info_indexed,
                    "company_info_last_updated": (
                        company_config.updated_at.isoformat()
                        if company_config.updated_at
                        else company_config.created_at.isoformat()
                    ),
                    "language": (
                        company_config.languages[0].value
                        if company_config.languages
                        else "vi"
                    ),
                    "timezone": "Asia/Ho_Chi_Minh",  # Default timezone
                }
            )

            return file_stats

        except Exception as e:
            self.logger.error(f"‚ùå Failed to get company stats: {e}")
            return {
                "total_files": 0,
                "processed_files": 0,
                "failed_files": 0,
                "total_chunks": 0,
                "vector_points": 0,
                "storage_used_mb": 0,
                "data_types": {},
                "total_tags": 0,
                "most_used_tags": [],
                "company_info_indexed": False,
            }

    def get_company_files(self, company_id: str) -> List[CompanyDataFile]:
        """
        Get all files for a company
        L·∫•y t·∫•t c·∫£ file c·ªßa m·ªôt c√¥ng ty
        """
        return [f for f in self.files.values() if f.company_id == company_id]

    def get_file_by_id(self, file_id: str) -> Optional[CompanyDataFile]:
        """
        Get file by ID
        L·∫•y file theo ID
        """
        return self.files.get(file_id)

    def get_company_by_id(self, company_id: str) -> Optional[CompanyConfig]:
        """
        Get company configuration by ID
        L·∫•y c·∫•u h√¨nh c√¥ng ty theo ID
        """
        # First check memory cache
        if company_id in self.companies:
            return self.companies[company_id]

        # If not in cache, try to load from MongoDB
        try:
            company_config = self.company_db.get_company(company_id)
            if company_config:
                # Add to cache for future access
                self.companies[company_id] = company_config
                self.logger.info(f"‚úÖ Loaded company from MongoDB: {company_id}")
                return company_config
        except Exception as e:
            self.logger.error(f"‚ùå Failed to load company from MongoDB: {e}")

        return None

    def get_industry_data_types(self, industry: Industry) -> List[IndustryDataType]:
        """
        Get suggested data types for an industry (same types, different descriptions)
        L·∫•y c√°c lo·∫°i d·ªØ li·ªáu ƒë∆∞·ª£c g·ª£i √Ω cho m·ªôt ng√†nh (c√πng types, kh√°c m√¥ t·∫£)
        """
        return self.industry_data_suggestions.get(
            industry, self.industry_data_suggestions["default"]
        )

    async def delete_company_file(self, file_id: str) -> Dict[str, Any]:
        """
        Delete a company file and its data
        X√≥a file c√¥ng ty v√† d·ªØ li·ªáu c·ªßa n√≥
        """
        try:
            if file_id not in self.files:
                return {"status": "error", "error": "File not found"}

            file_data = self.files[file_id]

            # Delete from Qdrant / X√≥a kh·ªèi Qdrant
            await self.qdrant_service.delete_company_data(
                company_id=file_data.company_id, file_ids=[file_id]
            )

            # Delete physical file / X√≥a file v·∫≠t l√Ω
            try:
                os.remove(file_data.file_path)
            except OSError:
                pass  # File might already be deleted / File c√≥ th·ªÉ ƒë√£ b·ªã x√≥a

            # Get file info for response / L·∫•y th√¥ng tin file cho response
            chunks_deleted = file_data.processed_chunks or 0
            tags_removed = file_data.metadata.get("tags", [])

            # Remove from records / X√≥a kh·ªèi records
            del self.files[file_id]

            self.logger.info(f"üóëÔ∏è Deleted file {file_id}")

            return {
                "status": "success",
                "file_id": file_id,
                "file_name": file_data.file_name,
                "chunks_deleted": chunks_deleted,
                "vector_points_removed": chunks_deleted,
                "tags_removed": tags_removed,
            }

        except Exception as e:
            self.logger.error(f"‚ùå Failed to delete file {file_id}: {e}")
            return {"status": "error", "error": str(e)}

    async def update_company_config(
        self, company_id: str, updates: Dict[str, Any]
    ) -> CompanyConfig:
        """
        Update company configuration
        C·∫≠p nh·∫≠t c·∫•u h√¨nh c√¥ng ty
        """
        try:
            if company_id not in self.companies:
                raise ValueError(f"Company {company_id} not found")

            company_config = self.companies[company_id]

            # Update fields / C·∫≠p nh·∫≠t c√°c field
            for field, value in updates.items():
                if hasattr(company_config, field):
                    setattr(company_config, field, value)

            company_config.updated_at = datetime.now()

            self.logger.info(f"‚úÖ Updated company config for {company_id}")

            return company_config

        except Exception as e:
            self.logger.error(f"‚ùå Failed to update company config: {e}")
            raise e

    def map_backend_data_type(
        self, backend_data_type: str, industry: Industry
    ) -> IndustryDataType:
        """
        Map backend DataType to IndustryDataType (unified for all industries)
        √Ånh x·∫° DataType t·ª´ backend sang IndustryDataType (th·ªëng nh·∫•t cho t·∫•t c·∫£ ng√†nh)

        All industries use the same data types:
        - company_info -> COMPANY_INFO
        - products -> PRODUCTS (menu for restaurant, rooms for hotel, loans for bank, etc.)
        - services -> SERVICES (restaurant services, hotel services, bank services, etc.)
        - policies -> POLICIES
        - faq -> FAQ
        - knowledge_base -> KNOWLEDGE_BASE
        - other -> OTHER

        The difference is in extraction templates and AI processing, not data types.
        """
        return self.backend_data_type_mapping.get(
            backend_data_type, IndustryDataType.OTHER
        )

    def get_supported_backend_data_types(self) -> List[str]:
        """
        Get list of supported backend data types
        L·∫•y danh s√°ch c√°c data type ƒë∆∞·ª£c h·ªó tr·ª£ t·ª´ backend
        """
        return list(self.backend_data_type_mapping.keys())

    async def _index_company_basic_info(
        self, company_config: CompanyConfig, metadata: Dict[str, Any]
    ):
        """
        Index company basic information to Qdrant for AI reference
        Index th√¥ng tin c∆° b·∫£n c√¥ng ty v√†o Qdrant ƒë·ªÉ AI tham kh·∫£o
        """
        try:
            # Create company info document / T·∫°o t√†i li·ªáu th√¥ng tin c√¥ng ty
            company_info_text = self._build_company_info_text(company_config, metadata)

            # Create document chunk for company info / T·∫°o chunk t√†i li·ªáu cho th√¥ng tin c√¥ng ty
            company_info_chunk = QdrantDocumentChunk(
                chunk_id=str(uuid.uuid4()),
                file_id="company_basic_info",
                file_name="Company Basic Information",
                company_id=company_config.company_id,
                industry=company_config.industry,
                data_type=IndustryDataType.COMPANY_INFO,
                content=company_info_text,
                content_type=IndustryDataType.COMPANY_INFO,  # Add required field
                language=Language.VIETNAMESE,
                chunk_index=0,
                total_chunks=1,
                metadata={
                    "data_source": "backend_registration",
                    "priority": "high",  # Cho AI ∆∞u ti√™n th√¥ng tin c√¥ng ty
                    "auto_generated": True,
                    "last_updated": datetime.now().isoformat(),
                    **metadata,
                },
            )

            # Add to Qdrant / Th√™m v√†o Qdrant
            await self.qdrant_service.add_document_chunks(
                chunks=[company_info_chunk], company_id=company_config.company_id
            )

            self.logger.info(
                f"üìÑ Indexed company basic info for {company_config.company_id}"
            )

        except Exception as e:
            self.logger.error(f"‚ùå Failed to index company basic info: {e}")
            # Don't fail the entire registration process / Kh√¥ng l√†m th·∫•t b·∫°i to√†n b·ªô qu√° tr√¨nh ƒëƒÉng k√Ω

    def _build_company_info_text(
        self, company_config: CompanyConfig, metadata: Dict[str, Any]
    ) -> str:
        """
        Build structured text from company information for AI processing
        X√¢y d·ª±ng vƒÉn b·∫£n c√≥ c·∫•u tr√∫c t·ª´ th√¥ng tin c√¥ng ty ƒë·ªÉ AI x·ª≠ l√Ω
        """
        info_parts = []

        # Company name and industry / T√™n c√¥ng ty v√† ng√†nh
        info_parts.append(f"T√™n c√¥ng ty: {company_config.company_name}")
        info_parts.append(f"Ng√†nh ngh·ªÅ: {company_config.industry.value}")

        # Contact information / Th√¥ng tin li√™n h·ªá
        if metadata.get("email"):
            info_parts.append(f"Email: {metadata['email']}")
        if metadata.get("phone"):
            info_parts.append(f"S·ªë ƒëi·ªán tho·∫°i: {metadata['phone']}")
        if metadata.get("website"):
            info_parts.append(f"Website: {metadata['website']}")

        # Address / ƒê·ªãa ch·ªâ
        location = metadata.get("location", {})
        if location.get("address"):
            info_parts.append(f"ƒê·ªãa ch·ªâ: {location['address']}")
        if location.get("city"):
            info_parts.append(f"Th√†nh ph·ªë: {location['city']}")
        if location.get("country"):
            info_parts.append(f"Qu·ªëc gia: {location['country']}")

        # Description / M√¥ t·∫£
        if metadata.get("description"):
            info_parts.append(f"M√¥ t·∫£: {metadata['description']}")

        # Social links / Li√™n k·∫øt x√£ h·ªôi
        social_links = metadata.get("social_links", {})
        if social_links:
            social_info = []
            for platform, link in social_links.items():
                if link:
                    social_info.append(f"{platform}: {link}")
            if social_info:
                info_parts.append(f"M·∫°ng x√£ h·ªôi: {', '.join(social_info)}")

        # Business info / Th√¥ng tin kinh doanh
        business_info = metadata.get("business_info", {})
        if business_info.get("language"):
            info_parts.append(f"Ng√¥n ng·ªØ h·ªó tr·ª£: {business_info['language']}")
        if business_info.get("timezone"):
            info_parts.append(f"M√∫i gi·ªù: {business_info['timezone']}")

        return "\n".join(info_parts)

    async def update_company_basic_info(
        self, company_id: str, updates: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Update company basic information and refresh in Qdrant
        C·∫≠p nh·∫≠t th√¥ng tin c∆° b·∫£n c√¥ng ty v√† l√†m m·ªõi trong Qdrant
        """
        try:
            if company_id not in self.companies:
                raise ValueError(f"Company {company_id} not found")

            company_config = self.companies[company_id]
            updated_fields = []

            # Update company name / C·∫≠p nh·∫≠t t√™n c√¥ng ty
            if updates.get("company_name"):
                company_config.company_name = updates["company_name"]
                updated_fields.append("company_name")

            # Update industry / C·∫≠p nh·∫≠t ng√†nh ngh·ªÅ
            if updates.get("industry"):
                try:
                    # Validate industry enum
                    from src.models.unified_models import Industry

                    industry_value = updates["industry"].lower()
                    if industry_value in [industry.value for industry in Industry]:
                        company_config.industry = Industry(industry_value)
                        updated_fields.append("industry")
                    else:
                        self.logger.warning(f"Invalid industry value: {industry_value}")
                except Exception as e:
                    self.logger.warning(f"Failed to update industry: {e}")

            # Update contact info from metadata / C·∫≠p nh·∫≠t th√¥ng tin li√™n h·ªá t·ª´ metadata
            metadata = updates.get("metadata", {})
            if metadata:
                if metadata.get("email"):
                    company_config.contact_info["email"] = metadata["email"]
                    updated_fields.append("email")
                if metadata.get("phone"):
                    company_config.contact_info["phone"] = metadata["phone"]
                    updated_fields.append("phone")
                if metadata.get("website"):
                    company_config.contact_info["website"] = metadata["website"]
                    updated_fields.append("website")

                # Update description / C·∫≠p nh·∫≠t m√¥ t·∫£
                if metadata.get("description"):
                    company_config.contact_info["description"] = metadata["description"]
                    updated_fields.append("description")

                # Update location / C·∫≠p nh·∫≠t ƒë·ªãa ch·ªâ
                location = metadata.get("location", {})
                if location:
                    if location.get("address"):
                        company_config.contact_info["address"] = location["address"]
                        updated_fields.append("address")
                    if location.get("country"):
                        company_config.contact_info["country"] = location["country"]
                        updated_fields.append("country")
                    if location.get("city"):
                        company_config.contact_info["city"] = location["city"]
                        updated_fields.append("city")

                # Update social links / C·∫≠p nh·∫≠t li√™n k·∫øt x√£ h·ªôi
                social_links = metadata.get("social_links", {})
                if social_links:
                    company_config.contact_info["social_links"].update(social_links)
                    updated_fields.append("social_links")

                # Update language and timezone from business_info
                business_info = metadata.get("business_info", {})
                if business_info:
                    if business_info.get("language"):
                        # Store in contact_info or add separate field if needed
                        company_config.contact_info["language"] = business_info[
                            "language"
                        ]
                        updated_fields.append("language")
                    if business_info.get("timezone"):
                        company_config.contact_info["timezone"] = business_info[
                            "timezone"
                        ]
                        updated_fields.append("timezone")
                    if business_info.get("owner_firebase_uid"):
                        company_config.contact_info["owner_firebase_uid"] = (
                            business_info["owner_firebase_uid"]
                        )
                        updated_fields.append("owner_firebase_uid")
                    if business_info.get("created_at"):
                        company_config.contact_info["created_at"] = business_info[
                            "created_at"
                        ]
                        updated_fields.append("created_at")

            # Remove old company info from Qdrant / X√≥a th√¥ng tin c√¥ng ty c≈© kh·ªèi Qdrant
            await self.qdrant_service.delete_company_data(
                company_id=company_id, file_ids=["company_basic_info"]
            )

            # Re-index updated company info / Index l·∫°i th√¥ng tin c√¥ng ty ƒë√£ c·∫≠p nh·∫≠t
            await self._index_company_basic_info(company_config, metadata)

            company_config.updated_at = datetime.now()

            # Save updated company to MongoDB / L∆∞u c√¥ng ty ƒë√£ c·∫≠p nh·∫≠t v√†o MongoDB
            if updated_fields:
                try:
                    await self.company_db_service.save_company(company_config)
                    self.logger.info(
                        f"üíæ Saved updated company to MongoDB: {company_id}"
                    )
                except Exception as e:
                    self.logger.warning(f"Failed to save company to MongoDB: {e}")

            self.logger.info(
                f"‚úÖ Updated company basic info for {company_id}: {updated_fields}"
            )

            return {
                "company_id": company_id,
                "updated_fields": updated_fields,
                "vector_points_updated": 1,  # Company info is 1 vector point
            }

        except Exception as e:
            self.logger.error(f"‚ùå Failed to update company basic info: {e}")
            raise e

    async def delete_files_by_tag(
        self, company_id: str, tag_name: str
    ) -> Dict[str, Any]:
        """
        Delete all files with a specific tag
        X√≥a t·∫•t c·∫£ files c√≥ tag c·ª• th·ªÉ
        """
        try:
            # Validate company exists / Ki·ªÉm tra c√¥ng ty c√≥ t·ªìn t·∫°i
            if company_id not in self.companies:
                raise ValueError(f"Company {company_id} not found")

            # Find files with the tag / T√¨m files c√≥ tag
            company_files = [
                f for f in self.files.values() if f.company_id == company_id
            ]
            files_to_delete = []

            for file in company_files:
                file_tags = file.metadata.get("tags", [])
                if tag_name in file_tags:
                    files_to_delete.append(file)

            if not files_to_delete:
                return {
                    "tag_name": tag_name,
                    "deleted_files": [],
                    "total_files_deleted": 0,
                    "total_chunks_deleted": 0,
                    "total_vector_points_removed": 0,
                }

            # Delete files one by one / X√≥a t·ª´ng file m·ªôt
            deleted_files = []
            total_chunks_deleted = 0
            total_vector_points_removed = 0

            for file in files_to_delete:
                # Delete from Qdrant / X√≥a kh·ªèi Qdrant
                await self.qdrant_service.delete_company_data(
                    company_id=company_id, file_ids=[file.file_id]
                )

                # Delete physical file / X√≥a file v·∫≠t l√Ω
                try:
                    os.remove(file.file_path)
                except OSError:
                    pass  # File might already be deleted

                # Count chunks / ƒê·∫øm chunks
                chunks_count = file.processed_chunks or 0
                total_chunks_deleted += chunks_count
                total_vector_points_removed += chunks_count

                # Add to deleted list / Th√™m v√†o danh s√°ch ƒë√£ x√≥a
                deleted_files.append(
                    {
                        "file_id": file.file_id,
                        "file_name": file.file_name,
                        "chunks_deleted": chunks_count,
                        "vector_points_removed": chunks_count,
                    }
                )

                # Remove from records / X√≥a kh·ªèi records
                del self.files[file.file_id]

            self.logger.info(
                f"üóëÔ∏è Deleted {len(deleted_files)} files with tag '{tag_name}' for company {company_id}"
            )

            return {
                "tag_name": tag_name,
                "deleted_files": deleted_files,
                "total_files_deleted": len(deleted_files),
                "total_chunks_deleted": total_chunks_deleted,
                "total_vector_points_removed": total_vector_points_removed,
            }

        except Exception as e:
            self.logger.error(f"‚ùå Failed to delete files by tag: {e}")
            raise e

    async def get_files_by_tag(
        self, company_id: str, tag_name: str
    ) -> List[Dict[str, Any]]:
        """
        Get all files with a specific tag
        L·∫•y t·∫•t c·∫£ files c√≥ tag c·ª• th·ªÉ
        """
        try:
            # Validate company exists / Ki·ªÉm tra c√¥ng ty c√≥ t·ªìn t·∫°i
            if company_id not in self.companies:
                raise ValueError(f"Company {company_id} not found")

            # Find files with the tag / T√¨m files c√≥ tag
            company_files = [
                f for f in self.files.values() if f.company_id == company_id
            ]
            tagged_files = []

            for file in company_files:
                file_tags = file.metadata.get("tags", [])
                if tag_name in file_tags:
                    tagged_files.append(
                        {
                            "file_id": file.file_id,
                            "file_name": file.file_name,
                            "original_name": file.metadata.get(
                                "original_file_name", file.file_name
                            ),
                            "status": file.extraction_status.value,
                            "uploaded_at": file.created_at.isoformat(),
                            "chunks_count": file.processed_chunks or 0,
                            "tags": file_tags,
                        }
                    )

            return tagged_files

        except Exception as e:
            self.logger.error(f"‚ùå Failed to get files by tag: {e}")
            return []

    async def get_company_tags(self, company_id: str) -> List[Dict[str, Any]]:
        """
        Get all tags for a company with statistics
        L·∫•y t·∫•t c·∫£ tags c·ªßa c√¥ng ty v·ªõi th·ªëng k√™
        """
        try:
            # Validate company exists / Ki·ªÉm tra c√¥ng ty c√≥ t·ªìn t·∫°i
            if company_id not in self.companies:
                raise ValueError(f"Company {company_id} not found")

            # Collect all tags / Thu th·∫≠p t·∫•t c·∫£ tags
            company_files = [
                f for f in self.files.values() if f.company_id == company_id
            ]
            tags_info = {}

            for file in company_files:
                file_tags = file.metadata.get("tags", [])
                for tag in file_tags:
                    if tag not in tags_info:
                        tags_info[tag] = {
                            "name": tag,
                            "file_count": 0,
                            "created_at": file.created_at.isoformat(),
                            "last_used": file.created_at.isoformat(),
                        }

                    tags_info[tag]["file_count"] += 1

                    # Update last used time / C·∫≠p nh·∫≠t th·ªùi gian s·ª≠ d·ª•ng cu·ªëi
                    file_time = file.updated_at or file.created_at
                    if file_time.isoformat() > tags_info[tag]["last_used"]:
                        tags_info[tag]["last_used"] = file_time.isoformat()

            # Convert to list and sort by usage / Chuy·ªÉn th√†nh list v√† s·∫Øp x·∫øp theo usage
            tags_list = list(tags_info.values())
            tags_list.sort(key=lambda x: x["file_count"], reverse=True)

            return tags_list

        except Exception as e:
            self.logger.error(f"‚ùå Failed to get company tags: {e}")
            return []

    def _load_companies_from_db(self):
        """
        Load all companies from MongoDB into memory cache
        Load t·∫•t c·∫£ c√¥ng ty t·ª´ MongoDB v√†o cache memory
        """
        try:
            companies_data = self.company_db.list_companies(limit=1000)

            for company_data in companies_data:
                company_config = self.company_db.get_company(company_data["company_id"])
                if company_config:
                    self.companies[company_config.company_id] = company_config

            self.logger.info(f"‚úÖ Loaded {len(self.companies)} companies from MongoDB")

        except Exception as e:
            self.logger.error(f"‚ùå Failed to load companies from MongoDB: {e}")
            # Continue without companies if database fails
            pass


# Global service instance / Instance service to√†n c·ª•c
company_data_manager = None


def get_company_data_manager() -> CompanyDataManager:
    """Get company data manager instance / L·∫•y instance manager d·ªØ li·ªáu c√¥ng ty"""
    global company_data_manager
    if company_data_manager is None:
        from src.core.config import APP_CONFIG

        storage_path = APP_CONFIG.get("data_dir", "./data") + "/companies"
        company_data_manager = CompanyDataManager(storage_base_path=storage_path)

    return company_data_manager
