import openai
from typing import List, Dict, AsyncGenerator
import json
import asyncio
from src.utils.logger import setup_logger

logger = setup_logger()


class ChatGPTClient:
    def __init__(self, api_key: str, model: str = "gpt-4o"):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
        self.reasoning_model = "o1-preview"  # Text-only reasoning
        self.vision_reasoning_model = "gpt-4o"  # Multimodal reasoning
        self.logger = logger

        # Test API key
        try:
            test_response = self.client.models.list()
            self.logger.debug(
                f"ChatGPT client initialized successfully. Available models: {len(test_response.data)}"
            )
        except Exception as e:
            self.logger.error(f"ChatGPT client initialization failed: {e}")

    def _should_use_vision_model(self, messages: List[Dict]) -> bool:
        """Kiá»ƒm tra cÃ³ cáº§n dÃ¹ng vision model khÃ´ng"""
        for message in messages:
            if isinstance(message.get("content"), list):
                for content in message["content"]:
                    if content.get("type") == "image_url":
                        return True
        return False

    def _prepare_messages_for_o1(self, messages: List[Dict]) -> List[Dict]:
        """Chuáº©n bá»‹ messages cho o1 model (text-only, khÃ´ng há»— trá»£ system role)"""
        prepared_messages = []
        system_content = ""

        for message in messages:
            if message["role"] == "system":
                # Gá»™p system prompt vÃ o user message Ä‘áº§u tiÃªn
                system_content = message["content"]
            elif message["role"] == "user":
                if isinstance(message["content"], list):
                    # Extract text only tá»« multimodal content
                    text_content = ""
                    for item in message["content"]:
                        if item["type"] == "text":
                            text_content = item["text"]
                            break

                    if system_content:
                        combined_content = f"{system_content}\n\n{text_content}"
                        system_content = ""
                    else:
                        combined_content = text_content

                    prepared_messages.append(
                        {"role": "user", "content": combined_content}
                    )
                else:
                    # Text only content
                    if system_content:
                        combined_content = f"{system_content}\n\n{message['content']}"
                        system_content = ""
                    else:
                        combined_content = message["content"]

                    prepared_messages.append(
                        {"role": "user", "content": combined_content}
                    )
            else:
                prepared_messages.append(message)

        return prepared_messages

    async def chat_completion_stream_with_reasoning(
        self, messages: List[Dict], use_reasoning: bool = False
    ) -> AsyncGenerator[str, None]:
        """Streaming vá»›i reasoning support - Sá»­ dá»¥ng gpt-4-vision-preview cho multimodal reasoning"""
        try:
            has_images = self._should_use_vision_model(messages)

            # Smart model selection
            if use_reasoning:
                if has_images:
                    # Use gpt-4-vision-preview for multimodal reasoning
                    model_to_use = self.vision_reasoning_model
                    prepared_messages = self._enhance_messages_for_vision_reasoning(
                        messages
                    )
                    is_streaming = True
                    self.logger.info(f"Using {model_to_use} for multimodal reasoning")
                else:
                    # Use o1-preview for text-only reasoning
                    model_to_use = self.reasoning_model
                    prepared_messages = self._prepare_messages_for_o1(messages)
                    is_streaming = False  # o1 doesn't support streaming
                    self.logger.info(f"Using {model_to_use} for text-only reasoning")
            else:
                # Standard mode
                model_to_use = self.model
                prepared_messages = messages
                is_streaming = True

            self.logger.info(
                f"ChatGPT request - Model: {model_to_use}, Reasoning: {use_reasoning}, Has Images: {has_images}, Messages: {len(prepared_messages)}"
            )

            if not is_streaming:
                # o1 models khÃ´ng há»— trá»£ streaming
                result = await self._chat_completion_reasoning(prepared_messages)
                # Simulate streaming
                chunk_size = 50
                for i in range(0, len(result), chunk_size):
                    chunk = result[i : i + chunk_size]
                    yield chunk
                    await asyncio.sleep(0.05)
            else:
                # Regular streaming cho gpt-4o vÃ  gpt-4-vision-preview
                stream = self.client.chat.completions.create(
                    model=model_to_use,
                    messages=prepared_messages,
                    max_tokens=4000,
                    temperature=0.2,
                    stream=True,
                )

                chunk_count = 0
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        chunk_count += 1
                        yield chunk.choices[0].delta.content

                self.logger.info(
                    f"ChatGPT streaming completed. Total chunks: {chunk_count}"
                )

        except Exception as e:
            self.logger.error(f"ChatGPT streaming error: {e}")
            import traceback

            self.logger.error(f"Full traceback: {traceback.format_exc()}")
            yield "Xin lá»—i, tÃ´i Ä‘ang gáº·p sá»± cá»‘ khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n."

    def _enhance_messages_for_vision_reasoning(
        self, messages: List[Dict]
    ) -> List[Dict]:
        """Enhance messages cho gpt-4-vision-preview vá»›i reasoning prompt"""
        enhanced_messages = []

        for msg in messages:
            if msg["role"] == "system":
                # Enhanced system prompt cho vision reasoning
                enhanced_content = (
                    msg["content"]
                    + """

                ðŸ§  PHÆ¯Æ NG PHÃP REASONING CHO Äá»ŠNH GIÃ Báº¤T Äá»˜NG Sáº¢N Vá»šI HÃŒNH áº¢NH:

                HÃ£y thá»±c hiá»‡n reasoning tá»«ng bÆ°á»›c chi tiáº¿t:

                ðŸ” BÆ¯á»šC 1: QUAN SÃT HÃŒNH áº¢NH CHI TIáº¾T
                - Äá»c vÃ  phÃ¢n tÃ­ch táº¥t cáº£ vÄƒn báº£n trong hÃ¬nh áº£nh
                - TrÃ­ch xuáº¥t thÃ´ng tin: Ä‘á»‹a chá»‰, diá»‡n tÃ­ch, loáº¡i hÃ¬nh, phÃ¡p lÃ½
                - ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng vÃ  tÃ­nh Ä‘áº§y Ä‘á»§ cá»§a tÃ i liá»‡u
                - Ghi nháº­n cÃ¡c chi tiáº¿t Ä‘áº·c biá»‡t áº£nh hÆ°á»Ÿng Ä‘áº¿n giÃ¡ trá»‹

                ðŸ“Š BÆ¯á»šC 2: PHÃ‚N TÃCH THá»Š TRÆ¯á»œNG 2025
                - So sÃ¡nh vá»›i giÃ¡ khu vá»±c tÆ°Æ¡ng tá»±
                - ÄÃ¡nh giÃ¡ xu hÆ°á»›ng thá»‹ trÆ°á»ng báº¥t Ä‘á»™ng sáº£n
                - PhÃ¢n tÃ­ch tiá»m nÄƒng phÃ¡t triá»ƒn khu vá»±c
                - Xem xÃ©t cÃ¡c yáº¿u tá»‘ vÄ© mÃ´ áº£nh hÆ°á»Ÿng

                ðŸ’° BÆ¯á»šC 3: TÃNH TOÃN GIÃ TRá»Š CHÃNH XÃC
                - Æ¯á»›c tÃ­nh giÃ¡ trá»‹ thá»‹ trÆ°á»ng hiá»‡n táº¡i
                - TÃ­nh giÃ¡ trá»‹ tháº©m Ä‘á»‹nh ngÃ¢n hÃ ng (LTV 70-80%)
                - XÃ¡c Ä‘á»‹nh sá»‘ tiá»n vay tháº¿ cháº¥p tá»‘i Ä‘a
                - So sÃ¡nh vá»›i nhiá»u phÆ°Æ¡ng phÃ¡p Ä‘á»‹nh giÃ¡

                âš ï¸ BÆ¯á»šC 4: ÄÃNH GIÃ Rá»¦I RO TOÃ€N DIá»†N
                - Rá»§i ro phÃ¡p lÃ½ vÃ  tÃ­nh thanh khoáº£n
                - Rá»§i ro biáº¿n Ä‘á»™ng giÃ¡ vÃ  lÃ£i suáº¥t
                - Rá»§i ro tÃ­n dá»¥ng vÃ  kháº£ nÄƒng thu há»“i ná»£
                - ÄÃ¡nh giÃ¡ tÃ¡c Ä‘á»™ng kinh táº¿ vÄ© mÃ´

                ðŸŽ¯ BÆ¯á»šC 5: KHUYáº¾N NGHá»Š CHIáº¾N LÆ¯á»¢C
                - PhÆ°Æ¡ng thá»©c tÃ i trá»£ tá»‘i Æ°u
                - Káº¿ hoáº¡ch tháº©m Ä‘á»‹nh thá»±c táº¿
                - Chiáº¿n lÆ°á»£c quáº£n lÃ½ rá»§i ro
                - Timeline thá»±c hiá»‡n cá»¥ thá»ƒ

                ðŸ“‹ YÃŠU Cáº¦U Äáº¦U RA:
                - TrÃ¬nh bÃ y reasoning logic tá»«ng bÆ°á»›c
                - Cung cáº¥p sá»‘ liá»‡u Ä‘á»‹nh lÆ°á»£ng cá»¥ thá»ƒ
                - ÄÆ°a ra khuyáº¿n nghá»‹ thá»±c táº¿ vÃ  kháº£ thi
                - Highlight cÃ¡c Ä‘iá»ƒm quan trá»ng cáº§n lÆ°u Ã½
                """
                )
                enhanced_messages.append(
                    {"role": "system", "content": enhanced_content}
                )
            else:
                enhanced_messages.append(msg)

        return enhanced_messages

    async def _chat_completion_reasoning(self, messages: List[Dict]) -> str:
        """Non-streaming completion cho o1 reasoning model"""
        try:
            # o1 models cÃ³ parameters khÃ¡c
            response = self.client.chat.completions.create(
                model=self.reasoning_model,
                messages=messages,
                # KhÃ´ng cÃ³ temperature, max_tokens cho o1
            )

            result = response.choices[0].message.content
            self.logger.info(f"ChatGPT reasoning response length: {len(result)}")
            return result

        except Exception as e:
            self.logger.error(f"ChatGPT reasoning error: {e}")
            raise e

    # BACKWARD COMPATIBILITY METHODS
    async def chat_completion(self, messages: List[Dict]) -> str:
        """Original chat completion method"""
        try:
            model_to_use = self.model
            if self._should_use_vision_model(messages):
                model_to_use = self.model

            self.logger.info(
                f"ChatGPT request - Model: {model_to_use}, Messages count: {len(messages)}"
            )

            response = self.client.chat.completions.create(
                model=model_to_use, messages=messages, max_tokens=4000, temperature=0.2
            )

            result = response.choices[0].message.content
            self.logger.info(f"ChatGPT response length: {len(result)}")
            return result

        except Exception as e:
            self.logger.error(f"ChatGPT API error: {e}")
            import traceback

            self.logger.error(f"Full traceback: {traceback.format_exc()}")
            raise e

    async def chat_completion_stream(
        self, messages: List[Dict]
    ) -> AsyncGenerator[str, None]:
        """Original streaming method"""
        async for chunk in self.chat_completion_stream_with_reasoning(
            messages, use_reasoning=False
        ):
            yield chunk
