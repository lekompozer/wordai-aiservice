"""
PDF Document Routes - Upload, Split, Merge, and AI Conversion
"""

import os
import logging
import tempfile
import uuid
from datetime import datetime
from typing import Optional
from fastapi import APIRouter, File, UploadFile, HTTPException, Depends, Form, Query
from fastapi.responses import JSONResponse

from src.models.pdf_models import (
    UploadPDFResponse,
    PreviewSplitResponse,
    SplitSuggestion,
    SplitDocumentRequest,
    SplitDocumentResponse,
    SplitDocumentPart,
    ConvertWithAIRequest,
    ConvertWithAIResponse,
    ConvertJobStartResponse,
    ConvertJobStatusResponse,
    SplitInfoResponse,
    SplitInfoDetail,
    SplitInfoSibling,
    MergeDocumentsRequest,
    MergeDocumentsResponse,
    MergedFromInfo,
)
from src.services.pdf_split_service import get_pdf_split_service
from src.services.pdf_ai_processor import get_pdf_ai_processor
from src.services.document_manager import DocumentManager
from src.services.background_job_manager import get_job_manager, JobStatus
from src.database.db_manager import DBManager
from src.middleware.firebase_auth import get_current_user
from config.config import R2_BUCKET_NAME

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/documents", tags=["PDF Documents"])

# Initialize services
pdf_split_service = get_pdf_split_service()
pdf_ai_processor = get_pdf_ai_processor()
db_manager = DBManager()
document_manager = DocumentManager(db=db_manager.db)


# Helper function to get R2 client (same pattern as gemini_slide_parser)
def _get_r2_client():
    """Initialize R2Client with full config"""
    from src.storage.r2_client import R2Client
    from src.core.config import APP_CONFIG

    return R2Client(
        account_id=APP_CONFIG["r2_account_id"],
        access_key_id=APP_CONFIG["r2_access_key_id"],
        secret_access_key=APP_CONFIG["r2_secret_access_key"],
        bucket_name=APP_CONFIG["r2_bucket_name"],
        region=APP_CONFIG["r2_region"],
    )


# ‚ùå ENDPOINT REMOVED: /upload-pdf-ai
# This endpoint was redundant - users should upload via /api/simple-files/upload
# Then convert using /api/documents/{document_id}/convert-with-ai


@router.get("/{document_id}/preview-split", response_model=PreviewSplitResponse)
async def preview_document_split(
    document_id: str,
    chunk_size: int = Query(10, ge=1, le=50),
    user_data: dict = Depends(get_current_user),
):
    """
    Preview how document will be split

    Args:
        document_id: Document ID
        chunk_size: Pages per chunk (1-50, default: 10)

    Returns:
        PreviewSplitResponse with split suggestions
    """
    try:
        user_id = user_data["uid"]

        logger.info(
            f"üîç Preview split request: doc={document_id}, chunk_size={chunk_size}"
        )

        # Get document from database
        document = db_manager.db.documents.find_one(
            {"document_id": document_id, "user_id": user_id}
        )

        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Check if document has PDF
        pdf_r2_path = document.get("pdf_r2_path")
        if not pdf_r2_path:
            raise HTTPException(
                status_code=400, detail="Document does not have PDF file"
            )

        # Download PDF from R2
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf:
            r2_client = _get_r2_client()
            pdf_content = r2_client.download_file(pdf_r2_path)
            temp_pdf.write(pdf_content)
            temp_pdf_path = temp_pdf.name

        try:
            # Calculate split suggestions
            suggestions = pdf_split_service.calculate_split_suggestions(
                temp_pdf_path, chunk_size
            )

            # Calculate total size
            total_size_mb = sum(s["estimated_size_mb"] for s in suggestions)

            # Convert to Pydantic models
            split_suggestions = [
                SplitSuggestion(**suggestion) for suggestion in suggestions
            ]

            response = PreviewSplitResponse(
                success=True,
                document_id=document_id,
                document_title=document.get("title", "Untitled"),
                total_pages=document.get("total_pages", 0),
                chunk_size=chunk_size,
                suggested_splits=split_suggestions,
                total_parts=len(suggestions),
                total_estimated_size_mb=round(total_size_mb, 2),
            )

            logger.info(
                f"‚úÖ Preview generated: {len(suggestions)} parts, "
                f"{total_size_mb:.2f}MB total"
            )

            return response

        finally:
            # Cleanup temp file
            try:
                os.remove(temp_pdf_path)
            except Exception as e:
                logger.warning(f"Failed to cleanup temp file: {e}")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Preview split failed: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Preview failed: {str(e)}")


@router.post("/{document_id}/split", response_model=SplitDocumentResponse)
async def split_document(
    document_id: str,
    request: SplitDocumentRequest,
    user_data: dict = Depends(get_current_user),
):
    """
    Split document into multiple parts

    Args:
        document_id: Document ID to split
        request: Split configuration (auto or manual mode)

    Returns:
        SplitDocumentResponse with created document parts
    """
    try:
        start_time = datetime.now()
        user_id = user_data["uid"]

        logger.info(
            f"‚úÇÔ∏è Split request: doc={document_id}, mode={request.mode}, "
            f"keep_original={request.keep_original}"
        )

        # Get document from database
        document = db_manager.db.documents.find_one(
            {"document_id": document_id, "user_id": user_id}
        )

        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Check if document has PDF
        pdf_r2_path = document.get("pdf_r2_path")
        if not pdf_r2_path:
            raise HTTPException(
                status_code=400, detail="Document does not have PDF file"
            )

        total_pages = document.get("total_pages", 0)

        # Download PDF from R2
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf:
            r2_client = _get_r2_client()
            pdf_content = r2_client.download_file(pdf_r2_path)
            temp_pdf.write(pdf_content)
            temp_pdf_path = temp_pdf.name

        try:
            split_parts = []

            if request.mode == "auto":
                # Auto split with equal chunks
                chunk_size = request.chunk_size or 10

                logger.info(f"ü§ñ Auto split mode: chunk_size={chunk_size}")

                # Split PDF
                chunk_files = pdf_split_service.split_pdf_to_chunks(
                    temp_pdf_path, chunk_size
                )

                # Create documents for each chunk
                for idx, chunk_path in enumerate(chunk_files):
                    part_num = idx + 1
                    start_page = (idx * chunk_size) + 1
                    end_page = min((idx + 1) * chunk_size, total_pages)
                    pages_count = end_page - start_page + 1

                    # Generate part document ID
                    part_doc_id = f"{document_id}_part{part_num}"
                    part_title = f"{document['title']} - Part {part_num}"

                    # Upload chunk to R2
                    with open(chunk_path, "rb") as f:
                        chunk_content = f.read()

                    part_r2_path = f"documents/{user_id}/{part_doc_id}.pdf"
                    r2_client = _get_r2_client()
                    r2_client.upload_file_object(
                        file_obj=chunk_content,
                        remote_path=part_r2_path,
                        content_type="application/pdf",
                    )

                    # Create part document in database
                    part_document = {
                        "document_id": part_doc_id,
                        "user_id": user_id,
                        "title": part_title,
                        "description": f"Part {part_num} of {document['title']}",
                        "document_type": document["document_type"],
                        "content_html": f"<p>Pages {start_page}-{end_page}</p>",
                        "created_at": datetime.now(),
                        "updated_at": datetime.now(),
                        # PDF info
                        "total_pages": pages_count,
                        "pdf_r2_path": part_r2_path,
                        "pdf_file_size": len(chunk_content),
                        # Split info
                        "is_split_part": True,
                        "original_document_id": document_id,
                        "parent_document_id": document_id,
                        "split_info": {
                            "start_page": start_page,
                            "end_page": end_page,
                            "part_number": part_num,
                            "total_parts": len(chunk_files),
                            "split_mode": "auto",
                        },
                        # AI processing (not done yet for parts)
                        "ai_processed": False,
                    }

                    db_manager.db.documents.insert_one(part_document)

                    split_parts.append(
                        SplitDocumentPart(
                            document_id=part_doc_id,
                            title=part_title,
                            pages=f"{start_page}-{end_page}",
                            pages_count=pages_count,
                            r2_path=part_r2_path,
                            file_size_mb=round(len(chunk_content) / (1024 * 1024), 2),
                            created_at=datetime.now().isoformat(),
                        )
                    )

                    logger.info(f"‚úÖ Created part {part_num}: {part_doc_id}")

                # Cleanup chunk files
                for chunk_path in chunk_files:
                    try:
                        os.remove(chunk_path)
                    except Exception as e:
                        logger.warning(f"Failed to cleanup chunk: {e}")

            else:
                # Manual split with custom ranges
                logger.info(f"‚úã Manual split mode: {len(request.split_ranges)} ranges")

                # Validate ranges
                is_valid, error_msg = pdf_split_service.validate_split_ranges(
                    total_pages, [r.dict() for r in request.split_ranges]
                )

                if not is_valid:
                    raise HTTPException(
                        status_code=400, detail=f"Invalid split ranges: {error_msg}"
                    )

                # Extract each range
                for idx, range_info in enumerate(request.split_ranges):
                    part_num = idx + 1

                    # Extract pages
                    part_pdf_path = os.path.join(
                        tempfile.gettempdir(), f"{document_id}_part{part_num}.pdf"
                    )

                    pdf_split_service.extract_page_range(
                        temp_pdf_path,
                        range_info.start_page,
                        range_info.end_page,
                        part_pdf_path,
                    )

                    # Generate part document ID
                    part_doc_id = f"{document_id}_part{part_num}"
                    part_title = range_info.title

                    # Upload to R2
                    with open(part_pdf_path, "rb") as f:
                        part_content = f.read()

                    part_r2_path = f"documents/{user_id}/{part_doc_id}.pdf"
                    r2_client = _get_r2_client()
                    r2_client.upload_file_object(
                        file_obj=part_content,
                        remote_path=part_r2_path,
                        content_type="application/pdf",
                    )

                    pages_count = range_info.end_page - range_info.start_page + 1

                    # Create part document
                    part_document = {
                        "document_id": part_doc_id,
                        "user_id": user_id,
                        "title": part_title,
                        "description": range_info.description
                        or f"Pages {range_info.start_page}-{range_info.end_page}",
                        "document_type": document["document_type"],
                        "content_html": f"<p>Pages {range_info.start_page}-{range_info.end_page}</p>",
                        "created_at": datetime.now(),
                        "updated_at": datetime.now(),
                        # PDF info
                        "total_pages": pages_count,
                        "pdf_r2_path": part_r2_path,
                        "pdf_file_size": len(part_content),
                        # Split info
                        "is_split_part": True,
                        "original_document_id": document_id,
                        "parent_document_id": document_id,
                        "split_info": {
                            "start_page": range_info.start_page,
                            "end_page": range_info.end_page,
                            "part_number": part_num,
                            "total_parts": len(request.split_ranges),
                            "split_mode": "manual",
                        },
                        "ai_processed": False,
                    }

                    db_manager.db.documents.insert_one(part_document)

                    split_parts.append(
                        SplitDocumentPart(
                            document_id=part_doc_id,
                            title=part_title,
                            pages=f"{range_info.start_page}-{range_info.end_page}",
                            pages_count=pages_count,
                            r2_path=part_r2_path,
                            file_size_mb=round(len(part_content) / (1024 * 1024), 2),
                            created_at=datetime.now().isoformat(),
                        )
                    )

                    # Cleanup part file
                    try:
                        os.remove(part_pdf_path)
                    except Exception as e:
                        logger.warning(f"Failed to cleanup part: {e}")

                    logger.info(f"‚úÖ Created part {part_num}: {part_doc_id}")

            # Update original document with child IDs
            child_ids = [part.document_id for part in split_parts]
            db_manager.db.documents.update_one(
                {"document_id": document_id},
                {
                    "$set": {
                        "child_document_ids": child_ids,
                        "updated_at": datetime.now(),
                    }
                },
            )

            # Delete original if not keeping
            original_kept = request.keep_original
            if not request.keep_original:
                db_manager.db.documents.delete_one({"document_id": document_id})
                logger.info(f"üóëÔ∏è Deleted original document: {document_id}")

            # Calculate processing time
            processing_time = (datetime.now() - start_time).total_seconds()

            response = SplitDocumentResponse(
                success=True,
                original_document_id=document_id,
                original_kept=original_kept,
                split_documents=split_parts,
                total_parts=len(split_parts),
                processing_time_seconds=round(processing_time, 2),
            )

            logger.info(
                f"‚úÖ Split complete: {len(split_parts)} parts created "
                f"({processing_time:.2f}s)"
            )

            return response

        finally:
            # Cleanup temp file
            try:
                os.remove(temp_pdf_path)
            except Exception as e:
                logger.warning(f"Failed to cleanup temp file: {e}")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Split failed: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Split failed: {str(e)}")


@router.post("/{document_id}/convert-with-ai", response_model=ConvertWithAIResponse)
async def convert_document_with_ai(
    document_id: str,
    request: ConvertWithAIRequest,
    user_data: dict = Depends(get_current_user),
):
    """
    Convert existing document with Gemini AI (SYNCHRONOUS - may timeout)

    ‚ö†Ô∏è WARNING: This endpoint waits for AI processing to complete.
    For large PDFs, use /convert-with-ai-async instead.

    Supports:
    - Full PDF conversion (no page_range specified)
    - Partial PDF conversion (with page_range selection)

    Args:
        document_id: Document ID to convert
        request: Conversion configuration (includes optional page_range)

    Returns:
        ConvertWithAIResponse with processing results
    """
    logger.info(
        f"üöÄ CONVERT ENDPOINT CALLED: document_id={document_id}, user={user_data.get('uid', 'unknown')}"
    )

    try:
        start_time = datetime.now()
        user_id = user_data["uid"]

        page_range_str = (
            f"{request.page_range.start_page}-{request.page_range.end_page}"
            if request.page_range
            else "all"
        )

        logger.info(
            f"ü§ñ Convert with AI request: doc={document_id}, "
            f"target={request.target_type}, pages={page_range_str}, "
            f"chunk_size={request.chunk_size}, force={request.force_reprocess}"
        )

        # Get file from database
        # Try user_files first (where simple file uploads are stored)
        file_doc = db_manager.db.user_files.find_one(
            {"file_id": document_id, "user_id": user_id}
        )

        if not file_doc:
            # Also try using _id field for backward compatibility
            file_doc = db_manager.db.user_files.find_one(
                {"_id": document_id, "user_id": user_id}
            )

        if not file_doc:
            raise HTTPException(status_code=404, detail="File not found in user_files")

        logger.info(
            f"‚úÖ Found file in 'user_files' collection: {file_doc.get('filename')}"
        )

        # Get PDF path from R2
        r2_key = file_doc.get("r2_key")
        if not r2_key:
            raise HTTPException(status_code=400, detail="File does not have R2 key")

        # Check if file is PDF
        file_type = file_doc.get("file_type", "")
        if file_type.lower() != ".pdf":
            raise HTTPException(
                status_code=400,
                detail=f"Only PDF files supported for AI conversion, got: {file_type}",
            )

        # Check if document already exists for this file (caching logic from slide parser)
        existing_doc = db_manager.db.documents.find_one(
            {"file_id": document_id, "user_id": user_id}
        )

        # Try to use cached content (fast path)
        cached_html = None
        if existing_doc and existing_doc.get("ai_processed"):
            prev_html = existing_doc.get("content_html")

            # Validate cached content has actual data (same as slide parser)
            if prev_html and prev_html.strip():
                from bs4 import BeautifulSoup

                soup = BeautifulSoup(prev_html, "html.parser")
                text_content = soup.get_text(strip=True)

                if text_content and len(text_content) > 100:  # At least 100 chars
                    if not request.force_reprocess:
                        # Cache is valid! Return cached document
                        logger.info(
                            f"‚ôªÔ∏è Reusing cached document (fast path - no Gemini call needed!)"
                        )
                        logger.info(
                            f"‚ôªÔ∏è Cached HTML: {len(prev_html)} chars, Text: {len(text_content)} chars"
                        )

                        # Return existing document info
                        # Format pages_converted properly
                        if existing_doc.get("ai_page_range"):
                            page_range = existing_doc.get("ai_page_range", {})
                            start = page_range.get("start_page")
                            end = page_range.get("end_page")
                            if start and end:
                                pages_converted_str = f"{start}-{end}"
                            else:
                                pages_converted_str = "all"
                        else:
                            pages_converted_str = "all"

                        # Format updated_at properly
                        updated_at_val = existing_doc.get("updated_at", datetime.now())
                        if isinstance(updated_at_val, datetime):
                            updated_at_str = updated_at_val.isoformat()
                        else:
                            updated_at_str = str(updated_at_val)

                        return ConvertWithAIResponse(
                            success=True,
                            document_id=existing_doc["document_id"],
                            title=existing_doc.get(
                                "title", file_doc.get("filename", "Untitled")
                            ),
                            document_type=existing_doc.get(
                                "document_type", request.target_type
                            ),
                            content_html=existing_doc.get("content_html", ""),
                            ai_processed=True,
                            ai_provider=existing_doc.get("ai_provider", "gemini"),
                            chunks_processed=existing_doc.get("ai_chunks_count", 0),
                            total_pages=existing_doc.get("total_pages", 0),
                            pages_converted=pages_converted_str,
                            processing_time_seconds=0.0,  # Cached, no processing time
                            reprocessed=False,  # Using cached content
                            updated_at=updated_at_str,
                        )
                    else:
                        logger.info(
                            f"‚ö†Ô∏è Cached content exists but force_reprocess=true, will re-parse"
                        )
                else:
                    text_len = len(text_content) if text_content else 0
                    logger.warning(
                        f"‚ö†Ô∏è Previous document has empty text content (HTML: {len(prev_html)} chars, Text: {text_len} chars), will re-process"
                    )
            else:
                logger.warning(
                    f"‚ö†Ô∏è Previous document has empty or no content_html, will re-process"
                )

        # Slow path: Download from R2 and process with Gemini (first time or cache invalid)
        logger.info(f"üì• Downloading file from R2: {r2_key}")

        # Download file to temp location
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_path = tmp_file.name

            # Initialize R2Client
            r2_client = _get_r2_client()

            file_obj = r2_client.get_file(r2_key)
            if not file_obj:
                raise HTTPException(
                    status_code=500, detail="Failed to download file from R2"
                )

            # Write to temp file
            file_content = file_obj["Body"].read()
            tmp_file.write(file_content)
            logger.info(f"‚úÖ Downloaded {len(file_content)} bytes to {tmp_path}")

        temp_pdf_path = tmp_path

        # Get PDF info
        pdf_info = pdf_split_service.get_pdf_info(temp_pdf_path)
        total_pages = pdf_info["total_pages"]

        # Validate page_range if provided
        if request.page_range:
            if request.page_range.start_page < 1:
                raise HTTPException(status_code=400, detail="start_page must be >= 1")
            if request.page_range.end_page > total_pages:
                raise HTTPException(
                    status_code=400,
                    detail=f"end_page ({request.page_range.end_page}) exceeds total pages ({total_pages})",
                )

        try:
            # Extract page range if specified
            pdf_to_process = temp_pdf_path
            temp_extracted_path = None

            if request.page_range:
                logger.info(
                    f"üìÑ Extracting pages {request.page_range.start_page}-{request.page_range.end_page}..."
                )
                temp_extracted_path = temp_pdf_path.replace(".pdf", "_extracted.pdf")
                pdf_split_service.extract_page_range(
                    pdf_path=temp_pdf_path,
                    start_page=request.page_range.start_page,
                    end_page=request.page_range.end_page,
                    output_path=temp_extracted_path,
                )
                pdf_to_process = temp_extracted_path

            # Process with AI
            logger.info(f"üöÄ Processing with Gemini AI...")

            html_content, metadata = await pdf_ai_processor.convert_existing_document(
                pdf_path=pdf_to_process,
                target_type=request.target_type,
                chunk_size=request.chunk_size,
            )

            chunks_processed = metadata["total_chunks"]
            processing_time = metadata["total_processing_time"]

            logger.info(
                f"‚úÖ AI processing complete: {chunks_processed} chunks, "
                f"{processing_time:.2f}s"
            )
            logger.info(f"üìÑ HTML content length: {len(html_content)} characters")
            logger.info(f"üìÑ HTML preview (first 200 chars): {html_content[:200]}")

            # Generate document ID if creating new, or use existing
            if existing_doc:
                doc_id = existing_doc["document_id"]
                is_update = True
            else:
                doc_id = f"{request.target_type}_{uuid.uuid4().hex[:12]}"
                is_update = False

            # Prepare document data
            document_data = {
                "document_id": doc_id,
                "user_id": user_id,
                "file_id": document_id,  # Link to user_files
                "title": file_doc.get("filename", "Untitled"),
                "document_type": request.target_type,
                "content_html": html_content,
                "created_at": (
                    existing_doc["created_at"] if existing_doc else datetime.now()
                ),
                "updated_at": datetime.now(),
                "last_opened_at": datetime.now(),
                "last_saved_at": datetime.now(),  # Required by document_editor
                "version": existing_doc.get("version", 1) if existing_doc else 1,
                "file_size_bytes": len(html_content.encode("utf-8")),
                "source_type": "ai_conversion",  # Mark as AI-converted
                "auto_save_count": 0,  # No auto-saves yet
                "manual_save_count": 0,  # No manual saves yet
                # PDF specific
                "total_pages": total_pages,
                # AI processing metadata
                "ai_processed": True,
                "ai_provider": "gemini",
                "ai_chunks_count": chunks_processed,
                "ai_processing_time": processing_time,
                "ai_chunk_size": request.chunk_size,
                "ai_processed_at": datetime.now(),
                # Other fields
                "is_deleted": False,
                "folder_id": None,
            }

            # Store page range info if partial conversion
            if request.page_range:
                document_data["ai_page_range"] = {
                    "start_page": request.page_range.start_page,
                    "end_page": request.page_range.end_page,
                }

            # Save or update document
            if is_update:
                db_manager.db.documents.update_one(
                    {"document_id": doc_id}, {"$set": document_data}
                )
                logger.info(f"üíæ Updated document: {doc_id}")
            else:
                db_manager.db.documents.insert_one(document_data)
                logger.info(f"üíæ Created new document: {doc_id}")

            # Calculate total time
            total_time = (datetime.now() - start_time).total_seconds()

            response = ConvertWithAIResponse(
                success=True,
                document_id=doc_id,
                title=file_doc.get("filename", "Untitled"),
                document_type=request.target_type,
                content_html=html_content,
                ai_processed=True,
                ai_provider="gemini",
                chunks_processed=chunks_processed,
                total_pages=total_pages,
                pages_converted=page_range_str,
                processing_time_seconds=round(total_time, 2),
                reprocessed=is_update,
                updated_at=datetime.now().isoformat(),
            )

            logger.info(
                f"‚úÖ Convert complete: {page_range_str} pages ({total_time:.2f}s)"
            )

            return response

        finally:
            # Cleanup temp files
            try:
                os.remove(temp_pdf_path)
                if temp_extracted_path and os.path.exists(temp_extracted_path):
                    os.remove(temp_extracted_path)
            except Exception as e:
                logger.warning(f"Failed to cleanup temp files: {e}")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Convert failed: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Convert failed: {str(e)}")


@router.post(
    "/{document_id}/convert-with-ai-async", response_model=ConvertJobStartResponse
)
async def convert_document_with_ai_async(
    document_id: str,
    request: ConvertWithAIRequest,
    user_data: dict = Depends(get_current_user),
):
    """
    Start background AI conversion job (RECOMMENDED for large PDFs)

    **SMART FLOW:**
    - Checks cache FIRST before creating job
    - If cached ‚Üí Returns result immediately (no polling needed)
    - If not cached ‚Üí Creates background job, returns job_id for polling

    Frontend should:
    1. Check response: if has 'result' field ‚Üí use directly (cache hit)
    2. If has 'job_id' only ‚Üí wait 15s, then poll status every 5s

    Args:
        document_id: Document ID to convert
        request: Conversion configuration

    Returns:
        ConvertJobStartResponse with result (if cached) OR job_id (if processing)
    """
    # Log IMMEDIATELY at function entry (before try block)
    print(f"üî•üî•üî• ASYNC ENDPOINT CALLED: document={document_id}")
    logger.info(f"üî•üî•üî• ASYNC ENDPOINT CALLED: document={document_id}")

    try:
        user_id = user_data["uid"]

        logger.info(
            f"üöÄ ASYNC conversion request: document={document_id}, user={user_id}"
        )

        # Get file metadata
        file_doc = db_manager.db.user_files.find_one(
            {"file_id": document_id, "user_id": user_id}
        )

        if not file_doc:
            raise HTTPException(status_code=404, detail=f"File {document_id} not found")

        # ‚ö° STEP 1: Check cache FIRST (avoid unnecessary job creation)
        if not request.force_reprocess:
            existing_doc = db_manager.db.documents.find_one(
                {"file_id": document_id, "user_id": user_id}
            )

            if existing_doc:
                logger.info(
                    f"üì¶ CACHE HIT! Returning result immediately (no job, no polling)"
                )

                # Build cached result
                cached_result = ConvertWithAIResponse(
                    success=True,
                    document_id=existing_doc["document_id"],
                    title=existing_doc["title"],
                    document_type=existing_doc.get("document_type", "doc"),
                    content_html=existing_doc["content_html"],
                    ai_processed=True,
                    ai_provider="gemini",
                    chunks_processed=0,
                    total_pages=existing_doc.get("total_pages", 0),
                    pages_converted="all",
                    processing_time_seconds=0,
                    reprocessed=False,
                    updated_at=existing_doc.get(
                        "last_saved_at", datetime.now()
                    ).isoformat(),
                )

                # Return with result embedded (no polling needed!)
                return ConvertJobStartResponse(
                    success=True,
                    job_id=None,  # No job created
                    file_id=document_id,
                    title=file_doc.get("filename", "Unknown"),
                    message="Using cached result (instant response, no processing)",
                    estimated_wait_seconds=0,  # No wait!
                    status_endpoint="",  # No endpoint needed
                    created_at=datetime.now().isoformat(),
                    result=cached_result,  # Result included directly
                )

        # ‚ö° STEP 2: No cache ‚Üí Create background job
        job_id = f"convert_{document_id}_{int(datetime.now().timestamp())}"

        logger.info(f"üîÑ NO CACHE: Creating background job {job_id}")

        job_manager = get_job_manager()
        job = job_manager.create_job(
            job_id=job_id,
            job_type="pdf_ai_conversion",
            user_id=user_id,
            params={
                "document_id": document_id,
                "target_type": request.target_type,
                "chunk_size": request.chunk_size,
                "force_reprocess": request.force_reprocess,
                "page_range": request.page_range.dict() if request.page_range else None,
            },
        )

        # Start background task (fire and forget)
        import asyncio

        logger.info(f"üöÄ Starting background task for job {job_id}...")
        task = asyncio.create_task(
            _run_conversion_job(job_id, document_id, request, user_id)
        )

        # CRITICAL: Store task reference to prevent garbage collection
        job.task = task
        logger.info(f"‚úÖ Background task created and stored: {task}")

        # Return job info for polling
        response = ConvertJobStartResponse(
            success=True,
            job_id=job_id,
            file_id=document_id,
            title=file_doc.get("filename", "Unknown"),
            message="AI conversion started in background. Check status after 15 seconds.",
            estimated_wait_seconds=15,  # Small docs: 20-30s, Large docs: 60-120s
            status_endpoint=f"/api/documents/jobs/{job_id}/status",
            created_at=datetime.now().isoformat(),
            result=None,  # No result yet, must poll
        )

        logger.info(f"‚úÖ Job {job_id} started, client should poll after 15s")
        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Failed to start conversion job: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to start job: {str(e)}")


@router.get("/jobs/{job_id}/status", response_model=ConvertJobStatusResponse)
async def get_conversion_job_status(
    job_id: str,
    user_data: dict = Depends(get_current_user),
):
    """
    Check status of background conversion job

    Args:
        job_id: Job ID returned from /convert-with-ai-async

    Returns:
        ConvertJobStatusResponse with current status and result (if completed)
    """
    try:
        user_id = user_data["uid"]

        # Log every status check (to see if frontend is polling)
        logger.info(f"üîç STATUS CHECK: job={job_id}, user={user_id}")

        job_manager = get_job_manager()
        job = job_manager.get_job(job_id)

        if not job:
            logger.warning(f"‚ùå Job {job_id} NOT FOUND")
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        # Security check: ensure user owns this job
        if job.user_id != user_id:
            logger.warning(
                f"‚ùå Access denied: job owner={job.user_id}, requester={user_id}"
            )
            raise HTTPException(status_code=403, detail="Access denied")

        logger.info(f"üìä Job status: {job.status}, progress: {job.progress}%")

        job_dict = job.to_dict()

        # Estimate remaining time based on progress
        estimated_remaining = None
        if job.status == JobStatus.PROCESSING and job.progress > 0:
            elapsed = job_dict["elapsed_seconds"]
            estimated_total = (elapsed / job.progress) * 100
            estimated_remaining = int(estimated_total - elapsed)

        response = ConvertJobStatusResponse(
            job_id=job_id,
            status=job.status,
            progress=job.progress,
            message=_get_status_message(job),
            result=job.result if job.status == JobStatus.COMPLETED else None,
            error=job.error,
            elapsed_seconds=job_dict["elapsed_seconds"],
            created_at=job_dict["created_at"],
            estimated_remaining_seconds=estimated_remaining,
        )

        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Failed to get job status: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to get status: {str(e)}")


async def _run_conversion_job(
    job_id: str,
    document_id: str,
    request: ConvertWithAIRequest,
    user_id: str,
):
    """Background task to run AI conversion"""
    logger.info(f"üî• === BACKGROUND JOB STARTED ===")
    logger.info(f"üÜî Job ID: {job_id}")
    logger.info(f"üìÑ Document: {document_id}")
    logger.info(f"üë§ User: {user_id}")
    logger.info(f"üéØ Target: {request.target_type}")

    job_manager = get_job_manager()

    try:
        logger.info(f"üîÑ Job {job_id}: Starting AI processing...")

        # Update progress
        job_manager.update_progress(job_id, 10, JobStatus.PROCESSING)

        # Run the actual conversion logic (reuse from sync endpoint)
        # Get file
        file_doc = db_manager.db.user_files.find_one(
            {"file_id": document_id, "user_id": user_id}
        )

        if not file_doc:
            raise Exception(f"File {document_id} not found")

        job_manager.update_progress(job_id, 20)

        # Check cache
        existing_doc = db_manager.db.documents.find_one(
            {"file_id": document_id, "user_id": user_id}
        )

        if existing_doc and not request.force_reprocess:
            logger.info(f"üì¶ Job {job_id}: Using cached content")
            job_manager.update_progress(job_id, 90)

            # Return cached result
            result = ConvertWithAIResponse(
                success=True,
                document_id=existing_doc["document_id"],
                title=existing_doc["title"],
                document_type=existing_doc.get("document_type", "doc"),
                content_html=existing_doc["content_html"],
                ai_processed=True,
                ai_provider="gemini",
                chunks_processed=0,
                total_pages=existing_doc.get("total_pages", 0),
                pages_converted="all",
                processing_time_seconds=0,
                reprocessed=False,
                updated_at=existing_doc.get(
                    "last_saved_at", datetime.now()
                ).isoformat(),
            )

            job_manager.complete_job(job_id, result.dict())
            return

        # Process with AI (slow path)
        job_manager.update_progress(job_id, 30)

        # Download PDF from R2
        r2_client = _get_r2_client()
        r2_key = file_doc.get("r2_key")

        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf:
            temp_pdf_path = temp_pdf.name
            await r2_client.download_file(r2_key, temp_pdf_path)

        job_manager.update_progress(job_id, 40)

        # Process with Gemini AI
        logger.info(f"ü§ñ Job {job_id}: Processing with Gemini AI...")
        logger.info(
            f"   Target type: {request.target_type}, Chunk size: {request.chunk_size}"
        )

        html_content, metadata = await pdf_ai_processor.convert_existing_document(
            pdf_path=temp_pdf_path,
            target_type=request.target_type,
            chunk_size=request.chunk_size,
        )

        logger.info(f"‚úÖ Job {job_id}: Gemini processing complete!")
        logger.info(f"   HTML size: {len(html_content)} chars")
        logger.info(f"   Metadata: {metadata}")

        job_manager.update_progress(job_id, 80)

        # Create document
        doc_id = document_manager.create_document(
            user_id=user_id,
            title=file_doc.get("filename", "Untitled"),
            content_html=html_content,
            content_text="",
            source_type="file",
            document_type=request.target_type,
            file_id=document_id,
            original_r2_url=file_doc.get("r2_key"),
            original_file_type=file_doc.get("file_type"),
        )

        job_manager.update_progress(job_id, 95)

        # Build result
        result = ConvertWithAIResponse(
            success=True,
            document_id=doc_id,
            title=file_doc.get("filename", "Untitled"),
            document_type=request.target_type,
            content_html=html_content,
            ai_processed=True,
            ai_provider=metadata.get("ai_provider", "gemini"),
            chunks_processed=metadata.get(
                "successful_chunks", metadata.get("total_chunks", 1)
            ),
            total_pages=metadata.get("total_pages", metadata.get("total_a4_pages", 0)),
            pages_converted="all",
            processing_time_seconds=metadata.get(
                "total_processing_time", metadata.get("processing_time_seconds", 0)
            ),
            reprocessed=True,
            updated_at=datetime.now().isoformat(),
        )

        # Log content size for debugging
        logger.info(
            f"üìä Job {job_id} result: content_html={len(html_content)} chars, "
            f"chunks={metadata.get('successful_chunks', 0)}/{metadata.get('total_chunks', 0)}, "
            f"pages={metadata.get('total_a4_pages', 0)}"
        )

        # Cleanup
        try:
            os.remove(temp_pdf_path)
        except:
            pass

        # Complete job
        job_manager.complete_job(job_id, result.dict())
        logger.info(f"‚úÖ Job {job_id}: Completed successfully")

    except Exception as e:
        logger.error(f"‚ùå Job {job_id} failed: {str(e)}", exc_info=True)
        job_manager.fail_job(job_id, str(e))


def _get_status_message(job) -> str:
    """Generate human-readable status message"""
    if job.status == JobStatus.PENDING:
        return "Job is queued and waiting to start"
    elif job.status == JobStatus.PROCESSING:
        if job.progress < 30:
            return "Preparing PDF file..."
        elif job.progress < 80:
            return "Processing with Gemini AI..."
        else:
            return "Finalizing document..."
    elif job.status == JobStatus.COMPLETED:
        return "Conversion completed successfully!"
    elif job.status == JobStatus.FAILED:
        return f"Conversion failed: {job.error}"
    return "Unknown status"


@router.get("/{document_id}/split-info", response_model=SplitInfoResponse)
async def get_document_split_info(
    document_id: str, user_data: dict = Depends(get_current_user)
):
    """
    Get split information for a document

    Args:
        document_id: Document ID

    Returns:
        SplitInfoResponse with split metadata and siblings
    """
    try:
        user_id = user_data["uid"]

        logger.info(f"‚ÑπÔ∏è Split info request: doc={document_id}")

        # Get document from database
        document = db_manager.db.documents.find_one(
            {"document_id": document_id, "user_id": user_id}
        )

        if not document:
            raise HTTPException(status_code=404, detail="Document not found")

        # Check if this is a split part
        is_split_part = document.get("is_split_part", False)

        if not is_split_part:
            # Not a split part
            return SplitInfoResponse(
                success=True,
                document_id=document_id,
                is_split_part=False,
                can_merge=False,
            )

        # Get split info
        split_info_data = document.get("split_info", {})
        original_doc_id = document.get("original_document_id")

        # Get all sibling documents
        siblings_docs = list(
            db_manager.db.documents.find(
                {
                    "original_document_id": original_doc_id,
                    "user_id": user_id,
                    "document_id": {"$ne": document_id},  # Exclude current document
                }
            )
        )

        # Build siblings list
        siblings = [
            SplitInfoSibling(
                document_id=sib["document_id"],
                title=sib["title"],
                pages=f"{sib['split_info']['start_page']}-{sib['split_info']['end_page']}",
            )
            for sib in siblings_docs
        ]

        # Build split info detail
        split_info = SplitInfoDetail(
            start_page=split_info_data.get("start_page", 0),
            end_page=split_info_data.get("end_page", 0),
            total_pages_in_part=document.get("total_pages", 0),
            part_number=split_info_data.get("part_number", 0),
            total_parts=split_info_data.get("total_parts", 0),
        )

        # Can merge if there are siblings
        can_merge = len(siblings) > 0

        response = SplitInfoResponse(
            success=True,
            document_id=document_id,
            is_split_part=True,
            original_document_id=original_doc_id,
            split_info=split_info,
            siblings=siblings,
            can_merge=can_merge,
        )

        logger.info(
            f"‚úÖ Split info: part {split_info.part_number}/{split_info.total_parts}, "
            f"{len(siblings)} siblings"
        )

        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Get split info failed: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Get split info failed: {str(e)}")


@router.post("/merge", response_model=MergeDocumentsResponse)
async def merge_documents(
    request: MergeDocumentsRequest, user_data: dict = Depends(get_current_user)
):
    """
    Merge multiple documents into one

    Args:
        request: Merge configuration with document IDs

    Returns:
        MergeDocumentsResponse with merged document info
    """
    try:
        start_time = datetime.now()
        user_id = user_data["uid"]

        logger.info(
            f"üîó Merge request: {len(request.document_ids)} documents, "
            f"title='{request.title}'"
        )

        # Get all documents
        documents = list(
            db_manager.db.documents.find(
                {"document_id": {"$in": request.document_ids}, "user_id": user_id}
            )
        )

        if len(documents) != len(request.document_ids):
            raise HTTPException(
                status_code=404, detail="One or more documents not found"
            )

        # Check all documents have PDFs
        pdf_paths = []
        merged_from_info = []

        for doc in documents:
            pdf_r2_path = doc.get("pdf_r2_path")
            if not pdf_r2_path:
                raise HTTPException(
                    status_code=400,
                    detail=f"Document {doc['document_id']} does not have PDF file",
                )
            pdf_paths.append(pdf_r2_path)

            # Build merged from info
            if doc.get("is_split_part") and doc.get("split_info"):
                pages = (
                    f"{doc['split_info']['start_page']}-{doc['split_info']['end_page']}"
                )
            else:
                pages = f"1-{doc.get('total_pages', 0)}"

            merged_from_info.append(
                MergedFromInfo(document_id=doc["document_id"], pages=pages)
            )

        # Download all PDFs to temp files
        temp_pdf_files = []
        r2_client = _get_r2_client()
        for pdf_r2_path in pdf_paths:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf:
                pdf_content = r2_client.download_file(pdf_r2_path)
                temp_pdf.write(pdf_content)
                temp_pdf_files.append(temp_pdf.name)

        try:
            # Merge PDFs
            merged_pdf_path = os.path.join(
                tempfile.gettempdir(), f"merged_{uuid.uuid4().hex[:8]}.pdf"
            )

            logger.info(f"üîó Merging {len(temp_pdf_files)} PDFs...")

            pdf_split_service.merge_pdfs(temp_pdf_files, merged_pdf_path)

            # Get merged PDF info
            merged_info = pdf_split_service.get_pdf_info(merged_pdf_path)
            total_pages = merged_info["total_pages"]

            # Upload merged PDF to R2
            with open(merged_pdf_path, "rb") as f:
                merged_content = f.read()

            # Generate merged document ID
            merged_doc_id = f"doc_{uuid.uuid4().hex[:12]}"
            merged_r2_path = f"documents/{user_id}/{merged_doc_id}.pdf"

            r2_client = _get_r2_client()
            r2_client.upload_file_object(
                file_obj=merged_content,
                remote_path=merged_r2_path,
                content_type="application/pdf",
            )

            logger.info(f"‚òÅÔ∏è Uploaded merged PDF: {merged_r2_path}")

            # Create merged document in database
            merged_document = {
                "document_id": merged_doc_id,
                "user_id": user_id,
                "title": request.title,
                "description": f"Merged from {len(documents)} documents",
                "document_type": documents[0]["document_type"],
                "content_html": f"<p>Merged document ({total_pages} pages)</p>",
                "created_at": datetime.now(),
                "updated_at": datetime.now(),
                # PDF info
                "total_pages": total_pages,
                "pdf_r2_path": merged_r2_path,
                "pdf_file_size": len(merged_content),
                # Merge info
                "is_merged": True,
                "merged_from": [
                    {"document_id": info.document_id, "pages": info.pages}
                    for info in merged_from_info
                ],
                "merged_at": datetime.now(),
                # Not split part
                "is_split_part": False,
                "ai_processed": False,
            }

            db_manager.db.documents.insert_one(merged_document)

            logger.info(f"üíæ Created merged document: {merged_doc_id}")

            # Delete originals if not keeping
            originals_deleted = not request.keep_originals
            if not request.keep_originals:
                result = db_manager.db.documents.delete_many(
                    {"document_id": {"$in": request.document_ids}}
                )
                logger.info(f"üóëÔ∏è Deleted {result.deleted_count} original documents")

            # Cleanup temp files
            for temp_file in temp_pdf_files + [merged_pdf_path]:
                try:
                    os.remove(temp_file)
                except Exception as e:
                    logger.warning(f"Failed to cleanup temp file: {e}")

            # Calculate processing time
            processing_time = (datetime.now() - start_time).total_seconds()

            response = MergeDocumentsResponse(
                success=True,
                merged_document_id=merged_doc_id,
                title=request.title,
                total_pages=total_pages,
                merged_from=merged_from_info,
                originals_deleted=originals_deleted,
                r2_path=merged_r2_path,
                created_at=datetime.now().isoformat(),
            )

            logger.info(
                f"‚úÖ Merge complete: {merged_doc_id} ({total_pages} pages, "
                f"{processing_time:.2f}s)"
            )

            return response

        finally:
            pass

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Merge failed: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Merge failed: {str(e)}")
