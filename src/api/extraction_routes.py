"""
AI Extraction Routes - Single Endpoint for R2 URL Processing
AI Extraction API - Endpoint duy nháº¥t cho xá»­ lÃ½ R2 URL

WORKFLOW / LUá»’NG Xá»¬ LÃ:
1. Backend uploads files to R2 and gets public URL
   Backend upload file lÃªn R2 vÃ  láº¥y public URL
2. Backend calls /api/extract/process with R2 URL + metadata
   Backend gá»i /api/extract/process vá»›i R2 URL + metadata
3. AI Service processes with template-based extraction
   AI Service xá»­ lÃ½ vá»›i extraction dá»±a trÃªn template
4. Returns raw data + structured JSON according to industry template
   Tráº£ vá» raw data + structured JSON theo template industry
5. Optionally uploads to Qdrant via worker
   TÃ¹y chá»n upload lÃªn Qdrant qua worker

CALLBACK URL PATTERNS / MáºªU CALLBACK URL:
- File Upload: /api/webhooks/file-processed (raw content processing)
- AI Extraction: /api/webhooks/ai/extraction-callback (structured data extraction)

IMPLEMENTED FLOW / LUá»’NG ÄÃƒ TRIá»‚N KHAI:
- Template selection based on Industry + DataType (âœ… ai_extraction_service.py)
- JSON schema integration from templates (âœ… ai_extraction_service.py)
- AI provider selection (ChatGPT Vision/DeepSeek) (âœ… ai_extraction_service.py)
- Raw + structured data extraction (âœ… ai_extraction_service.py)
- Qdrant worker integration ready (âœ… ai_extraction_service.py)
"""

import json
import requests
import time
import redis
import uuid
import os
import asyncio
import aiohttp
from typing import Dict, Any, Optional, List
from datetime import datetime

from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from pydantic import BaseModel, Field

from src.models.unified_models import Industry, Language
from src.services.ai_extraction_service import get_ai_service
from src.services.task_status_service import TaskStatusService, CallbackService
from src.utils.logger import setup_logger
from src.queue.queue_dependencies import (
    get_extraction_queue,
)  # Use extraction queue for ExtractionProcessingTask
from src.queue.queue_manager import QueueManager
from src.queue.task_models import ExtractionProcessingTask

# âœ… HYBRID STRATEGY: Import only necessary callback handler
from src.api.callbacks.enhanced_callback_handler import send_backend_callback

# Initialize router and logger
router = APIRouter(prefix="/api/extract")
logger = setup_logger(__name__)

# Global queue manager (initialized once for efficiency)
queue_manager: Optional[QueueManager] = None

# Redis client for task status tracking
redis_client = None


def get_redis_client():
    """Get Redis client for task status tracking"""
    global redis_client
    if redis_client is None:
        redis_client = redis.Redis(
            host=os.getenv("REDIS_HOST", "localhost"),
            port=int(os.getenv("REDIS_PORT", 6379)),
            db=int(os.getenv("REDIS_DB", 0)),
            decode_responses=True,
        )
    return redis_client


# ===== REQUEST MODEL =====


class ExtractionRequest(BaseModel):
    """
    Single extraction request for R2 URL processing
    YÃªu cáº§u extraction duy nháº¥t cho xá»­ lÃ½ R2 URL
    """

    # Required fields / TrÆ°á»ng báº¯t buá»™c
    r2_url: str = Field(..., description="Public R2 URL of the uploaded file")
    company_id: str = Field(..., description="Company ID for Qdrant storage")
    industry: Industry = Field(
        ..., description="Company industry for template selection"
    )

    # Optional extraction targeting / Nháº¯m má»¥c tiÃªu extraction tÃ¹y chá»n
    target_categories: Optional[List[str]] = Field(
        None,
        description="Target categories for extraction. If None, extracts both products and services automatically",
        example=["products", "services"],
    )

    # Data type specification for backend tracking
    data_type: Optional[str] = Field(
        None,
        description="Specific data type for backend tracking (products, services, or auto)",
        example="products",
    )

    # File metadata / Metadata file
    file_metadata: Dict[str, Any] = Field(
        ...,
        description="File metadata (original_name, file_size, etc.)",
        example={
            "original_name": "golden_dragon_menu.pdf",
            "file_size": 1024000,
            "file_type": "application/pdf",
            "uploaded_at": "2025-07-16T10:00:00Z",
        },
    )

    # Optional company context / Context cÃ´ng ty tÃ¹y chá»n
    company_info: Optional[Dict[str, Any]] = Field(
        None,
        description="Company information for extraction context",
        example={
            "id": "golden-dragon-restaurant",
            "name": "Golden Dragon Restaurant",
            "industry": "restaurant",
            "description": "Traditional Vietnamese restaurant",
        },
    )

    # Processing options / TÃ¹y chá»n xá»­ lÃ½
    language: Optional[Language] = Field(
        None, description="Target language for extraction results (from Frontend)"
    )
    upload_to_qdrant: bool = Field(
        False, description="Whether to upload results to Qdrant via background worker"
    )
    callback_url: Optional[str] = Field(
        None,
        description="Callback URL for async processing notifications",
        example="https://api.agent8x.io.vn/api/webhooks/ai/extraction-callback",
    )


# ===== RESPONSE MODEL =====


class ExtractionResponse(BaseModel):
    """
    Complete extraction response with raw and structured data
    Response extraction hoÃ n chá»‰nh vá»›i raw vÃ  structured data
    """

    # Status / Tráº¡ng thÃ¡i
    success: bool = Field(..., description="Whether extraction was successful")
    message: str = Field(..., description="Processing status message")

    # Core extraction results / Káº¿t quáº£ extraction chÃ­nh
    raw_content: Optional[str] = Field(
        None, description="Complete raw extracted content"
    )
    structured_data: Optional[Dict[str, Any]] = Field(
        None, description="Structured data according to industry template"
    )

    # Processing metadata / Metadata xá»­ lÃ½
    template_used: Optional[str] = Field(
        None, description="Industry template used for extraction"
    )
    ai_provider: Optional[str] = Field(
        None, description="AI provider used (ChatGPT Vision/DeepSeek)"
    )
    industry: Optional[str] = Field(None, description="Industry classification")
    data_type: Optional[str] = Field(None, description="Data type processed")

    # Performance metrics / Metrics hiá»‡u suáº¥t
    processing_time: Optional[float] = Field(
        None, description="Processing time in seconds"
    )
    total_items_extracted: Optional[int] = Field(
        None, description="Number of items extracted"
    )

    # Additional metadata / Metadata bá»• sung
    extraction_metadata: Optional[Dict[str, Any]] = Field(
        None, description="Detailed extraction metadata"
    )

    # Error handling / Xá»­ lÃ½ lá»—i
    error: Optional[str] = Field(None, description="Error message if extraction failed")
    error_details: Optional[Dict[str, Any]] = Field(
        None, description="Detailed error information"
    )


# ===== QUEUE-BASED REQUEST MODEL =====


class ExtractionQueueRequest(BaseModel):
    """
    Queue-based extraction request for async processing
    YÃªu cáº§u extraction dá»±a trÃªn queue cho xá»­ lÃ½ báº¥t Ä‘á»“ng bá»™
    """

    # Required fields / TrÆ°á»ng báº¯t buá»™c
    r2_url: str = Field(..., description="Public R2 URL of the uploaded file")
    company_id: str = Field(..., description="Company ID for Qdrant storage")
    industry: Industry = Field(
        ..., description="Company industry for template selection"
    )

    # File metadata / Metadata file
    file_name: str = Field(..., description="Original file name")
    file_size: Optional[int] = Field(None, description="File size in bytes")
    file_type: Optional[str] = Field(None, description="MIME type of the file")
    metadata: Optional[Dict[str, Any]] = Field(
        None,
        description="Additional metadata including file_id",
        example={"file_id": "file_uuid_123"},
    )

    # Backend tracking / Theo dÃµi backend
    data_type: Optional[str] = Field(
        None,
        description="Specific data type for backend tracking (products, services, or auto)",
        example="products",
    )
    target_categories: Optional[List[str]] = Field(
        None,
        description="Target categories for extraction",
        example=["products", "services"],
    )

    # Processing options / TÃ¹y chá»n xá»­ lÃ½
    language: Optional[Language] = Field(
        None, description="Target language for extraction (from Frontend)"
    )
    callback_url: Optional[str] = Field(
        None,
        description="Callback URL for extraction completion notifications",
        example="https://api.agent8x.io.vn/api/webhooks/ai/extraction-callback",
    )

    # Optional company context / Context cÃ´ng ty tÃ¹y chá»n
    company_info: Optional[Dict[str, Any]] = Field(
        None, description="Company information"
    )


class ExtractionQueueResponse(BaseModel):
    """
    Queue-based extraction response with task ID
    Response extraction dá»±a trÃªn queue vá»›i task ID
    """

    success: bool = Field(..., description="Whether task was queued successfully")
    task_id: str = Field(..., description="Unique task ID for tracking")
    company_id: str = Field(..., description="Company ID")
    status: str = Field(..., description="Task status (queued)")
    message: str = Field(..., description="Status message")
    estimated_time: int = Field(..., description="Estimated processing time in seconds")

    # Error handling / Xá»­ lÃ½ lá»—i
    error: Optional[str] = Field(None, description="Error message if queueing failed")


# ===== TASK STATUS MODELS =====


class TaskStatusResponse(BaseModel):
    """Task status response model"""

    task_id: str = Field(..., description="Unique task identifier")
    status: str = Field(
        ..., description="Task status: queued, processing, completed, failed"
    )
    progress: Optional[Dict[str, Any]] = Field(None, description="Progress information")
    submitted_at: str = Field(..., description="Task submission timestamp")
    completed_at: Optional[str] = Field(None, description="Task completion timestamp")
    processing_time: Optional[float] = Field(
        None, description="Processing time in seconds"
    )
    error_message: Optional[str] = Field(None, description="Error message if failed")


class TaskResultResponse(BaseModel):
    """Task result response model"""

    task_id: str = Field(..., description="Unique task identifier")
    success: bool = Field(..., description="Whether extraction was successful")
    message: str = Field(..., description="Processing result message")
    processing_time: Optional[float] = Field(
        None, description="Processing time in seconds"
    )
    total_items_extracted: Optional[int] = Field(
        None, description="Number of items extracted"
    )
    ai_provider: Optional[str] = Field(None, description="AI provider used")
    template_used: Optional[str] = Field(
        None, description="Template used for extraction"
    )
    industry: Optional[str] = Field(None, description="Industry classification")
    data_type: Optional[str] = Field(None, description="Data type processed")

    # Full extraction data
    raw_content: Optional[str] = Field(
        None, description="Complete raw extracted content"
    )
    structured_data: Optional[Dict[str, Any]] = Field(
        None, description="Structured extraction results"
    )
    extraction_metadata: Optional[Dict[str, Any]] = Field(
        None, description="Detailed extraction metadata"
    )

    # Error handling
    error: Optional[str] = Field(None, description="Error message if extraction failed")
    error_details: Optional[Dict[str, Any]] = Field(
        None, description="Detailed error information"
    )


# ===== DEPENDENCY =====


async def get_extraction_service():
    """Get AI extraction service instance"""
    return get_ai_service()


async def get_queue_manager():
    """Get queue manager instance"""
    global queue_manager
    if queue_manager is None:
        try:
            queue_manager = QueueManager()
            await queue_manager.connect()
            logger.info("âœ… Queue manager initialized successfully")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize queue manager: {str(e)}")
            raise HTTPException(status_code=503, detail="Queue service unavailable")
    return queue_manager


# Global task status service
task_status_service = None


def get_task_status_service():
    """Get task status service instance"""
    global task_status_service
    if task_status_service is None:
        redis_client = get_redis_client()
        task_status_service = TaskStatusService(redis_client)
    return task_status_service


# ===== MAIN ENDPOINT REMOVED =====
#
# âš ï¸ ENDPOINT /process ÄÃƒ ÄÆ¯á»¢C BACKUP VÃ€ Gá»  Bá»Ž
#
# Sync endpoint /process Ä‘Ã£ Ä‘Æ°á»£c backup vÃ o: /src/api/extraction_routes_sync_backup.py
# Giá» chá»‰ sá»­ dá»¥ng queue-based async endpoint /process-async vá»›i Hybrid Strategy
#
# Äá»ƒ khÃ´i phá»¥c sync endpoint, copy tá»« backup file vÃ  uncomment


# ===== BACKGROUND TASK =====


async def schedule_qdrant_upload(
    extraction_result_json: str,
    metadata_json: str,
    company_id: str,
    industry: str,
    language: str = "auto",  # Default to auto-detect
    callback_url: Optional[str] = None,
):
    """
    Schedule Qdrant upload via ingestion worker
    LÃªn lá»‹ch upload Qdrant qua ingestion worker

    Uses ai_extraction_service.prepare_for_qdrant_ingestion()
    Sá»­ dá»¥ng ai_extraction_service.prepare_for_qdrant_ingestion()
    """
    try:
        # Deserialize the JSON strings back into dictionaries
        extraction_result = json.loads(extraction_result_json)
        metadata = json.loads(metadata_json)

        logger.info("ðŸ“¤ Processing Qdrant upload via background worker")
        logger.info(
            f"ðŸ“Š Deserialized extraction result with {len(extraction_result.get('products', []))} products"
        )
        logger.info(f"ðŸ“¦ Products found: {len(extraction_result.get('products', []))}")
        logger.info(f"ðŸ”§ Services found: {len(extraction_result.get('services', []))}")
        logger.info(f"ðŸ­ Industry input: '{industry}' (type: {type(industry)})")
        logger.info(f"ðŸŒ Language input: '{language}' (type: {type(language)})")
        logger.info(f"ðŸ“Š Extraction result size: {len(extraction_result_json)} chars")
        logger.info(f"ðŸ“‹ Metadata size: {len(metadata_json)} chars")

        # Get AI service instance
        ai_service = get_ai_service()

        # Convert string enums to proper enum types with error handling
        from src.models.unified_models import Industry, Language

        try:
            industry_enum = (
                Industry(industry) if isinstance(industry, str) else industry
            )
            logger.info(f"âœ… Industry enum converted: {industry_enum}")
        except ValueError as e:
            logger.warning(f"âš ï¸ Unknown industry '{industry}', using OTHER. Error: {e}")
            industry_enum = Industry.OTHER

        try:
            if language:
                language_enum = (
                    Language(language) if isinstance(language, str) else language
                )
            else:
                # Default to English if no language specified from frontend
                language_enum = Language.ENGLISH
            logger.info(f"âœ… Language enum converted: {language_enum}")
        except ValueError as e:
            logger.warning(
                f"âš ï¸ Unknown language '{language}', using ENGLISH. Error: {e}"
            )
            language_enum = Language.ENGLISH

        # Use implemented method to prepare data for worker with proper parameters
        ingestion_data = await ai_service.prepare_for_qdrant_ingestion(
            extraction_result=extraction_result,
            user_id=metadata.get("user_id", "system"),
            document_id=metadata.get("file_id", metadata.get("document_id", "unknown")),
            company_id=company_id,
            industry=industry_enum,
            language=language_enum,
            callback_url=callback_url,
        )

        ingestion_metadata = ingestion_data.get("ingestion_metadata", {})
        logger.info("âœ… Qdrant company ingestion scheduled successfully")
        logger.info(f"   ðŸ¢ Company: {company_id}")
        logger.info(f"   ðŸ­ Industry: {industry}")
        logger.info(f"   ðŸ“Š Total chunks: {ingestion_metadata.get('total_chunks', 0)}")

        # ðŸ“ž STANDARD CALLBACK: Use enhanced_callback_handler vá»›i webhook signature
        if callback_url:
            try:
                callback_payload = {
                    "task_id": metadata.get("file_id", "background_task"),
                    "company_id": company_id,
                    "status": "completed",
                    "processing_type": "background_qdrant_upload",
                    "results": {
                        "products_count": len(extraction_result.get("products", [])),
                        "services_count": len(extraction_result.get("services", [])),
                        "total_chunks": ingestion_metadata.get("total_chunks", 0),
                    },
                    "timestamp": datetime.now().isoformat(),
                }

                logger.info(
                    f"ðŸ“ž Sending background callback via enhanced_callback_handler"
                )
                logger.info(f"   ðŸ”— Callback URL: {callback_url}")

                # âœ… USE ENHANCED_CALLBACK_HANDLER: With webhook signature and proper error handling
                callback_success = await send_backend_callback(
                    callback_url=callback_url,
                    callback_payload=callback_payload,
                    event_type="background_qdrant_upload_completed",
                    timeout=30,
                )

                if callback_success:
                    logger.info(
                        f"âœ… Background callback sent successfully via enhanced_callback_handler"
                    )
                else:
                    logger.warning(
                        f"âš ï¸ Background callback failed via enhanced_callback_handler"
                    )

            except Exception as callback_error:
                logger.error(f"âŒ Failed to send background callback: {callback_error}")

    except Exception as e:
        logger.error(f"âŒ Failed to schedule Qdrant upload: {str(e)}")

        # ðŸ“ž STANDARD ERROR CALLBACK: Use enhanced_callback_handler vá»›i webhook signature
        if callback_url:
            try:
                error_callback_payload = {
                    "task_id": (
                        metadata.get("file_id", "background_task")
                        if "metadata" in locals()
                        else "background_task"
                    ),
                    "company_id": company_id,
                    "status": "failed",
                    "processing_type": "background_qdrant_upload",
                    "error": str(e),
                    "timestamp": datetime.now().isoformat(),
                }

                logger.info(
                    f"ðŸ“ž Sending background error callback via enhanced_callback_handler"
                )
                logger.info(f"   ðŸ”— Callback URL: {callback_url}")

                # âœ… USE ENHANCED_CALLBACK_HANDLER: With webhook signature and proper error handling
                callback_success = await send_backend_callback(
                    callback_url=callback_url,
                    callback_payload=error_callback_payload,
                    event_type="background_qdrant_upload_failed",
                    timeout=30,
                )

                if callback_success:
                    logger.info(
                        f"âœ… Background error callback sent successfully via enhanced_callback_handler"
                    )
                else:
                    logger.warning(
                        f"âš ï¸ Background error callback failed via enhanced_callback_handler"
                    )

            except Exception as callback_error:
                logger.error(
                    f"âŒ Failed to send background error callback: {callback_error}"
                )


# ===== HYBRID STRATEGY QUEUE-BASED ENDPOINT =====


@router.post("/process-async", response_model=ExtractionQueueResponse)
async def process_extraction_async_hybrid(
    request: ExtractionQueueRequest,
    queue: QueueManager = Depends(
        get_extraction_queue
    ),  # Use extraction queue for /process-async
    status_service: TaskStatusService = Depends(get_task_status_service),
) -> ExtractionQueueResponse:
    """
    ðŸŽ¯ **HYBRID STRATEGY QUEUE-BASED WORKFLOW**: Process extraction vá»›i AI Auto-Categorization + Individual Storage + Enhanced Callback

    **QUEUE WORKFLOW vá»›i HYBRID STRATEGY:**

    1. âœ… **API receives request and validates parameters**
       API nháº­n request vÃ  validate parameters

    2. âœ… **Create ExtractionProcessingTask vá»›i HYBRID STRATEGY flags**
       Táº¡o task vá»›i cÃ¡c cá» chá»‰ dáº«n worker sá»­ dá»¥ng Hybrid Strategy

    3. âœ… **Push task to Redis queue (document_processing)**
       Äáº©y task vÃ o Redis queue vá»›i metadata Ä‘áº§y Ä‘á»§

    4. âœ… **Return task_id immediately to backend**
       Tráº£ vá» task_id ngay láº­p tá»©c cho backend

    5. ðŸ”„ **Worker processes task vá»›i HYBRID STRATEGY:**
       - **AI Extraction**: Gá»i AIExtractionService vá»›i generic prompts (auto-categorization)
       - **Individual Processing**: Xá»­ lÃ½ tá»«ng product/service riÃªng biá»‡t:
         * Generate embedding cho tá»«ng item
         * Táº¡o Qdrant point riÃªng vá»›i rich metadata (category, tags, target_audience)
         * Upsert vÃ o collection "multi_company_data"
       - **Enhanced Callback**: Gá»i callback vá»›i qdrant_point_id + category cho tá»«ng item

    **HYBRID STRATEGY FEATURES Ä‘Æ°á»£c enable trong task:**
    âœ… **AI Auto-Categorization**: Generic prompts, khÃ´ng cáº§n examples cá»¥ thá»ƒ
    âœ… **Individual Storage**: Má»—i product/service = 1 Qdrant point riÃªng biá»‡t
    âœ… **Metadata Filtering**: Rich metadata (category, sub_category, tags, target_audience)
    âœ… **Vector Search**: Semantic similarity search trÃªn embeddings
    âœ… **Enhanced Callback**: Callback chá»©a qdrant_point_id cho individual CRUD
    """

    # Generate unique task ID vá»›i prefix hybrid
    task_id = f"hybrid_extract_{int(time.time() * 1000)}_{uuid.uuid4().hex[:8]}"

    logger.info(
        "ðŸŽ¯ [HYBRID QUEUE-API] Received Hybrid Strategy extraction queue request"
    )
    logger.info(f"   ðŸ†” Task ID: {task_id}")
    logger.info(f"   ðŸ”— R2 URL: {request.r2_url}")
    logger.info(f"   ðŸ¢ Company ID: {request.company_id}")
    logger.info(f"   ðŸ­ Industry: {request.industry}")
    logger.info(
        f"   ðŸŽ¯ Strategy: Queue-based Hybrid Search (Metadata Filtering + Vector Search + Individual Storage)"
    )

    # DETAILED REQUEST TRACKING - Same as sync endpoint
    backend_data_type = request.data_type or "auto"  # String for data_type field
    target_categories_specified = request.target_categories or ["products", "services"]

    logger.info("=" * 60)
    logger.info("ðŸ“Š HYBRID STRATEGY QUEUE REQUEST ANALYSIS:")
    logger.info(f"   ðŸ“‹ Backend Data Type: '{backend_data_type}'")
    logger.info(f"   ðŸ“Š Target Categories: {target_categories_specified}")
    logger.info(f"   ðŸ“„ File: {request.file_name}")
    logger.info(f"   ðŸ“¦ File Size: {request.file_size or 'unknown'} bytes")
    logger.info(f"   ðŸ“ File Type: {request.file_type or 'unknown'}")
    logger.info(f"   ðŸ“ž Callback URL: {request.callback_url or 'None'}")
    logger.info(f"   ðŸŽ¯ Processing Mode: Hybrid Strategy vá»›i Individual Storage")
    logger.info("=" * 60)

    try:
        # âœ… CREATE EXTRACTION PROCESSING TASK vá»›i HYBRID STRATEGY FLAGS
        processing_task = ExtractionProcessingTask(
            task_id=task_id,
            company_id=request.company_id,
            r2_url=request.r2_url,
            file_name=request.file_name,
            file_type=request.file_type or "text/plain",
            file_size=request.file_size or 0,
            industry=request.industry,
            language=request.language or Language.AUTO_DETECT,  # Default to auto-detect
            data_type=backend_data_type,
            target_categories=target_categories_specified,
            callback_url=request.callback_url,
            company_info=request.company_info or {},
            created_at=datetime.utcnow(),
            # ðŸŽ¯ HYBRID STRATEGY: Enhanced processing metadata vá»›i worker instructions
            processing_metadata={
                # === FILE INFORMATION (for catalog service) ===
                "file_id": (
                    request.metadata.get("file_id") if request.metadata else None
                ),
                "file_name": request.file_name,
                "original_filename": request.file_name,
                # === HYBRID STRATEGY FLAGS ===
                "hybrid_strategy_enabled": True,
                "individual_storage_mode": True,
                "ai_auto_categorization": True,
                "enhanced_callback_mode": True,
                # === WORKER INSTRUCTIONS ===
                "extraction_mode": "hybrid_search_ready",
                "processing_type": "hybrid_extraction_workflow",
                "storage_strategy": "individual_points_with_metadata",
                "callback_strategy": "enhanced_with_qdrant_ids",
                # === AI PROCESSING SETTINGS ===
                "use_generic_prompts": True,  # No industry-specific examples
                "auto_categorization_fields": [
                    "category",
                    "sub_category",
                    "tags",
                    "target_audience",
                    "coverage_type",
                    "service_type",
                ],
                # === QDRANT STORAGE SETTINGS ===
                "collection_name": "multi_company_data",
                "point_strategy": "one_product_one_point",
                "metadata_filtering_enabled": True,
                "vector_search_enabled": True,
                # === EMBEDDING & CHUNKING ===
                "embedding_strategy": "individual_item_embeddings",
                "chunking_strategy": "per_item_chunking",
                "content_for_embedding_required": True,
                # === CALLBACK REQUIREMENTS ===
                "callback_includes_qdrant_ids": True,
                "callback_includes_categories": True,
                "callback_handler": "enhanced_callback_handler",
                # === PERFORMANCE SETTINGS ===
                "min_items_per_chunk": 1,  # Individual processing
                "chunk_by_categories": False,  # Individual items, not chunks
                "upload_to_qdrant": True,
                # === SEARCH CAPABILITIES ===
                "search_capabilities": {
                    "metadata_filtering_fields": [
                        "category",
                        "sub_category",
                        "tags",
                        "target_audience",
                        "coverage_type",
                        "service_type",
                        "industry",
                        "item_type",
                    ],
                    "vector_search_enabled": True,
                    "individual_item_crud": True,
                    "filter_then_search": True,  # Hybrid approach
                },
            },
        )

        # Enqueue task vá»›i Hybrid Strategy flags
        logger.info(
            f"ðŸ“¤ [HYBRID QUEUE-API] Enqueueing Hybrid Strategy extraction task {task_id}"
        )
        logger.info(
            f"   ðŸŽ¯ Worker sáº½ thá»±c hiá»‡n: AI Auto-Categorization + Individual Storage + Enhanced Callback"
        )
        success = await queue.enqueue_generic_task(processing_task)

        if not success:
            logger.error(f"âŒ [HYBRID QUEUE-API] Failed to enqueue task {task_id}")
            return ExtractionQueueResponse(
                success=False,
                task_id=task_id,
                company_id=request.company_id,
                status="failed",
                message="Failed to queue Hybrid Strategy extraction task",
                estimated_time=0,
                error="Queue service unavailable",
            )

        # Create task status tracking
        await status_service.create_task(task_id, request.company_id, datetime.now())

        logger.info(
            f"âœ… [HYBRID QUEUE-API] Task {task_id} queued successfully vá»›i Hybrid Strategy"
        )
        logger.info(
            f"   ðŸ“Š Worker sáº½ xá»­ lÃ½: {len(target_categories_specified)} categories"
        )
        logger.info(
            f"   ðŸŽ¯ Strategy: Metadata Filtering + Vector Search + Individual Storage"
        )
        logger.info(
            f"   ðŸ“ž Enhanced Callback: Sáº½ bao gá»“m qdrant_point_id + category cho tá»«ng item"
        )

        return ExtractionQueueResponse(
            success=True,
            task_id=task_id,
            company_id=request.company_id,
            status="queued",
            message=f"âœ… HYBRID STRATEGY: Task queued successfully for {backend_data_type} extraction vá»›i AI auto-categorization + individual storage",
            estimated_time=30,  # Hybrid Strategy cÃ³ thá»ƒ máº¥t nhiá»u thá»i gian hÆ¡n do individual processing
        )

    except Exception as e:
        logger.error(
            f"âŒ [HYBRID QUEUE-API] Failed to queue Hybrid Strategy extraction task: {str(e)}"
        )
        return ExtractionQueueResponse(
            success=False,
            task_id=task_id,
            company_id=request.company_id,
            status="error",
            message="Failed to queue Hybrid Strategy extraction task",
            estimated_time=0,
            error=str(e),
        )

    # ===== HYBRID STRATEGY WORKER SUPPORT FUNCTION =====
    #
    # âš ï¸ HÃ€M NÃ€Y ÄÃƒ Lá»–I THá»œI VÃ€ ÄÆ¯á»¢C Gá»  Bá»Ž
    #
    # Logic xá»­ lÃ½ (embedding, lÆ°u Qdrant) Ä‘Ã£ Ä‘Æ°á»£c chuyá»ƒn hoÃ n toÃ n sang
    # function chuyÃªn dá»¥ng trong `src/api/callbacks/enhanced_callback_handler.py`.
    #
    # Worker bÃ¢y giá» sáº½ gá»i AIExtractionService, sau Ä‘Ã³ gá»i trá»±c tiáº¿p function
    # `process_extraction_with_hybrid_strategy()` thay vÃ¬ gá»i hÃ m nÃ y.
    #
    # âœ… KIáº¾N TRÃšC Má»šI:
    # Worker â†’ AIExtractionService â†’ enhanced_callback_handler.process_extraction_with_hybrid_strategy()
    #
    # async def schedule_hybrid_qdrant_upload_with_enhanced_callback(...):
    #    ... (TOÃ€N Bá»˜ CODE Cá»¦A HÃ€M NÃ€Y ÄÃƒ Bá»Š XÃ“A VÃ€ CHUYá»‚N SANG enhanced_callback_handler.py)

    # ===== STATUS & RESULT ENDPOINTS =====
    """
    ðŸŽ¯ **HYBRID STRATEGY WORKER FUNCTION**: Schedule individual Qdrant upload vá»›i enhanced callback

    **HYBRID STRATEGY WORKFLOW:**
    1. âœ… **Individual Processing**: Xá»­ lÃ½ tá»«ng product/service riÃªng biá»‡t
    2. âœ… **Rich Metadata Generation**: Táº¡o metadata Ä‘áº§y Ä‘á»§ cho filtering
    3. âœ… **Individual Qdrant Points**: Má»—i item = 1 point vá»›i embedding riÃªng
    4. âœ… **Enhanced Callback**: Callback chá»©a qdrant_point_id + category cho tá»«ng item

    **CALLBACK FORMAT:**
    ```json
    {
        "task_id": "hybrid_extract_xxx",
        "company_id": "company_123",
        "status": "completed",
        "processing_type": "hybrid_individual_storage",
        "items_processed": [
            {
                "item_id": "item_1",
                "qdrant_point_id": "uuid_xxx",
                "category": "products",
                "sub_category": "main_course",
                "item_type": "food_item",
                "title": "Phá»Ÿ BÃ²",
                "metadata": { ... }
            },
            ...
        ],
        "summary": {
            "total_items": 25,
            "products_count": 15,
            "services_count": 10,
            "collection": "multi_company_data"
        }
    }
    ```
    """
    try:
        # Deserialize input data
        extraction_result = json.loads(extraction_result_json)
        metadata = json.loads(metadata_json)
        processing_metadata = processing_metadata or {}

        logger.info(
            "ðŸŽ¯ [HYBRID WORKER] Starting individual Qdrant upload with enhanced callback"
        )
        logger.info(f"   ðŸ¢ Company: {company_id}")
        logger.info(f"   ðŸ­ Industry: {industry}")
        logger.info(f"   ðŸ“¦ Products: {len(extraction_result.get('products', []))}")
        logger.info(f"   ðŸ”§ Services: {len(extraction_result.get('services', []))}")
        logger.info(f"   ðŸ“ž Callback URL: {callback_url or 'None'}")
        logger.info(f"   ðŸŽ¯ Mode: Individual Storage + Enhanced Callback")

        # Get services
        ai_service = get_ai_service()
        embedding_service = get_embedding_service()
        qdrant_manager = create_qdrant_manager()

        # Get MongoDB catalog service for dual storage
        from src.services.product_catalog_service import get_product_catalog_service

        catalog_service = await get_product_catalog_service()

        # Convert enums
        from src.models.unified_models import Industry, Language

        industry_enum = Industry(industry) if isinstance(industry, str) else industry
        language_enum = Language(language) if isinstance(language, str) else language

        # Collection name for Hybrid Strategy
        collection_name = processing_metadata.get(
            "collection_name", "multi_company_data"
        )

        # Process each item individually
        processed_items = []
        total_items = 0

        # Process products individually
        products = extraction_result.get("products", [])
        for idx, product in enumerate(products):
            try:
                logger.info(
                    f"ðŸŽ¯ [HYBRID WORKER] Processing product {idx+1}/{len(products)}: {product.get('name', 'Unknown')}"
                )

                # Generate rich metadata for filtering
                product_metadata = {
                    "company_id": company_id,
                    "industry": str(industry_enum),
                    "item_type": "product",
                    "category": "products",
                    "sub_category": product.get("category", "uncategorized"),
                    "tags": product.get("tags", []),
                    "target_audience": product.get("target_audience", "general"),
                    "coverage_type": product.get("coverage_type", "standard"),
                    "item_id": f"prod_{company_id}_{idx}",
                    "source_document": metadata.get("original_name", "unknown"),
                    "extraction_timestamp": datetime.now().isoformat(),
                    "hybrid_strategy": True,
                    "individual_storage": True,
                }

                # Generate content for embedding
                content_for_embedding = f"""
                Name: {product.get('name', '')}
                Description: {product.get('description', '')}
                Category: {product.get('category', '')}
                Price: {product.get('price', '')}
                Tags: {', '.join(product.get('tags', []))}
                Company: {company_id}
                Industry: {industry}
                """.strip()

                # Generate embedding
                embedding = await embedding_service.generate_embedding(
                    content_for_embedding
                )

                # Create Qdrant point ID
                point_id = str(uuid.uuid4())

                # Upsert to Qdrant
                await qdrant_manager.upsert_points(
                    collection_name=collection_name,
                    points=[
                        {
                            "id": point_id,
                            "vector": embedding,
                            "payload": {
                                **product_metadata,
                                "content": content_for_embedding,
                                "raw_data": product,
                            },
                        }
                    ],
                )

                # ðŸ”¥ DUAL STORAGE: Also save to MongoDB ProductCatalogService
                enriched_product = await catalog_service.register_item(
                    item_data=product, company_id=company_id, item_type="product"
                )

                # Get MongoDB product_id for tracking
                mongo_product_id = enriched_product.get("product_id", "N/A")

                # Track processed item
                processed_items.append(
                    {
                        "item_id": product_metadata["item_id"],
                        "qdrant_point_id": point_id,
                        "mongo_product_id": mongo_product_id,  # Track MongoDB ID
                        "category": "products",
                        "sub_category": product.get("category", "uncategorized"),
                        "item_type": "product",
                        "title": product.get("name", "Unknown Product"),
                        "metadata": product_metadata,
                    }
                )

                total_items += 1
                logger.info(
                    f"   âœ… Product saved to Qdrant: {point_id} & MongoDB: {mongo_product_id}"
                )

            except Exception as e:
                logger.error(f"   âŒ Failed to process product {idx}: {e}")

        # Process services individually
        services = extraction_result.get("services", [])
        for idx, service in enumerate(services):
            try:
                logger.info(
                    f"ðŸŽ¯ [HYBRID WORKER] Processing service {idx+1}/{len(services)}: {service.get('name', 'Unknown')}"
                )

                # Generate rich metadata for filtering
                service_metadata = {
                    "company_id": company_id,
                    "industry": str(industry_enum),
                    "item_type": "service",
                    "category": "services",
                    "sub_category": service.get("category", "uncategorized"),
                    "tags": service.get("tags", []),
                    "target_audience": service.get("target_audience", "general"),
                    "service_type": service.get("service_type", "standard"),
                    "coverage_type": service.get("coverage_type", "standard"),
                    "item_id": f"serv_{company_id}_{idx}",
                    "source_document": metadata.get("original_name", "unknown"),
                    "extraction_timestamp": datetime.now().isoformat(),
                    "hybrid_strategy": True,
                    "individual_storage": True,
                }

                # Generate content for embedding
                content_for_embedding = f"""
                Name: {service.get('name', '')}
                Description: {service.get('description', '')}
                Category: {service.get('category', '')}
                Price: {service.get('price', '')}
                Duration: {service.get('duration', '')}
                Tags: {', '.join(service.get('tags', []))}
                Company: {company_id}
                Industry: {industry}
                """.strip()

                # Generate embedding
                embedding = await embedding_service.generate_embedding(
                    content_for_embedding
                )

                # Create Qdrant point ID
                point_id = str(uuid.uuid4())

                # Upsert to Qdrant
                await qdrant_manager.upsert_points(
                    collection_name=collection_name,
                    points=[
                        {
                            "id": point_id,
                            "vector": embedding,
                            "payload": {
                                **service_metadata,
                                "content": content_for_embedding,
                                "raw_data": service,
                            },
                        }
                    ],
                )

                # ðŸ”¥ DUAL STORAGE: Also save to MongoDB ProductCatalogService
                enriched_service = await catalog_service.register_item(
                    item_data=service, company_id=company_id, item_type="service"
                )

                # Get MongoDB service_id for tracking
                mongo_service_id = enriched_service.get("service_id", "N/A")

                # Track processed item
                processed_items.append(
                    {
                        "item_id": service_metadata["item_id"],
                        "qdrant_point_id": point_id,
                        "mongo_service_id": mongo_service_id,  # Track MongoDB ID
                        "category": "services",
                        "sub_category": service.get("category", "uncategorized"),
                        "item_type": "service",
                        "title": service.get("name", "Unknown Service"),
                        "metadata": service_metadata,
                    }
                )

                total_items += 1
                logger.info(
                    f"   âœ… Service saved to Qdrant: {point_id} & MongoDB: {mongo_service_id}"
                )

            except Exception as e:
                logger.error(f"   âŒ Failed to process service {idx}: {e}")

        logger.info(
            f"ðŸŽ¯ [HYBRID WORKER] Dual storage (Qdrant + MongoDB) processing completed"
        )
        logger.info(f"   ðŸ“Š Total items processed: {total_items}")
        logger.info(
            f"   ðŸ“¦ Products: {len([i for i in processed_items if i['category'] == 'products'])}"
        )
        logger.info(
            f"   ðŸ”§ Services: {len([i for i in processed_items if i['category'] == 'services'])}"
        )
        logger.info(f"   ðŸ—‚ï¸ Qdrant Collection: {collection_name}")
        logger.info(f"   ðŸ’¾ MongoDB Collection: internal_products_catalog")

        # ðŸ“ž ENHANCED CALLBACK: Use enhanced_callback_handler vá»›i webhook signature
        if callback_url:
            try:
                enhanced_callback_payload = {
                    "task_id": metadata.get("file_id", "hybrid_background_task"),
                    "company_id": company_id,
                    "status": "completed",
                    "processing_type": "hybrid_individual_storage",
                    "items_processed": processed_items,
                    "summary": {
                        "total_items": total_items,
                        "products_count": len(
                            [i for i in processed_items if i["category"] == "products"]
                        ),
                        "services_count": len(
                            [i for i in processed_items if i["category"] == "services"]
                        ),
                        "collection": collection_name,
                        "individual_storage": True,
                        "hybrid_strategy": True,
                    },
                    "qdrant_info": {
                        "collection_name": collection_name,
                        "total_points_created": total_items,
                        "individual_crud_enabled": True,
                    },
                    "timestamp": datetime.now().isoformat(),
                }

                logger.info(
                    f"ðŸ“ž [HYBRID WORKER] Sending enhanced callback via enhanced_callback_handler"
                )
                logger.info(f"   ðŸ”— Callback URL: {callback_url}")
                logger.info(
                    f"   ðŸ“Š Callback includes {len(processed_items)} items with qdrant_point_id"
                )

                # âœ… USE ENHANCED_CALLBACK_HANDLER: With webhook signature and proper error handling
                callback_success = await send_backend_callback(
                    callback_url=callback_url,
                    callback_payload=enhanced_callback_payload,
                    event_type="hybrid_individual_storage_completed",
                    timeout=30,
                )

                if callback_success:
                    logger.info(
                        f"âœ… [HYBRID WORKER] Enhanced callback sent successfully via enhanced_callback_handler"
                    )
                else:
                    logger.warning(
                        f"âš ï¸ [HYBRID WORKER] Enhanced callback failed via enhanced_callback_handler"
                    )

            except Exception as callback_error:
                logger.error(
                    f"âŒ [HYBRID WORKER] Failed to send enhanced callback: {callback_error}"
                )

    except Exception as e:
        logger.error(
            f"âŒ [HYBRID WORKER] Failed to process hybrid individual storage: {str(e)}"
        )

        # ðŸ“ž ENHANCED ERROR CALLBACK: Use enhanced_callback_handler vá»›i webhook signature
        if callback_url:
            try:
                error_callback_payload = {
                    "task_id": (
                        metadata.get("file_id", "hybrid_background_task")
                        if "metadata" in locals()
                        else "hybrid_background_task"
                    ),
                    "company_id": company_id,
                    "status": "failed",
                    "processing_type": "hybrid_individual_storage",
                    "error": str(e),
                    "timestamp": datetime.now().isoformat(),
                }

                logger.info(
                    f"ðŸ“ž [HYBRID WORKER] Sending error callback via enhanced_callback_handler"
                )
                logger.info(f"   ðŸ”— Callback URL: {callback_url}")

                # âœ… USE ENHANCED_CALLBACK_HANDLER: With webhook signature and proper error handling
                callback_success = await send_backend_callback(
                    callback_url=callback_url,
                    callback_payload=error_callback_payload,
                    event_type="hybrid_individual_storage_failed",
                    timeout=30,
                )

                if callback_success:
                    logger.info(
                        f"âœ… [HYBRID WORKER] Error callback sent successfully via enhanced_callback_handler"
                    )
                else:
                    logger.warning(
                        f"âš ï¸ [HYBRID WORKER] Error callback failed via enhanced_callback_handler"
                    )

            except Exception as callback_error:
                logger.error(
                    f"âŒ [HYBRID WORKER] Failed to send error callback: {callback_error}"
                )


# ===== STATUS & RESULT ENDPOINTS =====


@router.get("/status/{task_id}", response_model=TaskStatusResponse)
async def get_task_status(
    task_id: str, status_service: TaskStatusService = Depends(get_task_status_service)
) -> TaskStatusResponse:
    """
    Get current status of an extraction task
    Láº¥y tráº¡ng thÃ¡i hiá»‡n táº¡i cá»§a task extraction
    """
    try:
        logger.info(f"ðŸ“Š Checking status for task: {task_id}")

        task_data = await status_service.get_task_status(task_id)

        if not task_data:
            logger.warning(f"âš ï¸ Task not found: {task_id}")
            raise HTTPException(
                status_code=404, detail=f"Task {task_id} not found or expired"
            )

        # Check if task is still in Redis queue (processing)
        redis_client = get_redis_client()
        queue_length = redis_client.llen("document_processing")

        # If status is queued but queue is empty, update to processing
        if task_data["status"] == "queued" and queue_length == 0:
            # Check if this is likely processing by looking at timing
            submitted_time = datetime.fromisoformat(task_data["submitted_at"])
            time_elapsed = (datetime.now() - submitted_time).total_seconds()

            if time_elapsed > 5:  # If more than 5 seconds, likely processing
                task_data["status"] = "processing"
                task_data["progress"] = {
                    "stage": "ai_extraction",
                    "estimated_remaining": "10-20 seconds",
                }
                await status_service.update_task_status(
                    task_id, "processing", task_data.get("progress")
                )

        response = TaskStatusResponse(
            task_id=task_data["task_id"],
            status=task_data["status"],
            progress=task_data.get("progress"),
            submitted_at=task_data["submitted_at"],
            completed_at=task_data.get("completed_at"),
            processing_time=task_data.get("processing_time"),
            error_message=task_data.get("error_message"),
        )

        logger.info(f"âœ… Status retrieved for {task_id}: {response.status}")
        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Failed to get task status for {task_id}: {e}")
        raise HTTPException(
            status_code=500, detail=f"Failed to retrieve task status: {str(e)}"
        )


@router.get("/result/{task_id}", response_model=TaskResultResponse)
async def get_task_result(
    task_id: str, status_service: TaskStatusService = Depends(get_task_status_service)
) -> TaskResultResponse:
    """
    Get extraction results for a completed task
    Láº¥y káº¿t quáº£ extraction cho task Ä‘Ã£ hoÃ n thÃ nh
    """
    try:
        logger.info(f"ðŸ“Š Retrieving results for task: {task_id}")

        # Check task status first
        task_data = await status_service.get_task_status(task_id)

        if not task_data:
            logger.warning(f"âš ï¸ Task not found: {task_id}")
            raise HTTPException(
                status_code=404, detail=f"Task {task_id} not found or expired"
            )

        if task_data["status"] not in ["completed", "failed"]:
            logger.warning(
                f"âš ï¸ Task {task_id} not completed yet, status: {task_data['status']}"
            )
            raise HTTPException(
                status_code=202,  # Accepted but not ready
                detail=f"Task {task_id} is still {task_data['status']}. Please check status endpoint first.",
            )

        # Get result data
        result_data = await status_service.get_task_result(task_id)

        if not result_data:
            logger.warning(f"âš ï¸ No result data found for task: {task_id}")
            raise HTTPException(
                status_code=404,
                detail=f"Result data for task {task_id} not found or expired",
            )

        response = TaskResultResponse(
            task_id=task_id,
            success=result_data.get("success", False),
            message=result_data.get("message", ""),
            processing_time=result_data.get("processing_time"),
            total_items_extracted=result_data.get("total_items_extracted"),
            ai_provider=result_data.get("ai_provider"),
            template_used=result_data.get("template_used"),
            industry=result_data.get("industry"),
            data_type=result_data.get("data_type"),
            raw_content=result_data.get("raw_content"),
            structured_data=result_data.get("structured_data"),
            extraction_metadata=result_data.get("extraction_metadata"),
            error=result_data.get("error"),
            error_details=result_data.get("error_details"),
        )

        logger.info(f"âœ… Results retrieved for {task_id}: {response.success}")
        logger.info(f"   ðŸ“Š Items extracted: {response.total_items_extracted}")

        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Failed to get task result for {task_id}: {e}")
        raise HTTPException(
            status_code=500, detail=f"Failed to retrieve task result: {str(e)}"
        )
