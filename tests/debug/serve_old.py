from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware 
from pydantic import BaseModel
import os, sys, asyncio
from pathlib import Path
from datetime import datetime
from fastapi.responses import StreamingResponse
import json
import time 
import re
from datetime import datetime

import threading

import schedule
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager
from src.providers.ai_provider_manager import AIProviderManager  # TH√äM M·ªöI
from config.config import (
    DEEPSEEK_API_KEY,
    CHATGPT_API_KEY,
    DEFAULT_AI_PROVIDER,
)  # TH√äM M·ªöI
import fitz  # PyMuPDF
from docx import Document
import base64
from PIL import Image
import io
# Add import at the top
from src.utils.web_search_utils import search_real_estate_properties, search_real_estate_properties_with_logging  # ‚úÖ ADD THIS
from src.utils.real_estate_analyzer import analyze_real_estate_query


# ‚úÖ FIXED: Smart environment configuration loading
ENV = os.getenv("ENV", "production").lower()

# ‚úÖ Load environment files based on environment
try:
    from dotenv import load_dotenv
    
    if ENV == "development":
        # Try development.env first, fallback to .env
        dev_env_file = Path(__file__).parent / "development.env"
        env_file = Path(__file__).parent / ".env"
        
        if dev_env_file.exists():
            load_dotenv(dev_env_file)
            print(f"Loaded DEVELOPMENT configuration from development.env")
        elif env_file.exists():
            load_dotenv(env_file)
            print(f"Loaded DEVELOPMENT configuration from .env (fallback)")
        else:
            print(f"No environment file found, using system environment")
    else:
        # Production - use .env
        env_file = Path(__file__).parent / ".env"
        if env_file.exists():
            load_dotenv(env_file)
            print(f"Loaded PRODUCTION configuration from .env")
        else:
            print(f"No .env file found for production")
            
except ImportError:
    print("python-dotenv not installed, using environment variables only")

# ‚úÖ Re-read ENV after loading files
ENV = os.getenv("ENV", "production").lower()

# ‚úÖ Set configuration based on environment
if ENV == "development":
    # Development configuration
    DEBUG = True
    HOST = "localhost"
    PORT = 8000
    DOMAIN = "localhost:8000"
    BASE_URL = "http://localhost:8000"
    CORS_ORIGINS = [
        "http://localhost:3000",
        "http://localhost:8080",
        "http://localhost:8000",
        "http://127.0.0.1:3000",
        "http://127.0.0.1:8080",
        "http://127.0.0.1:8000"
    ]
    print(f"üîß Development mode active")
    print(f"   Debug: {DEBUG}")
    print(f"   Host: {HOST}")
    print(f"   Port: {PORT}")
    print(f"   Domain: {DOMAIN}")
    print(f"   Base URL: {BASE_URL}")
    
else:
    # Production configuration (from .env or environment variables)
    DEBUG = os.getenv("DEBUG", "false").lower() == "true"
    HOST = os.getenv("HOST", "0.0.0.0")
    PORT = int(os.getenv("PORT", 8000))
    DOMAIN = os.getenv("DOMAIN", "ai.aimoney.io.vn")
    BASE_URL = os.getenv("BASE_URL", f"https://{DOMAIN}")
    
    # CORS configuration for production
    CORS_ORIGINS = os.getenv("CORS_ORIGINS", "").split(",") if os.getenv("CORS_ORIGINS") else [
        "https://ai.aimoney.io.vn",
        "https://www.ai.aimoney.io.vn", 
        "https://aimoney.io.vn",
        "https://www.aimoney.io.vn",
        "http://localhost:3000",  # For development testing
        "http://localhost:8080"   # For development testing
    ]
    print(f"üè≠ Production mode active")
    print(f"   Debug: {DEBUG}")
    print(f"   Host: {HOST}")
    print(f"   Port: {PORT}")
    print(f"   Domain: {DOMAIN}")
    print(f"   Base URL: {BASE_URL}")

# ‚úÖ Load API keys (always needed)
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
CHATGPT_API_KEY = os.getenv("CHATGPT_API_KEY")
DEFAULT_AI_PROVIDER = os.getenv("DEFAULT_AI_PROVIDER", "deepseek")
SERPAPI_KEY = os.getenv("SERPAPI_KEY")

# ‚úÖ Database configuration
MONGODB_URI = os.getenv("MONGODB_URI", "mongodb://localhost:27017/" if ENV == "development" else "mongodb://host.docker.internal:27017/")
MONGODB_NAME = os.getenv("MONGODB_NAME", "ai_service_db")
# Add near other config variables
CHATGPT_VISION_REASONING_MODEL = os.getenv("CHATGPT_VISION_REASONING_MODEL", "gpt-4o")
# ‚úÖ Data directory
DATA_DIR = os.getenv("DATA_DIR", "./data")

print(f"üîë API Keys loaded:")
print(f"   DeepSeek: {'‚úÖ' if DEEPSEEK_API_KEY else '‚ùå'}")
print(f"   ChatGPT: {'‚úÖ' if CHATGPT_API_KEY else '‚ùå'}")
print(f"   SerpAPI: {'‚úÖ' if SERPAPI_KEY else '‚ùå'}")
print(f"   Default AI: {DEFAULT_AI_PROVIDER}")
print(f"   MongoDB: {MONGODB_URI}")
print(f"   Data Dir: {DATA_DIR}")

# ‚úÖ System optimization
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

sys.path.append(str(Path(__file__).parent))
# Ensure src is in sys.path for absolute imports
src_path = str(Path(__file__).parent / "src")
if src_path not in sys.path:
    sys.path.append(src_path)
try:
    import faiss
    faiss.omp_set_num_threads(1) # Thi·∫øt l·∫≠p ngay sau khi import faiss
    print("‚úÖ FAISS omp_set_num_threads(1) applied in serve.py")
except ImportError:
    print("‚ö†Ô∏è FAISS not found, skipping omp_set_num_threads in serve.py")
except Exception as e_faiss_setup:
    print(f"‚ö†Ô∏è Error setting FAISS threads in serve.py: {e_faiss_setup}")

# TH√äM D√íNG N√ÄY
import nest_asyncio
nest_asyncio.apply()

from src.utils.logger import setup_logger
logger = setup_logger()

import json
import uuid
from datetime import datetime
import os

from src.models import (
    QuestionRequest,
    ChatWithFilesRequest,
    OCRRequest,
    CCCDImageRequest,
    CCCDOCRResponse,
    IDCardInfo,
    ExistingLoan,
    LoanApplicationRequest,
    LoanAssessmentResponse,
    validate_loan_application_minimal,
    build_assessment_context
)

# Create results directory
RESULTS_DIR = "results"
if not os.path.exists(RESULTS_DIR):
    os.makedirs(RESULTS_DIR)
    logger.info(f"‚úÖ Created results directory: {RESULTS_DIR}")

def save_real_estate_analysis_log(analysis_data: dict):
    """Save comprehensive real estate analysis log"""
    try:
        # Generate unique filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_id = str(uuid.uuid4())[:8]
        filename = f"real_estate_analysis_{timestamp}_{session_id}.json"
        filepath = os.path.join(RESULTS_DIR, filename)
        
        # Add metadata
        analysis_data.update({
            "timestamp": datetime.now().isoformat(),
            "session_id": session_id,
            "analysis_type": "real_estate_valuation",
            "version": "1.0"
        })
        
        # Save to file with proper formatting
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Real estate analysis saved: {filename}")
        return filepath
        
    except Exception as e:
        logger.error(f"‚ùå Error saving analysis log: {e}")
        return None

def save_web_search_detailed_log(search_data: dict):
    """Save detailed web search log with full content"""
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_id = str(uuid.uuid4())[:8]
        filename = f"web_search_detailed_{timestamp}_{session_id}.json"
        filepath = os.path.join(RESULTS_DIR, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(search_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Web search detailed log saved: {filename}")
        return filepath
        
    except Exception as e:
        logger.error(f"‚ùå Error saving web search log: {e}")
        return None


from src.rag.chatbot import Chatbot


# Khai b√°o bi·∫øn chatbot ·ªü m·ª©c global
chatbot = None
conversation_manager = None
ai_provider_manager = None  # TH√äM M·ªöI


# T·∫°o context manager cho lifespan c·ªßa ·ª©ng d·ª•ng
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Kh·ªüi t·∫°o chatbot khi ·ª©ng d·ª•ng kh·ªüi ƒë·ªông
    global chatbot, conversation_manager, ai_provider_manager
    try:
        print("üöÄ Starting AI Service initialization...")
        
        # Initialize components
        print("üìù Initializing Chatbot...")
        chatbot = Chatbot()
        print("‚úÖ Chatbot initialized")

        # Initialize AI Provider Manager
        print("ü§ñ Initializing AI Provider Manager...")
        ai_provider_manager = AIProviderManager(
            deepseek_api_key=DEEPSEEK_API_KEY, chatgpt_api_key=CHATGPT_API_KEY
        )
        print("‚úÖ AI Provider Manager initialized")

        print("üí¨ Initializing Conversation Manager...")
        conversation_manager = chatbot.conversation_manager
        print("‚úÖ Conversation Manager initialized")
        
        # ‚úÖ LOAD DOCUMENTS WITH PROPER ERROR HANDLING
        print("üìö Loading documents for RAG...")
        try:
            documents_loaded = await load_documents()
            if documents_loaded > 0:
                print(f"‚úÖ Documents loaded successfully: {documents_loaded} files")
            else:
                print("‚ö†Ô∏è No documents loaded - RAG will use empty context")
        except Exception as doc_error:
            print(f"‚ùå Failed to load documents: {doc_error}")
            print("‚ö†Ô∏è RAG will operate without document context")
        
        print("üéâ AI Service initialized successfully")
        yield
        
    except Exception as e:
        print(f"‚ùå Critical error during startup: {str(e)}")
        import traceback
        print(traceback.format_exc())
        
        # ‚úÖ STILL YIELD TO PREVENT APP CRASH
        print("‚ö†Ô∏è Starting with minimal functionality...")
        yield
        
    finally:
        print("üõë Shutting down AI Service...")
        # ‚úÖ CLEANUP IF NEEDED
        try:
            if chatbot:
                # Cleanup operations if any
                pass
        except Exception as cleanup_error:
            print(f"‚ö†Ô∏è Error during cleanup: {cleanup_error}")


# T·∫°o FastAPI app v·ªõi production config
app = FastAPI(
    title="AI Money Chatbot API",
    description="AI-powered real estate chatbot with RAG capabilities",
    version="1.0.0",
    docs_url="/docs" if DEBUG else None,  # Disable docs in production
    redoc_url="/redoc" if DEBUG else None,  # Disable redoc in production
    openapi_url="/openapi.json" if DEBUG else None,  # Disable OpenAPI schema in production
    lifespan=lifespan
)
# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Add security headers middleware
@app.middleware("http")
async def add_security_headers(request: Request, call_next):
    response = await call_next(request)
    
    # Security headers for production
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
    # Add server info
    response.headers["X-Powered-By"] = "AI Money Platform"
    response.headers["X-API-Version"] = "1.0.0"
    
    return response

class QuestionRequest(BaseModel):
    question: str
    userId: Optional[str] = None
    deviceId: Optional[str] = None


async def load_documents():
    try:
        # S·ª≠ d·ª•ng bi·∫øn m√¥i tr∆∞·ªùng DATA_DIR, m·∫∑c ƒë·ªãnh l√† './data'
        data_dir = os.getenv("DATA_DIR", "./data")
        print(f"üìÇ ƒêang ƒë·ªçc t√†i li·ªáu t·ª´ th∆∞ m·ª•c: {data_dir}")
        
        # Ki·ªÉm tra th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng
        if not os.path.exists(data_dir):
            print(f"‚ö†Ô∏è Th∆∞ m·ª•c {data_dir} kh√¥ng t·ªìn t·∫°i - t·∫°o th∆∞ m·ª•c m·ªõi")
            os.makedirs(data_dir, exist_ok=True)
            print(f"üìÅ T·∫°o th∆∞ m·ª•c {data_dir} th√†nh c√¥ng")
            return 0

        # Ki·ªÉm tra c√≥ files n√†o trong th∆∞ m·ª•c kh√¥ng
        files_in_dir = [f for f in os.listdir(data_dir) 
                       if os.path.isfile(os.path.join(data_dir, f)) 
                       and not f.startswith('.')]  # B·ªè hidden files
        
        print(f"üìÑ T√¨m th·∫•y {len(files_in_dir)} files trong {data_dir}")
        for file in files_in_dir[:5]:  # Show first 5 files
            print(f"   - {file}")
        if len(files_in_dir) > 5:
            print(f"   ... v√† {len(files_in_dir) - 5} files kh√°c")

        if len(files_in_dir) == 0:
            print(f"‚ö†Ô∏è Kh√¥ng c√≥ files n√†o trong th∆∞ m·ª•c {data_dir}")
            return 0

        # ‚úÖ TRY-CATCH CHO INGEST DOCUMENTS
        try:
            # S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c ingest_documents c·ªßa chatbot
            # ‚ö†Ô∏è TH√äM TIMEOUT V√Ä BETTER ERROR HANDLING
            def safe_ingest():
                try:
                    return chatbot.ingest_documents(data_dir)
                except Exception as e:
                    print(f"‚ùå Error in ingest_documents: {e}")
                    import traceback
                    print(traceback.format_exc())
                    return 0

            files_processed = await asyncio.get_event_loop().run_in_executor(
                None, safe_ingest
            )

            print(f"‚úÖ ƒê√£ x·ª≠ l√Ω {files_processed} file t·ª´ th∆∞ m·ª•c {data_dir}")
            
            # Ki·ªÉm tra s·ªë documents ƒë√£ load v√†o vector store
            try:
                total_docs = len(chatbot.vector_store.documents) if chatbot.vector_store else 0
                print(f"üìä Vector store hi·ªán c√≥ {total_docs} documents")
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë·∫øm documents trong vector store: {e}")
                total_docs = 0
            
            return files_processed

        except Exception as ingest_error:
            print(f"‚ùå Error during document ingestion: {ingest_error}")
            import traceback
            print(traceback.format_exc())
            return 0

    except Exception as e:
        print(f"‚ùå Error loading documents: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return 0


# Production health check with more info
@app.get("/ping")
def ping():
    if DEBUG:
        print("Ping endpoint called!")
    
    return {
        "status": "ok",
        "environment": ENV,
        "domain": DOMAIN,
        "timestamp": str(datetime.now()),
        "version": "1.0.0"
    }

@app.get("/health")
async def health_check():
    """Comprehensive health check for production monitoring"""
    global chatbot, ai_provider_manager
    
    health_status = {
        "status": "healthy",
        "timestamp": str(datetime.now()),
        "environment": ENV,
        "domain": DOMAIN,
        "base_url": BASE_URL,
        "services": {
            "chatbot": "initialized" if chatbot else "initializing",
            "ai_provider": "initialized" if ai_provider_manager else "initializing",
            "database": "connected",  # Add DB check if needed
        },
        "metrics": {
            "documents_loaded": len(chatbot.vector_store.documents) if chatbot and chatbot.vector_store else 0,
            "uptime": "unknown",  # Add uptime tracking if needed
        }
    }
    
    # Check AI providers availability
    if ai_provider_manager:
        health_status["ai_providers"] = {
            "deepseek": bool(DEEPSEEK_API_KEY),
            "chatgpt": bool(CHATGPT_API_KEY),
            "default": DEFAULT_AI_PROVIDER
        }
    
    return health_status

@app.get("/status")
async def status():
    global chatbot
    return {
        "status": "running",
        "documents_loaded": len(chatbot.vector_store.documents) if chatbot else 0,
        "api_status": "initialized" if chatbot else "initializing",
        "timestamp": str(datetime.now()),
    }


@app.post("/chat")
async def answer(req: QuestionRequest, request: Request):
    try:
        # ∆Øu ti√™n userId, fallback v·ªÅ deviceId
        user_id = req.userId if req.userId else req.deviceId

        # N·∫øu kh√¥ng c√≥ c·∫£ userId v√† deviceId, d√πng IP + user agent
        if not user_id:
            user_id = f"{request.client.host}_{request.headers.get('user-agent', '')}"

        if not chatbot:
            return {"answer": "System is initializing, please try again in a moment"}

        # S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c m·ªõi v·ªõi l·ªãch s·ª≠
        response = chatbot.generate_response_with_history(req.question, user_id)
        return {"answer": response}
    except Exception as e:
        logger.error(f"Error in /chat endpoint: {e}")
        return {"answer": "An error occurred while processing your request"}


@app.post("/chat-stream")
async def stream_answer(req: QuestionRequest, request: Request):
    if not chatbot:
        return StreamingResponse(
            iter(
                [
                    json.dumps(
                        {
                            "answer": "System is initializing, please try again in a moment"
                        }
                    )
                ]
            ),
            media_type="application/json",
        )

    # ∆Øu ti√™n userId, fallback v·ªÅ deviceId
    user_id = req.userId if req.userId else req.deviceId

    # N·∫øu kh√¥ng c√≥ c·∫£ userId v√† deviceId, d√πng IP + user agent
    if not user_id:
        user_id = f"{request.client.host}_{request.headers.get('user-agent', '')}"

    def generate():
        try:
            prefix = "[Theo d·ªØ li·ªáu ·ª©ng d·ª•ng] "
            yield f'data: {json.dumps({"chunk": prefix})}\n\n'

            # S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c streaming m·ªõi v·ªõi l·ªãch s·ª≠
            for chunk in chatbot.generate_response_streaming_with_history(
                req.question, user_id
            ):
                yield f'data: {json.dumps({"chunk": chunk})}\n\n'

            yield f'data: {json.dumps({"done": True})}\n\n'
        except Exception as e:
            error_msg = f"Error: {str(e)}"
            logger.error(f"Error in stream_answer: {e}")
            yield f'data: {json.dumps({"error": error_msg})}\n\n'
            yield f'data: {json.dumps({"done": True})}\n\n'

    return StreamingResponse(generate(), media_type="text/event-stream")


@app.post("/clear-history")
async def clear_history(req: QuestionRequest, request: Request):
    try:
        user_id = req.userId if req.userId else req.deviceId

        if not user_id:
            user_id = f"{request.client.host}_{request.headers.get('user-agent', '')}"

        # X√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i c·ªßa user
        deleted = chatbot.db_manager.clear_history(user_id)

        return {"success": True, "message": "Chat history cleared"}
    except Exception as e:
        logger.error(f"Error clearing history: {e}")
        return {"success": False, "message": "Error clearing history"}


# S·ª¨A L·∫†I class ChatWithFilesRequest
class ChatWithFilesRequest(BaseModel):
    question: str
    userId: Optional[str] = None
    deviceId: Optional[str] = None
    files: Optional[List[Dict[str, Any]]] = None
    context: Optional[str] = None
    ai_provider: Optional[str] = "deepseek"  # TH√äM M·ªöI
    use_backend_ocr: Optional[bool] = True  # TH√äM M·ªöI
    url_image: Optional[str] = None  # ‚Üê TH√äM FIELD M·ªöI CHO URL IMAGE


# THAY TH·∫æ endpoint /chat-with-files-stream c≈©
@app.post("/chat-with-files-stream")
async def chat_with_files_stream(request: ChatWithFilesRequest):
    """
    ‚úÖ CLEAN ROUTER: AI Provider Selection Only
    """
    def generate():
        try:
            # ‚úÖ STEP 1: VALIDATE CHATBOT AVAILABILITY
            if not chatbot:
                yield f'data: {json.dumps({"chunk": "System is initializing, please try again in a moment"})}\n\n'
                yield f'data: {json.dumps({"done": True})}\n\n'
                return

            # ‚úÖ STEP 2: USER ID RESOLUTION
            user_id = request.userId or request.deviceId or "anonymous"

            # ‚úÖ STEP 3: FILE ANALYSIS FOR AI PROVIDER SELECTION
            image_files = []
            document_files = []

            if request.files:
                for file_data in request.files:
                    content_type = file_data.get("content_type", "")
                    if content_type and content_type.startswith("image/"):
                        image_files.append(file_data)
                    else:
                        document_files.append(file_data)

            # ‚úÖ STEP 4: SMART AI PROVIDER SELECTION
            has_images = len(image_files) > 0
            has_documents = len(document_files) > 0

            if has_images and not has_documents:
                ai_provider = "chatgpt"
                processing_mode = "images_only"
                mode_msg = f"üì∏ B·∫°n ƒë√£ ƒë√≠nh k√®m {len(image_files)} h√¨nh ·∫£nh - AI ƒëang ƒë·ªçc file c·ªßa b·∫°n.."
            elif has_documents and not has_images:
                ai_provider = "deepseek"
                processing_mode = "documents_only"
                mode_msg = f"üìÑ B·∫°n ƒë√£ ƒë√≠nh k√®m {len(document_files)} t√†i li·ªáu - AI ƒëang ƒë·ªçc file c·ªßa b·∫°n.."
            elif has_images and has_documents:
                ai_provider = "chatgpt"
                processing_mode = "mixed_files"
                mode_msg = f"üìé B·∫°n ƒë√£ ƒë√≠nh k√®m ({len(image_files)} h√¨nh ·∫£nh v√† {len(document_files)} t√†i li·ªáu) - AI ƒëang ƒë·ªçc file c·ªßa b·∫°n.."
            else:
                ai_provider = request.ai_provider or "deepseek"
                processing_mode = "no_files"
                mode_msg = f"üí¨ Ch·ªâ c√≥ c√¢u h·ªèi - s·ª≠ d·ª•ng {ai_provider.upper()}"

            # ‚úÖ STEP 5: SEND PROCESSING MODE INFO
            yield f'data: {json.dumps({"chunk": mode_msg})}\n\n'
            # yield f'data: {json.dumps({"chunk": "ü§ñ ƒêang k·∫øt n·ªëi v·ªõi AI..."})}\n\n'
            logger.info(f"‚úÖ Router: {ai_provider}, Mode: {processing_mode}")

            # ‚úÖ STEP 6: DELEGATE TO CHATBOT WITH STREAMING
            chunk_count = 0
            start_time = time.time()
            

            try:
                
                def run_chatbot_simple():
                    """Simple sync wrapper - no complex threading"""
                    try:
                        import asyncio
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        
                        async def collect_all_chunks():
                            chunks = []
                            async for chunk in chatbot.process_files_streaming(
                                query=request.question,
                                user_id=user_id,
                                files=request.files or [],
                                ai_provider=ai_provider,
                                ai_provider_manager=ai_provider_manager
                            ):
                                chunks.append(chunk)
                            return chunks
                        
                        # Get all chunks at once
                        all_chunks = loop.run_until_complete(collect_all_chunks())
                        loop.close()
                        return all_chunks
                        
                    except Exception as e:
                        logger.error(f"Chatbot simple call error: {e}")
                        return [f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω: {str(e)}"]
                
                # ‚úÖ GET CHUNKS - SAME PATTERN AS /chat-stream
                all_chunks = run_chatbot_simple()
                
                # ‚úÖ STREAM CHUNKS ONE BY ONE - EXACT SAME AS /chat-stream
                for chunk in all_chunks:
                    chunk_count += 1
                    yield f'data: {json.dumps({"chunk": chunk})}\n\n'
                
                # ‚úÖ COMPLETION - SAME AS /chat-stream
                total_time = time.time() - start_time
                yield f'data: {json.dumps({"done": True})}\n\n'
                logger.info(f"‚úÖ Chat with files completed: {chunk_count} chunks in {total_time:.1f}s")
                
            except Exception as e:
                # ‚úÖ ERROR HANDLING - EXACT SAME AS /chat-stream
                error_msg = f"Error: {str(e)}"
                logger.error(f"Error in chat_with_files_stream: {e}")
                yield f'data: {json.dumps({"error": error_msg})}\n\n'
                yield f'data: {json.dumps({"done": True})}\n\n'

                # ‚úÖ STEP 7: COMPLETION WITH METADATA
                total_time = time.time() - start_time
                completion_msg = {
                    "done": True,
                    "metadata": {
                        "provider": ai_provider,
                        "mode": processing_mode,
                        "chunks": chunk_count,
                        "duration": round(total_time, 2),
                        "files_processed": len(request.files) if request.files else 0,
                        "images": len(image_files),
                        "documents": len(document_files)
                    }
                }

                yield f'data: {json.dumps(completion_msg)}\n\n'

                logger.info(f"‚úÖ Chat with files completed: {chunk_count} chunks in {total_time:.1f}s")

            except Exception as processing_error:
                logger.error(f"‚ùå Processing error: {processing_error}")
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")

                error_msg = f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω: {str(processing_error)}"
                yield f'data: {json.dumps({"chunk": error_msg})}\n\n'
                yield f'data: {json.dumps({"done": True, "error": True})}\n\n'

        except Exception as outer_error:
            logger.error(f"‚ùå Outer chat-with-files error: {outer_error}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")

            error_msg = f"‚ö†Ô∏è L·ªói h·ªá th·ªëng: {str(outer_error)}"
            yield f'data: {json.dumps({"chunk": error_msg})}\n\n'
            yield f'data: {json.dumps({"done": True, "error": True})}\n\n'

    return StreamingResponse(generate(), media_type="text/event-stream")


def cleanup_task():
    try:
        if chatbot:
            deleted_count = chatbot.cleanup_old_conversations(days=3)
            logger.info(f"Cleaned up {deleted_count} old conversations")
    except Exception as e:
        logger.error(f"Error in cleanup task: {e}")


# S·ª≠ d·ª•ng schedule ƒë·ªÉ ch·∫°y task m·ªói ng√†y l√∫c 3 gi·ªù s√°ng
def run_scheduler():
    schedule.every().day.at("03:00").do(cleanup_task)

    while True:
        schedule.run_pending()
        time.sleep(60)


# TH√äM endpoint m·ªõi
@app.get("/ai-providers")
async def get_available_providers():
    """Endpoint ƒë·ªÉ FE bi·∫øt providers n√†o c√≥ th·ªÉ d√πng"""
    providers = {
        "deepseek": {
            "available": bool(DEEPSEEK_API_KEY),
            "supports_multimodal": False,
            "requires_backend_ocr": True,
        },
        "chatgpt": {
            "available": bool(CHATGPT_API_KEY),
            "supports_multimodal": True,
            "requires_backend_ocr": False,
        },
    }

    return {"providers": providers, "default": DEFAULT_AI_PROVIDER}


@app.post("/real-estate/deepseek-reasoning")
async def real_estate_deepseek_reasoning(request: ChatWithFilesRequest):
    """
    ‚úÖ COMPLETE: Real estate analysis with DeepSeek reasoning + web search + file processing
    """
    
    def generate():
        # ‚úÖ IMPORTS AT FUNCTION START
        import time as time_module
        import asyncio
        import concurrent.futures
        import threading
        import queue
        import os
        import glob
        
        # ‚úÖ INITIALIZE COMPREHENSIVE ANALYSIS LOG
        analysis_log = {
            "request": {
                "question": request.question,
                "files_count": len(request.files) if request.files else 0,
                "user_id": request.userId or "anonymous",
                "timestamp": datetime.now().isoformat()
            },
            "processing_steps": [],
            "web_search": {
                "search_query": None,
                "target_urls": [],
                "full_responses": {},
                "properties_found": [],
                "performance_metrics": {}
            },
            "processed_files": [],
            "ai_response": "",
            "performance_metrics": {},
            "errors": []
        }
        
        start_time = time_module.time()
        try:
            # ‚úÖ INITIALIZE VARIABLES
            full_response = ""
            processed_files = []
            user_id = request.userId or "anonymous"
            web_search_data = None
            analysis = None

            # ‚úÖ B∆Ø·ªöC 0: QUICK ANALYSIS
            logger.info("=== STEP 0: QUICK ANALYSIS ===")
            analysis = analyze_real_estate_query(request.question)
            logger.info(f"Quick Analysis - Confidence: {analysis.confidence:.2f}")
            
            # ‚úÖ LOG STEP 0 - DETAILED ANALYSIS
            analysis_log["processing_steps"].append({
                "step": 0,
                "name": "quick_analysis",
                "timestamp": time_module.time(),
                "result": {
                    "property_type": analysis.property_type,
                    "project_name": analysis.project_name,
                    "location": {
                        "province": analysis.location.province,
                        "district": analysis.location.district
                    },
                    "search_query": analysis.search_query,
                    "confidence": analysis.confidence,
                    "dientich": analysis.dientich,
                    "bedrooms": analysis.bedrooms
                }
            })

            # ‚úÖ SAVE SEARCH QUERY TO LOG
            if analysis.search_query:
                analysis_log["web_search"]["search_query"] = analysis.search_query

            # ‚úÖ B∆Ø·ªöC 1: FILE PROCESSING
            if request.files and len(request.files) > 0:
                logger.info("=== STEP 1: PROCESSING FILES ===")
                
                files_msg = f'üìÑ ƒêang x·ª≠ l√Ω {len(request.files)} t√†i li·ªáu...'
                yield f'data: {json.dumps({"chunk": files_msg})}\n\n'

                file_processing_start = time_module.time()

                files_result = []
                for i, file_data in enumerate(request.files):
                    filename = file_data.get("filename", f"file_{i}")
                    content_type = file_data.get("content_type", "")
                    
                    file_log = {
                        "filename": filename,
                        "content_type": content_type,
                        "file_size": len(file_data.get("content", "")),
                        "extraction_start": time_module.time()
                    }

                    extracted_text = ""
                    extraction_method = ""
                    
                    # Image processing
                    if content_type and content_type.startswith("image/"):
                        image_url = file_data.get("url", "") or file_data.get("public_url", "")
                        file_log["image_url"] = image_url
                        if image_url:
                            try:
                                loop = asyncio.new_event_loop()
                                asyncio.set_event_loop(loop)
                                extracted_text = loop.run_until_complete(
                                    extract_text_with_chatgpt_vision_url(image_url, filename)
                                )
                                loop.close()
                                extraction_method = "ChatGPT Vision"
                            except Exception as e:
                                logger.error(f"Vision OCR error: {e}")
                                extracted_text = f"[L·ªói OCR: {str(e)}]"
                                extraction_method = "Failed"
                                file_log["error"] = str(e)
                        else:
                            extracted_text = f"[Kh√¥ng c√≥ URL h√¨nh ·∫£nh cho {filename}]"
                            extraction_method = "Failed - No URL"
                    
                    # Document processing
                    elif content_type in ["application/pdf", "application/vnd.openxmlformats-officedocument.wordprocessingml.document"]:
                        file_base64 = file_data.get("content", "")
                        if file_base64:
                            try:
                                if content_type == "application/pdf":
                                    loop = asyncio.new_event_loop()
                                    asyncio.set_event_loop(loop)
                                    extracted_text = loop.run_until_complete(
                                        loop.run_in_executor(None, extract_text_from_pdf, file_base64)
                                    )
                                    loop.close()
                                    extraction_method = "PyMuPDF"
                                else:
                                    loop = asyncio.new_event_loop()
                                    asyncio.set_event_loop(loop)
                                    extracted_text = loop.run_until_complete(
                                        loop.run_in_executor(None, extract_text_from_docx, file_base64)
                                    )
                                    loop.close()
                                    extraction_method = "python-docx"
                            except Exception as e:
                                logger.error(f"Document processing error: {e}")
                                extracted_text = f"[L·ªói x·ª≠ l√Ω t√†i li·ªáu: {str(e)}]"
                                extraction_method = "Failed"
                                file_log["error"] = str(e)
                    
                    # Complete file log
                    file_log.update({
                        "extracted_text": extracted_text,
                        "extraction_method": extraction_method,
                        "text_length": len(extracted_text),
                        "success": len(extracted_text.strip()) > 10,
                        "processing_time": time_module.time() - file_log["extraction_start"]
                    })
                    # Save processed file
                    files_result.append(file_log)
                    
                    if len(extracted_text.strip()) > 10:
                        success_msg = f'‚úÖ {filename}: {len(extracted_text)} k√Ω t·ª± ({extraction_method})'
                        yield f'data: {json.dumps({"chunk": success_msg})}\n\n'
                
                processed_files = files_result
                file_processing_time = time_module.time() - file_processing_start

                # ‚úÖ LOG STEP 1
                analysis_log["processing_steps"].append({
                    "step": 1,
                    "name": "file_processing",
                    "timestamp": time_module.time(),
                    "duration": file_processing_time,
                    "result": {
                        "files_processed": len(processed_files),
                        "successful_files": len([f for f in processed_files if f["success"]]),
                        "total_text_length": sum(len(f["extracted_text"]) for f in processed_files),
                        "files_details": processed_files
                    }
                })
                analysis_log["processed_files"] = processed_files

            # ‚úÖ B∆Ø·ªöC 2: WEB SEARCH WITH TIMEOUT
            if analysis.search_query and analysis.confidence > 0.5:
                search_msg = f'üîç T√¨m ki·∫øm BƒêS: {analysis.property_type or "BƒêS"} t·∫°i {analysis.location.province or "ƒë·ªãa ph∆∞∆°ng"}'
                yield f'data: {json.dumps({"chunk": search_msg})}\n\n'
                
                search_start = time_module.time()
                
                try:
                    logger.info("üîç DEBUG: Starting web search with 10s timeout...")
                    
                    def sync_web_search():
                        try:
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                            result = loop.run_until_complete(
                                search_real_estate_properties_with_logging(
                                    search_query=analysis.search_query,
                                    analysis_log=analysis_log
                                )
                            )
                            loop.close()
                            return result
                        except Exception as e:
                            logger.error(f"Sync web search error: {e}")
                            return None
                    
                    # Run in background with timeout
                    executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
                    future = executor.submit(sync_web_search)
                    
                    try:
                        web_search_data = future.result(timeout=10)
                        search_time = time_module.time() - search_start
                        logger.info(f"üîç DEBUG: Web search completed in {search_time:.1f}s")

                        # ‚úÖ LOG WEB SEARCH PERFORMANCE
                        analysis_log["web_search"]["performance_metrics"] = {
                            "total_time": search_time,
                            "timeout": False,
                            "success": web_search_data is not None
                        }
                        
                    except concurrent.futures.TimeoutError:
                        search_time = time_module.time() - search_start
                        logger.info(f"üîç DEBUG: Web search timeout after {search_time:.1f}s - continuing...")
                        web_search_data = None
                        future.cancel()
                        
                        analysis_log["web_search"]["performance_metrics"] = {
                            "total_time": search_time,
                            "timeout": True,
                            "success": False
                        }
                        
                    except Exception as search_error:
                        logger.error(f"üîç DEBUG: Web search error: {search_error}")
                        web_search_data = None
                        error_msg = f'‚ö†Ô∏è L·ªói t√¨m ki·∫øm: {str(search_error)}'
                        yield f'data: {json.dumps({"chunk": error_msg})}\n\n'
                        
                        analysis_log["errors"].append({
                            "step": 2,
                            "error_type": "web_search_error",
                            "error_message": str(search_error),
                            "timestamp": time_module.time()
                        })
                    
                    finally:
                        executor.shutdown(wait=False)
                    
                    # Process web search results
                    if web_search_data and isinstance(web_search_data, dict):
                        all_properties = web_search_data.get('all_properties', [])
                        actual_count = len(all_properties)
                        
                        if actual_count > 0:
                            successful_sites = web_search_data.get('successful_websites', 0)
                            processing_time = web_search_data.get('processing_time', 0)
                            
                            search_result_msg = f'‚ö° T√¨m th·∫•y {actual_count} BƒêS t·ª´ {successful_sites}/3 website trong {processing_time:.1f}s'
                            yield f'data: {json.dumps({"chunk": search_result_msg})}\n\n'
                            
                            # Show top 3 properties
                            for i, prop in enumerate(all_properties[:3]):
                                title_short = prop.get('title', 'No title')[:50]
                                if len(prop.get('title', '')) > 50:
                                    title_short += "..."
                                
                                price = prop.get('price', '') or "Li√™n h·ªá"
                                area = prop.get('area', '') or ""
                                website = prop.get('website', 'unknown')
                                
                                prop_msg = f'  {i+1}. {title_short}'
                                if price and price != "Li√™n h·ªá":
                                    prop_msg += f' | {price}'
                                if area:
                                    prop_msg += f' | {area}'
                                prop_msg += f' | {website}'
                                
                                yield f'data: {json.dumps({"chunk": prop_msg})}\n\n'
                        else:
                            yield f'data: {json.dumps({"chunk": "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu li√™n quan"})}\n\n'
                    
                    # ‚úÖ LOG STEP 2
                    analysis_log["processing_steps"].append({
                        "step": 2,
                        "name": "web_search",
                        "timestamp": time_module.time(),
                        "duration": search_time,
                        "search_query": analysis.search_query,
                        "result": {
                            "success": web_search_data is not None,
                            "properties_found": len(web_search_data.get('all_properties', [])) if web_search_data else 0,
                            "successful_websites": web_search_data.get('successful_websites', 0) if web_search_data else 0,
                            "timeout": search_time >= 15,
                            "raw_data": web_search_data
                        }
                    })

                except Exception as outer_error:
                    logger.error(f"üîç DEBUG: Outer web search error: {outer_error}")
                    # ‚úÖ LOG ERROR
                    analysis_log["errors"].append({
                        "step": 2,
                        "error_type": "web_search_outer_error",
                        "error_message": str(outer_error),
                        "timestamp": time_module.time()
                    })
                    web_search_data = None
                    error_msg = f'‚ö†Ô∏è L·ªói h·ªá th·ªëng t√¨m ki·∫øm: {str(outer_error)}'
                    yield f'data: {json.dumps({"chunk": error_msg})}\n\n'
            else:
                confidence_percent = round(analysis.confidence * 100)
                skip_msg = f'D√πng d·ªØ li·ªáu t·ªïng qu√°t (ƒë·ªô tin c·∫≠y: {confidence_percent}%)'
                yield f'data: {json.dumps({"chunk": skip_msg})}\n\n'
                web_search_data = None

                # ‚úÖ LOG SKIPPED SEARCH
                analysis_log["processing_steps"].append({
                    "step": 2,
                    "name": "web_search_skipped",
                    "timestamp": time_module.time(),
                    "reason": "low_confidence" if analysis.confidence <= 0.5 else "no_search_query",
                    "confidence": analysis.confidence
                })

            # ‚úÖ FALLBACK: Try to read from saved JSON
            if not web_search_data:
                logger.info("üîç DEBUG: Attempting to read from saved JSON file...")
                try:
                    # import os
                    # import glob

                    log_files = glob.glob("web_search_log_*.json")
                    if log_files:
                        latest_file = max(log_files, key=os.path.getmtime)
                        file_age = time_module.time() - os.path.getmtime(latest_file)
                        
                        if file_age < 60:
                            logger.info(f"üîç DEBUG: Reading from recent file: {latest_file} (age: {file_age:.1f}s)")
                            
                            with open(latest_file, 'r', encoding='utf-8') as f:
                                file_data = json.load(f)
                            
                            if file_data.get('all_properties_details'):
                                properties = file_data['all_properties_details']
                                
                                web_search_data = {
                                    'total_properties': len(properties),
                                    'successful_websites': len([r for r in file_data.get('website_results_summary', []) if r.get('success')]),
                                    'all_properties': properties,
                                    'processing_time': file_data.get('total_elapsed', 0),
                                    'is_partial': True,
                                    'website_results': file_data.get('website_results_summary', []),
                                    'status': 'recovered_from_file'
                                }
                                
                                # Get website names
                                website_names = []
                                # ‚úÖ METHOD 1: From website_results_summary (corrected key)
                                for result in file_data.get('website_results_summary', []):
                                    if result.get('success') and result.get('properties'):
                                        website_names.append(result.get('website', 'unknown'))
                                
                                # ‚úÖ METHOD 2: From all_properties_details if method 1 fails
                                if not website_names and properties:
                                    seen_websites = set()
                                    for prop in properties:
                                        website = prop.get('website', '')
                                        if website and website not in seen_websites:
                                            website_names.append(website)
                                            seen_websites.add(website)
                                
                                # ‚úÖ METHOD 3: Check for other possible keys
                                if not website_names:
                                    # Check website_results instead of website_results_summary
                                    for result in file_data.get('website_results', []):
                                        if result.get('success') and result.get('properties'):
                                            website_names.append(result.get('website', 'unknown'))
                                
                                # ‚úÖ METHOD 4: Debug - log all available keys
                                logger.info(f"üîç DEBUG: File data keys: {list(file_data.keys())}")
                                if 'website_results_summary' in file_data:
                                    logger.info(f"üîç DEBUG: website_results_summary: {file_data['website_results_summary']}")
                                if 'website_results' in file_data:
                                    logger.info(f"üîç DEBUG: website_results: {file_data['website_results']}")
                                
                                # ‚úÖ FINAL WEBSITE LIST
                                if website_names:
                                    website_list = ', '.join(website_names)
                                    logger.info(f"üîç DEBUG: Found websites: {website_list}")
                                else:
                                    website_list = 'd·ªØ li·ªáu cache'
                                    logger.warning(f"üîç DEBUG: No website names found, using fallback")
                                
                                logger.info(f"üîç DEBUG: Recovered {len(properties)} properties from file")
                                recovered_msg = f'‚ö° ƒê√£ t√¨m th·∫•y {len(properties)} BƒêS t·ª´ {website_list}. ƒêang ph√¢n t√≠ch v·ªõi AI v·ªÅ d·ªØ li·ªáu web t√¨m th·∫•y...'
                                yield f'data: {json.dumps({"chunk": recovered_msg})}\n\n'
                            else:
                                logger.info(f"üîç DEBUG: File exists but no properties found")
                        else:
                            logger.info(f"üîç DEBUG: File too old ({file_age:.1f}s), skipping")
                    else:
                        logger.info(f"üîç DEBUG: No web_search_log files found")
                        
                except Exception as file_error:
                    logger.error(f"üîç DEBUG: Error reading from file: {file_error}")

            # ‚úÖ B∆Ø·ªöC 3: BUILD CONTEXT
            logger.info("üîç DEBUG: ===== FORCED CONTINUATION AFTER WEB SEARCH =====")
            logger.info(f"üîç DEBUG: web_search_data is not None: {web_search_data is not None}")
            
            web_data_context = ""
            if web_search_data and isinstance(web_search_data, dict):
                all_properties = web_search_data.get('all_properties', [])
                total_properties = len(all_properties)
                
                if total_properties > 0:
                    processing_time = web_search_data.get('processing_time', 0)
                    is_partial = web_search_data.get('is_partial', False)
                    successful_websites = web_search_data.get('successful_websites', 0)
                    
                    status_note = " (d·ªØ li·ªáu m·ªôt ph·∫ßn do timeout)" if is_partial else " (d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß)"
                    
                    web_data_context = f"\n\n=== D·ªÆ LI·ªÜU B·∫§T ƒê·ªòNG S·∫¢N T·ª™ WEB{status_note} ===\n"
                    web_data_context += f"T√¨m th·∫•y {total_properties} b·∫•t ƒë·ªông s·∫£n t·ª´ {successful_websites}/3 website:\n"
                    
                    # Build context from all_properties
                    website_groups = {}
                    for prop in all_properties:
                        website = prop.get('website', 'unknown')
                        if website not in website_groups:
                            website_groups[website] = []
                        website_groups[website].append(prop)
                    
                    for website, props in website_groups.items():
                        property_count = len(props)
                        web_data_context += f"\n--- {website} ({property_count} BƒêS) ---\n"
                        
                        for i, prop in enumerate(props[:8], 1):
                            web_data_context += f"{i}. {prop.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')}\n"
                            if prop.get('price'):
                                web_data_context += f"   üí∞ Gi√°: {prop['price']}\n"
                            if prop.get('area'):
                                web_data_context += f"   üìê Di·ªán t√≠ch: {prop['area']}\n"
                            if prop.get('detail') and len(prop['detail']) > 20:
                                detail_short = prop['detail'][:100]
                                if len(prop['detail']) > 100:
                                    detail_short += "..."
                                web_data_context += f"   üìù Chi ti·∫øt: {detail_short}\n"
                            web_data_context += "\n"
                            
                    web_data_context += f"\nüïê D·ªØ li·ªáu c·∫≠p nh·∫≠t: {processing_time:.1f}s tr∆∞·ªõc\n"
                    
                    logger.info(f"üîç DEBUG: Built web_data_context: {len(web_data_context)} chars")
                else:
                    web_data_context = "\n‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu web, s·ª≠ d·ª•ng ki·∫øn th·ª©c chung ƒë·ªÉ ƒë·ªãnh gi√°\n"
            else:
                web_data_context = "\n‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu web, s·ª≠ d·ª•ng ki·∫øn th·ª©c chung ƒë·ªÉ ƒë·ªãnh gi√°\n"

            # Build file content
            file_content_text = ""
            if processed_files:
                file_content_text = "\n\n=== T√ÄI LI·ªÜU ƒê√É X·ª¨ L√ù ===\n"
                for file_info in processed_files:
                    file_content_text += f"\n--- {file_info['filename']} ---\n{file_info['extracted_text']}\n"

            # ‚úÖ B∆Ø·ªöC 4: BUILD SYSTEM PROMPT
            system_prompt = f"""
B·∫°n l√† CHUY√äN GIA TH·∫®M ƒê·ªäNH V√Ä ƒê·ªäNH GI√Å B·∫§T ƒê·ªòNG S·∫¢N h√†ng ƒë·∫ßu Vi·ªát Nam v·ªõi 25 nƒÉm kinh nghi·ªám.

üéØ NHI·ªÜM V·ª§: Ph√¢n t√≠ch ƒë·ªãnh gi√° b·∫•t ƒë·ªông s·∫£n v·ªõi reasoning chi ti·∫øt t·ª´ng b∆∞·ªõc.

üìä D·ªÆ LI·ªÜU PH√ÇN T√çCH:
- C√¢u h·ªèi: {request.question}
- Lo·∫°i BƒêS: {analysis.property_type or 'Ch∆∞a x√°c ƒë·ªãnh'}
- D·ª± √°n: {analysis.project_name or 'Ch∆∞a x√°c ƒë·ªãnh'}  
- V·ªã tr√≠: {analysis.location.province or 'Ch∆∞a x√°c ƒë·ªãnh'}, {analysis.location.district or ''}
- Di·ªán t√≠ch: {analysis.dientich or 'Ch∆∞a x√°c ƒë·ªãnh'}
- Ph√≤ng ng·ªß: {analysis.bedrooms or 'Ch∆∞a x√°c ƒë·ªãnh'}
- ƒê·ªô tin c·∫≠y truy v·∫•n: {round(analysis.confidence * 100)}%

{web_data_context}

{file_content_text}

===== PH∆Ø∆†NG PH√ÅP REASONING T·ª™NG B∆Ø·ªöC =====

üîç B∆Ø·ªöC 1: PH√ÇN T√çCH TH√îNG TIN C∆† B·∫¢N
- X√°c ƒë·ªãnh ch√≠nh x√°c lo·∫°i b·∫•t ƒë·ªông s·∫£n, v·ªã tr√≠, di·ªán t√≠ch
- ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng v√† ƒë·ªô tin c·∫≠y c·ªßa th√¥ng tin

üìä B∆Ø·ªöC 2: PH√ÇN T√çCH D·ªÆ LI·ªÜU TH·ªä TR∆Ø·ªúNG  
- So s√°nh v·ªõi c√°c b·∫•t ƒë·ªông s·∫£n t∆∞∆°ng t·ª± t·ª´ d·ªØ li·ªáu web
- X√°c ƒë·ªãnh m·ª©c gi√° trung b√¨nh, cao nh·∫•t, th·∫•p nh·∫•t
- Ph√¢n t√≠ch xu h∆∞·ªõng gi√° theo khu v·ª±c

üíé B∆Ø·ªöC 3: ƒê√ÅNH GI√Å C√ÅC Y·∫æU T·ªê GI√Å TR·ªä
- V·ªã tr√≠: Ti·ªán √≠ch xung quanh, giao th√¥ng, h·∫° t·∫ßng
- Ph√°p l√Ω: S·ªï ƒë·ªè, gi·∫•y ph√©p, quy ho·∫°ch
- Ch·∫•t l∆∞·ª£ng: ƒê·ªô m·ªõi, n·ªôi th·∫•t, ti·ªán nghi
- Th·ªã tr∆∞·ªùng: Cung c·∫ßu, thanh kho·∫£n

üßÆ B∆Ø·ªöC 4: T√çNH TO√ÅN GI√Å TR·ªä CH√çNH X√ÅC
- Gi√° tr·ªã th·ªã tr∆∞·ªùng hi·ªán t·∫°i (d·ª±a tr√™n d·ªØ li·ªáu th·ª±c t·∫ø)
- Gi√° tr·ªã th·∫©m ƒë·ªãnh ng√¢n h√†ng (85% gi√° th·ªã tr∆∞·ªùng)
- Kh·∫£ nƒÉng vay th·∫ø ch·∫•p t·ªëi ƒëa

‚ö†Ô∏è B∆Ø·ªöC 5: PH√ÇN T√çCH R·ª¶I RO V√Ä KHUY·∫æN NGH·ªä
- R·ªßi ro ph√°p l√Ω, thanh kho·∫£n, bi·∫øn ƒë·ªông gi√°  
- ƒêi·ªÅu ki·ªán v√† l∆∞u √Ω c·∫ßn thi·∫øt
- ƒê√°nh gi√° ƒë·ªô tin c·∫≠y ph√¢n t√≠ch

L∆ØU √ù QUAN TR·ªåNG: 
- S·ª≠ d·ª•ng d·ªØ li·ªáu web m·ªõi nh·∫•t ƒë·ªÉ ƒë∆∞a ra ƒë·ªãnh gi√° ch√≠nh x√°c nh·∫•t
- Reasoning ph·∫£i logic, c√≥ cƒÉn c·ª© v√† s·ªë li·ªáu c·ª• th·ªÉ
"""

            # ‚úÖ B∆Ø·ªöC 5: GET CONVERSATION HISTORY
            history_messages = conversation_manager.get_optimized_messages(
                user_id=user_id, 
                rag_context="",
                current_query=request.question
            )

            # Create messages
            messages = [{"role": "system", "content": system_prompt}]
            if history_messages:
                messages.extend(history_messages)
            messages.append({"role": "user", "content": request.question})

            logger.info(f"Messages prepared: {len(messages)} total")

            # ‚úÖ B∆Ø·ªöC 6: AI STREAMING WITH THREADING
            try:
                logger.info("üîç DEBUG: Starting DeepSeek real-time threading stream...")
                
                
                # ‚úÖ SIMPLE ASYNC TO SYNC CONVERSION - SAME AS /chat-with-files-stream
                def run_ai_simple():
                    """Simple sync wrapper for DeepSeek AI"""
                    try:
                        import asyncio
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        
                        async def collect_ai_chunks():
                            chunks = []
                            async for chunk in ai_provider_manager.chat_completion_stream_with_reasoning(
                                messages, "deepseek", use_reasoning=True
                            ):
                                chunks.append(chunk)
                            return chunks
                        
                        # Get all chunks at once
                        all_chunks = loop.run_until_complete(collect_ai_chunks())
                        loop.close()
                        return all_chunks
                        
                    except Exception as e:
                        logger.error(f"AI simple call error: {e}")
                        return [f"‚ö†Ô∏è L·ªói AI: {str(e)}"]
                
                # ‚úÖ GET AI CHUNKS - SAME PATTERN AS WORKING /chat-with-files-stream
                all_ai_chunks = run_ai_simple()
                
                # ‚úÖ STREAM AI CHUNKS ONE BY ONE - EXACT SAME AS OTHER ENDPOINTS
                ai_chunk_count = 0
                for chunk in all_ai_chunks:
                    ai_chunk_count += 1
                    full_response += chunk
                    yield f'data: {json.dumps({"chunk": chunk})}\n\n'
                    
                    # Progress logging every 25 chunks
                    if ai_chunk_count % 25 == 0:
                        logger.info(f"üìä Streamed {ai_chunk_count} AI chunks")
                
                logger.info(f"‚úÖ AI streaming completed: {ai_chunk_count} chunks, {len(full_response)} chars")
                    
            except Exception as ai_error:
                logger.error(f"‚ùå AI streaming error: {ai_error}")
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")
                error_chunk = f"‚ö†Ô∏è L·ªói AI: {str(ai_error)}"
                yield f'data: {json.dumps({"chunk": error_chunk})}\n\n'

            # ‚úÖ B∆Ø·ªöC 7: SAVE CONVERSATION HISTORY
            try:
                conversation_manager.add_message(user_id, "user", request.question)
                conversation_manager.add_message(user_id, "assistant", full_response)
            except Exception as history_error:
                logger.error(f"Error saving history: {history_error}")

            # ‚úÖ B∆Ø·ªöC 8: COMPLETION
            yield f'data: {json.dumps({"done": True})}\n\n'

        # ‚úÖ CALCULATE FINAL PERFORMANCE METRICS
            total_time = time_module.time() - start_time
            analysis_log["performance_metrics"] = {
                "total_duration": total_time,
                "start_time": start_time,
                "end_time": time_module.time(),
                "steps_completed": len(analysis_log["processing_steps"]),
                "errors_count": len(analysis_log["errors"]),
                "success": len(analysis_log["errors"]) == 0
            }

            # ‚úÖ SAVE COMPREHENSIVE ANALYSIS LOG
            try:
                log_filepath = save_real_estate_analysis_log(analysis_log)
                if log_filepath:
                    logger.info(f"üìÑ Analysis log saved: {log_filepath}")
                    
                    # ‚úÖ SAVE DETAILED WEB SEARCH LOG SEPARATELY
                    if analysis_log["web_search"]["properties_found"]:
                        web_log_filepath = save_web_search_detailed_log(analysis_log["web_search"])
                        if web_log_filepath:
                            logger.info(f"üåê Web search log saved: {web_log_filepath}")
                    
                    # ‚úÖ SEND LOG INFO TO USER
                    # log_msg = f'üìÑ Ph√¢n t√≠ch ƒë√£ ƒë∆∞·ª£c l∆∞u: {os.path.basename(log_filepath)}'
                    # yield f'data: {json.dumps({"chunk": log_msg})}\n\n'
                    
            except Exception as log_error:
                logger.error(f"‚ùå Error saving analysis log: {log_error}")

            # ‚úÖ COMPLETION
            yield f'data: {json.dumps({"done": True})}\n\n'

        except Exception as e:
            logger.error(f"Real estate reasoning error: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            # ‚úÖ LOG SYSTEM ERROR
            analysis_log["errors"].append({
                "step": "system",
                "error_type": "system_error",
                "error_message": str(e),
                "traceback": traceback.format_exc(),
                "timestamp": time_module.time()
            })
            
            # ‚úÖ SAVE ERROR LOG
            try:
                save_real_estate_analysis_log(analysis_log)
            except Exception:
                pass
            
            error_msg = f"‚ö†Ô∏è L·ªói h·ªá th·ªëng: {str(e)}"
            yield f'data: {json.dumps({"chunk": error_msg})}\n\n'
            yield f'data: {json.dumps({"done": True})}\n\n'

    return StreamingResponse(generate(), media_type="text/event-stream")

# Th√™m model m·ªõi cho OCR request kh√¥ng c·∫ßn question
class OCRRequest(BaseModel):
    files: Optional[List[Dict[str, Any]]] = None


# Thay ƒë·ªïi logger cho d·ªÖ ph√¢n bi·ªát
logger_prefix = "[AI Service]"


def set_logger_prefix(prefix: str):
    global logger_prefix
    logger_prefix = f"[{prefix}]"


import fitz  # PyMuPDF
from docx import Document


# Th√™m helper functions cho OCR
def extract_text_from_pdf(file_base64: str) -> str:
    """Extract text from PDF using PyMuPDF"""
    try:
        import base64

        pdf_data = base64.b64decode(file_base64)
        doc = fitz.open(stream=pdf_data, filetype="pdf")

        text = ""
        for page_num in range(doc.page_count):
            page = doc[page_num]
            text += page.get_text() + "\n"

        doc.close()
        return text.strip()
    except Exception as e:
        logger.error(f"PDF extraction error: {e}")
        return ""


def extract_text_from_docx(file_base64: str) -> str:
    """Extract text from DOCX"""
    try:
        import base64
        import io

        docx_data = base64.b64decode(file_base64)
        doc = Document(io.BytesIO(docx_data))

        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"

        return text.strip()
    except Exception as e:
        logger.error(f"DOCX extraction error: {e}")
        return ""


async def extract_text_with_chatgpt_vision_url(image_url: str, filename: str) -> str:
    """
    Extract text from image using ChatGPT Vision API with direct URL
    """
    try:
        logger.info(f"  === CHATGPT VISION OCR START ===")
        logger.info(f"  File: {filename}")
        logger.info(f"  Image URL: {image_url}")
        
        # ‚úÖ PROPER MULTIMODAL MESSAGE FORMAT WITH URL
        messages = [
            {
                "role": "system",
                "content": "You are a professional OCR specialist for Vietnamese documents. Extract ALL text from the image, preserve formatting."
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"Please extract all text from this document image: {filename}. Provide ONLY the text without commentary."
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": image_url,
                            "detail": "high"
                        }
                    }
                ]
            }
        ]

        # ‚úÖ CALL CHATGPT VISION API
        logger.info(f"  Calling ChatGPT Vision API...")
        loop = asyncio.get_event_loop()

        def sync_chatgpt_call():
            return ai_provider_manager.chatgpt_client.client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=4000,
                temperature=0.1,
            )

        response = await loop.run_in_executor(None, sync_chatgpt_call)
        extracted_text = response.choices[0].message.content.strip()

        logger.info(f"  ‚úÖ ChatGPT Vision OCR completed")
        logger.info(f"  Extracted: {len(extracted_text)} characters")
        logger.info(f"  Raw OCR response: {extracted_text!r}")

        # ‚úÖ CHECK FOR SAFETY REFUSAL
        refusal_patterns = [
            "i'm sorry", "i'm sorry", "can't assist", "cannot help", 
            "unable to", "i cannot", "not able to"
        ]
        text_lower = extracted_text.lower()
        
        if any(pattern in text_lower for pattern in refusal_patterns):
            logger.warning(f"  ‚ö†Ô∏è ChatGPT Vision safety refusal detected")
            return ""

        if extracted_text and len(extracted_text) > 20:
            logger.info(f"  ===== CHATGPT VISION EXTRACTED TEXT =====")
            chunk_size = 500
            for i in range(0, len(extracted_text), chunk_size):
                chunk = extracted_text[i:i+chunk_size]
                chunk_num = i//chunk_size + 1
                logger.info(f"  CHUNK {chunk_num}: {chunk}")
            logger.info(f"  ===== END EXTRACTED TEXT =====")

            return extracted_text.strip()
        else:
            logger.warning(f"  ‚ö†Ô∏è ChatGPT Vision returned short/empty result")
            return ""

    except Exception as e:
        logger.error(f"  ‚ùå ChatGPT Vision OCR failed: {e}")
        return ""

class CCCDImageRequest(BaseModel):
    images: List[Dict[str, Any]]  # [{"url": "...", "type": "front/back", "fileName": "..."}]
    requestId: Optional[str] = None
    userId: Optional[str] = None

class CCCDOCRResponse(BaseModel):
    success: bool
    requestId: Optional[str] = None
    extractedData: Optional[Dict[str, Any]] = None
    processingDetails: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

@app.post("/api/ocr/cccd", response_model=CCCDOCRResponse)
async def ocr_cccd_extraction(request: CCCDImageRequest):
    """
    ‚úÖ OCR CCCD - Tr√≠ch xu·∫•t th√¥ng tin t·ª´ ·∫£nh CƒÉn c∆∞·ªõc c√¥ng d√¢n
    H·ªó tr·ª£: JPG, JPEG, PNG, HEIC, HEIF, WEBP, TIFF, BMP
    """
    
    processing_start = time.time()
    request_id = request.requestId or f"cccd_{int(time.time() * 1000)}"
    
    try:
        logger.info(f"üÜî [CCCD OCR] {request_id}: Starting OCR processing")
        logger.info(f"üì∏ [CCCD OCR] {request_id}: {len(request.images)} images to process")
        
        # ‚úÖ STEP 1: Validate v√† prepare images
        processed_images = []
        
        for i, img_data in enumerate(request.images):
            try:
                img_url = img_data.get("url", "")
                img_type = img_data.get("type", "unknown")  # front/back
                file_name = img_data.get("fileName", f"cccd_image_{i+1}")
                
                logger.info(f"üì∑ [CCCD OCR] {request_id}: Processing {img_type} - {file_name}")
                
                # Validate image URL
                if not img_url:
                    logger.warning(f"‚ö†Ô∏è [CCCD OCR] {request_id}: Empty URL for image {i+1}")
                    continue
                
                # Validate image format
                file_ext = file_name.split('.')[-1].lower() if '.' in file_name else 'jpg'
                supported_formats = ['jpg', 'jpeg', 'png', 'heic', 'heif', 'webp', 'tiff', 'bmp']
                
                if file_ext not in supported_formats:
                    logger.warning(f"‚ö†Ô∏è [CCCD OCR] {request_id}: Unsupported format {file_ext}")
                    continue
                
                # Test image accessibility
                try:
                    response = requests.head(img_url, timeout=10)
                    if response.status_code != 200:
                        logger.warning(f"‚ö†Ô∏è [CCCD OCR] {request_id}: Image not accessible - {response.status_code}")
                        continue
                except Exception as url_error:
                    logger.warning(f"‚ö†Ô∏è [CCCD OCR] {request_id}: URL test failed - {url_error}")
                    continue
                
                processed_images.append({
                    "url": img_url,
                    "type": img_type,
                    "fileName": file_name,
                    "fileType": f"image/{file_ext}" if file_ext != 'jpg' else "image/jpeg",
                    "index": i
                })
                
            except Exception as img_error:
                logger.error(f"‚ùå [CCCD OCR] {request_id}: Image {i+1} processing error - {img_error}")
                continue
        
        if not processed_images:
            return CCCDOCRResponse(
                success=False,
                requestId=request_id,
                error="No valid images found to process",
                processingDetails={
                    "totalImages": len(request.images),
                    "validImages": 0,
                    "processingTime": time.time() - processing_start
                }
            )
        
        logger.info(f"‚úÖ [CCCD OCR] {request_id}: {len(processed_images)} valid images prepared")
        
        # ‚úÖ STEP 2: Prepare ChatGPT Vision messages
        try:
            logger.info(f"ü§ñ [CCCD OCR] {request_id}: Preparing ChatGPT Vision request")
            
            # Create multimodal messages
            messages = [
                {
                    "role": "system",
                    "content": """B·∫°n l√† chuy√™n gia OCR cho CƒÉn c∆∞·ªõc c√¥ng d√¢n Vi·ªát Nam. 
Nhi·ªám v·ª•: Tr√≠ch xu·∫•t CH√çNH X√ÅC t·∫•t c·∫£ th√¥ng tin t·ª´ ·∫£nh CCCD (c·∫£ m·∫∑t tr∆∞·ªõc v√† m·∫∑t sau).

Y√äU C·∫¶U:
1. ƒê·ªçc c·∫©n th·∫≠n t·∫•t c·∫£ text trong ·∫£nh
2. Tr√≠ch xu·∫•t th√¥ng tin theo ƒë√∫ng format JSON
3. N·∫øu kh√¥ng r√µ th√¥ng tin n√†o, ƒë·ªÉ null
4. Ch√∫ √Ω ng√†y th√°ng theo format DD/MM/YYYY
5. T√™n ƒë·ªãa danh ch√≠nh x√°c (t·ªânh, th√†nh ph·ªë, qu·∫≠n, huy·ªán)

ƒê·ªäNH D·∫†NG JSON TR·∫¶N V·ªÄ:
{
  "soCCCD": "s·ªë cƒÉn c∆∞·ªõc 12 s·ªë",
  "hoTen": "h·ªç v√† t√™n ƒë·∫ßy ƒë·ªß",
  "ngaySinh": "DD/MM/YYYY",
  "gioiTinh": "Nam/N·ªØ",
  "queQuan": "qu√™ qu√°n ƒë·∫ßy ƒë·ªß",
  "diaChiThuongTru": "ƒë·ªãa ch·ªâ th∆∞·ªùng tr√∫ ƒë·∫ßy ƒë·ªß", 
  "ngayCap": "DD/MM/YYYY",
  "noiCap": "n∆°i c·∫•p",
  "ngayHetHan": "DD/MM/YYYY ho·∫∑c 'Kh√¥ng th·ªùi h·∫°n'",
  "danToc": "d√¢n t·ªôc (n·∫øu c√≥)",
  "tonGiao": "t√¥n gi√°o (n·∫øu c√≥)",
  "dacDiemNhanDang": "ƒë·∫∑c ƒëi·ªÉm nh·∫≠n d·∫°ng (n·∫øu c√≥)",
  "confidence": 0.95
}"""
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": f"H√£y tr√≠ch xu·∫•t th√¥ng tin t·ª´ {len(processed_images)} ·∫£nh CCCD n√†y. Tr·∫£ v·ªÅ CH√çNH X√ÅC ƒë·ªãnh d·∫°ng JSON nh∆∞ y√™u c·∫ßu:"
                        }
                    ]
                }
            ]
            
            # Add images to message
            for img in processed_images:
                messages[1]["content"].append({
                    "type": "image_url",
                    "image_url": {
                        "url": img["url"],
                        "detail": "high"  # High detail for OCR accuracy
                    }
                })
            
            logger.info(f"üì§ [CCCD OCR] {request_id}: ChatGPT payload prepared - {len(messages)} messages")
            
        except Exception as message_error:
            logger.error(f"‚ùå [CCCD OCR] {request_id}: Message preparation error - {message_error}")
            return CCCDOCRResponse(
                success=False,
                requestId=request_id,
                error=f"Message preparation failed: {str(message_error)}",
                processingDetails={
                    "totalImages": len(request.images),
                    "validImages": len(processed_images),
                    "processingTime": time.time() - processing_start
                }
            )
        
        # ‚úÖ STEP 3: Call ChatGPT Vision API
        try:
            logger.info(f"ü§ñ [CCCD OCR] {request_id}: Calling ChatGPT Vision API")
            
            # Check if ChatGPT is available
            if not CHATGPT_API_KEY:
                raise Exception("ChatGPT API key not configured")
            
            # Initialize ChatGPT client
            from src.clients.chatgpt_client import ChatGPTClient
            chatgpt_client = ChatGPTClient()
            
            # Call ChatGPT Vision with non-streaming
            ocr_start_time = time.time()
            
            # Use vision reasoning model for better OCR accuracy
            chatgpt_response = chatgpt_client.client.chat.completions.create(
                model=CHATGPT_VISION_REASONING_MODEL,  # gpt-4o for best OCR
                messages=messages,
                max_tokens=2000,
                temperature=0.1,  # Low temperature for accuracy
                response_format={"type": "json_object"}  # Force JSON response
            )
            
            ocr_duration = time.time() - ocr_start_time
            raw_response = chatgpt_response.choices[0].message.content
            
            logger.info(f"‚úÖ [CCCD OCR] {request_id}: ChatGPT response received in {ocr_duration:.2f}s")
            logger.info(f"üìÑ [CCCD OCR] {request_id}: Response length: {len(raw_response)} chars")
            
        except Exception as api_error:
            logger.error(f"‚ùå [CCCD OCR] {request_id}: ChatGPT API error - {api_error}")
            return CCCDOCRResponse(
                success=False,
                requestId=request_id,
                error=f"ChatGPT API failed: {str(api_error)}",
                processingDetails={
                    "totalImages": len(request.images),
                    "validImages": len(processed_images),
                    "processingTime": time.time() - processing_start
                }
            )
        
        # ‚úÖ STEP 4: Parse v√† validate JSON response
        try:
            logger.info(f"üîç [CCCD OCR] {request_id}: Parsing ChatGPT response")
            
            # Parse JSON
            import json
            try:
                ocr_data = json.loads(raw_response)
            except json.JSONDecodeError:
                # Try to extract JSON from response
                json_match = re.search(r'\{.*\}', raw_response, re.DOTALL)
                if json_match:
                    ocr_data = json.loads(json_match.group())
                else:
                    raise Exception("No valid JSON found in response")
            
            # Validate essential fields
            essential_fields = ['soCCCD', 'hoTen', 'ngaySinh']
            missing_fields = [field for field in essential_fields if not ocr_data.get(field)]
            
            if missing_fields:
                logger.warning(f"‚ö†Ô∏è [CCCD OCR] {request_id}: Missing essential fields: {missing_fields}")
            
            # Clean and format data
            extracted_data = {
                "idNumber": ocr_data.get("soCCCD"),
                "fullName": ocr_data.get("hoTen"),
                "dateOfBirth": ocr_data.get("ngaySinh"),
                "gender": ocr_data.get("gioiTinh"),
                "nationality": "Vi·ªát Nam",
                "placeOfOrigin": ocr_data.get("queQuan"),
                "permanentAddress": ocr_data.get("diaChiThuongTru"),
                "dateOfIssue": ocr_data.get("ngayCap"),
                "placeOfIssue": ocr_data.get("noiCap"),
                "expirationDate": ocr_data.get("ngayHetHan"),
                "ethnicity": ocr_data.get("danToc"),
                "religion": ocr_data.get("tonGiao"),
                "identificationMarks": ocr_data.get("dacDiemNhanDang"),
                "ocrProcessedAt": datetime.now().isoformat(),
                "ocrConfidence": ocr_data.get("confidence", 0.85),
                "ocrRawData": ocr_data
            }
            
            # Remove null values
            extracted_data = {k: v for k, v in extracted_data.items() if v is not None}
            
            logger.info(f"‚úÖ [CCCD OCR] {request_id}: OCR extraction successful")
            logger.info(f"üë§ [CCCD OCR] {request_id}: Extracted name: {extracted_data.get('fullName', 'N/A')}")
            logger.info(f"üÜî [CCCD OCR] {request_id}: Extracted ID: {extracted_data.get('idNumber', 'N/A')}")
            
        except Exception as parse_error:
            logger.error(f"‚ùå [CCCD OCR] {request_id}: Response parsing error - {parse_error}")
            logger.error(f"üìÑ [CCCD OCR] {request_id}: Raw response: {raw_response[:500]}...")
            
            return CCCDOCRResponse(
                success=False,
                requestId=request_id,
                error=f"Response parsing failed: {str(parse_error)}",
                processingDetails={
                    "totalImages": len(request.images) if hasattr(request, 'images') else 0,
                    "validImages": len(processed_images) if hasattr(request, 'images') else 0,
                    "processingTime": time.time() - processing_start,
                    "rawResponse": raw_response[:200] + "..." if len(raw_response) > 200 else raw_response
                }
            )
        
        # ‚úÖ STEP 5: Return successful response
        total_time = time.time() - processing_start
        
        processing_details = {
            "totalImages": len(request.images),
            "validImages": len(processed_images),
            "processingTime": round(total_time, 2),
            "ocrDuration": round(ocr_duration, 2),
            "modelUsed": CHATGPT_VISION_REASONING_MODEL,
            "imageTypes": [img["type"] for img in processed_images],
            "extractedFields": len([k for k, v in extracted_data.items() if v is not None])
        }
        
        logger.info(f"üéâ [CCCD OCR] {request_id}: Processing completed successfully in {total_time:.2f}s")
        
        return CCCDOCRResponse(
            success=True,
            requestId=request_id,
            extractedData=extracted_data,
            processingDetails=processing_details
        )
        
    except Exception as general_error:
        logger.error(f"‚ùå [CCCD OCR] {request_id}: General error - {general_error}")
        import traceback
        logger.error(f"üîç [CCCD OCR] {request_id}: Traceback - {traceback.format_exc()}")
        
        return CCCDOCRResponse(
            success=False,
            requestId=request_id,
            error=f"Processing failed: {str(general_error)}",
            processingDetails={
                "totalImages": len(request.images) if hasattr(request, 'images') else 0,
                "processingTime": time.time() - processing_start
            }
        )


@app.post("/api/loan/assessment", response_model=LoanAssessmentResponse)
async def loan_credit_assessment(request: LoanApplicationRequest):
    """
    ‚úÖ LOAN CREDIT ASSESSMENT - Th·∫©m ƒë·ªãnh h·ªì s∆° vay v·ªõi DeepSeek Reasoning
    """

    processing_start = time.time()
    assessment_id = f"assessment_{int(time.time() * 1000)}"

    try:
        logger.info(f"üè¶ [LOAN ASSESSMENT] {assessment_id}: Starting credit assessment")
        logger.info(f"üìã [LOAN ASSESSMENT] {assessment_id}: Application ID: {request.applicationId}")
        logger.info(f"üí∞ [LOAN ASSESSMENT] {assessment_id}: Loan amount: {request.loanAmount:,} VNƒê")

        # ‚úÖ SAFE APPLICANT NAME ACCESS
        applicant_name = "N/A"
        if hasattr(request, 'fullName') and request.fullName:
            applicant_name = request.fullName
        elif request.idCardInfo and hasattr(request.idCardInfo, 'fullName') and request.idCardInfo.fullName:
            applicant_name = request.idCardInfo.fullName

        logger.info(f"üë§ [LOAN ASSESSMENT] {assessment_id}: Applicant: {applicant_name}")

        # ‚úÖ STEP 1: Basic validation using utility function
        is_valid, validation_errors = validate_loan_application_minimal(request.dict())

        if not is_valid:
            return LoanAssessmentResponse(
                success=False,
                applicationId=request.applicationId or "unknown",
                error=f"Validation failed: {', '.join(validation_errors)}"
            )

        # ‚úÖ STEP 2: Check DeepSeek availability
        if not DEEPSEEK_API_KEY:
            return LoanAssessmentResponse(
                success=False,
                applicationId=request.applicationId,
                error="DeepSeek API not configured",
                processingDetails={
                    "processingTime": time.time() - processing_start
                }
            )

        # ‚úÖ STEP 3: Calculate comprehensive financial metrics for loan assessment
        try:
            logger.info(f"üìä [LOAN ASSESSMENT] {assessment_id}: Starting financial metrics calculation")

            # =================================================================
            # üì• INPUT DATA COLLECTION - Thu th·∫≠p d·ªØ li·ªáu ƒë·∫ßu v√†o an to√†n
            # =================================================================

            # Thu nh·∫≠p h√†ng th√°ng (VNƒê)
            monthly_income = getattr(request, 'monthlyIncome', 0) or 0
            logger.info(f"   üí∞ Monthly income: {monthly_income:,} VNƒê")

            # Thu nh·∫≠p kh√°c (cho thu√™, kinh doanh ph·ª•, etc.)
            other_income_amount = getattr(request, 'otherIncomeAmount', 0) or 0
            logger.info(f"   üíº Other income: {other_income_amount:,} VNƒê")

            # Kho·∫£n n·ª£ hi·ªán t·∫°i ph·∫£i tr·∫£ h√†ng th√°ng
            monthly_debt_payment = getattr(request, 'monthlyDebtPayment', 0) or 0
            logger.info(f"   üí≥ Current monthly debt: {monthly_debt_payment:,} VNƒê")

            # Gi√° tr·ªã t√†i s·∫£n ƒë·∫£m b·∫£o (th∆∞·ªùng l√† BƒêS)
            collateral_value = getattr(request, 'collateralValue', 0) or 0
            logger.info(f"   üè† Collateral value: {collateral_value:,} VNƒê")

            # S·ªë ti·ªÅn mu·ªën vay
            loan_amount = getattr(request, 'loanAmount', 0) or 0
            logger.info(f"   üíµ Loan amount: {loan_amount:,} VNƒê")

            # L√£i su·∫•t t·ª´ backend (% nƒÉm)
            backend_interest_rate = getattr(request, 'interestRate', 8.5) or 8.5
            logger.info(f"   üìà Interest rate: {backend_interest_rate}% per year")

            # Th·ªùi h·∫°n vay
            loan_term = getattr(request, 'loanTerm', '15 nƒÉm') or '15 nƒÉm'
            logger.info(f"   ‚è∞ Loan term: {loan_term}")

            # =================================================================
            # üßÆ PRIMARY CALCULATIONS - T√≠nh to√°n c√°c ch·ªâ s·ªë ch√≠nh
            # =================================================================

            # 1Ô∏è‚É£ T·ªîNG THU NH·∫¨P H√ÄNG TH√ÅNG
            # M·ª•c ƒë√≠ch: X√°c ƒë·ªãnh kh·∫£ nƒÉng t√†i ch√≠nh t·ªïng th·ªÉ c·ªßa kh√°ch h√†ng
            total_monthly_income = monthly_income + other_income_amount
            logger.info(f"   ‚úÖ Total monthly income: {total_monthly_income:,} VNƒê")

            # 2Ô∏è‚É£ DTI HI·ªÜN T·∫†I (Current Debt-to-Income Ratio)
            # M·ª•c ƒë√≠ch: ƒê√°nh gi√° t√¨nh tr·∫°ng n·ª£ hi·ªán t·∫°i so v·ªõi thu nh·∫≠p
            # Ti√™u chu·∫©n ng√¢n h√†ng: ‚â§ 40% (t·ªët), 40-50% (c·∫£nh b√°o), >50% (t·ª´ ch·ªëi)
            current_debt_ratio = monthly_debt_payment / total_monthly_income if total_monthly_income > 0 else 0
            logger.info(f"   üìä Current DTI ratio: {current_debt_ratio:.2%} (Standard: ‚â§40% good, >50% risky)")

            # =================================================================
            # üí≥ LOAN PAYMENT CALCULATION - T√≠nh to√°n kho·∫£n tr·∫£ g√≥p
            # =================================================================

            # Chuy·ªÉn ƒë·ªïi l√£i su·∫•t nƒÉm sang th·∫≠p ph√¢n
            estimated_rate = backend_interest_rate / 100
            logger.info(f"   üî¢ Annual rate (decimal): {estimated_rate}")

            # Tr√≠ch xu·∫•t s·ªë nƒÉm t·ª´ th·ªùi h·∫°n vay
            years = 15  # Default
            try:
                if "nƒÉm" in loan_term:
                    years = int(loan_term.split()[0])
                    logger.info(f"   ‚è≥ Extracted loan years: {years}")
            except Exception as term_error:
                logger.warning(f"   ‚ö†Ô∏è Cannot parse loan term '{loan_term}', using default 15 years")
                years = 15

            # T√≠nh to√°n c√°c th√¥ng s·ªë cho c√¥ng th·ª©c tr·∫£ g√≥p
            monthly_rate = estimated_rate / 12  # L√£i su·∫•t th√°ng
            n_payments = years * 12  # T·ªïng s·ªë k·ª≥ tr·∫£

            logger.info(f"   üìÖ Monthly rate: {monthly_rate:.6f} ({monthly_rate*100:.4f}%)")
            logger.info(f"   üî¢ Total payments: {n_payments} months")

            # 3Ô∏è‚É£ MONTHLY PAYMENT CALCULATION (C√¥ng th·ª©c tr·∫£ g√≥p ƒë·ªÅu)
            # C√¥ng th·ª©c: PMT = P * [r(1+r)^n] / [(1+r)^n - 1]
            # M·ª•c ƒë√≠ch: T√≠nh kho·∫£n tr·∫£ h√†ng th√°ng cho kho·∫£n vay m·ªõi
            if loan_amount > 0 and monthly_rate > 0:
                # √Åp d·ª•ng c√¥ng th·ª©c tr·∫£ g√≥p ƒë·ªÅu (Equal Monthly Installment)
                compound_factor = (1 + monthly_rate) ** n_payments
                estimated_monthly_payment = loan_amount * (monthly_rate * compound_factor) / (compound_factor - 1)

                logger.info(f"   üí∞ Estimated monthly payment: {estimated_monthly_payment:,} VNƒê")
                logger.info(f"   üìä Payment calculation: Loan={loan_amount:,}, Rate={monthly_rate:.6f}, Periods={n_payments}")
            else:
                estimated_monthly_payment = 0
                logger.warning(f"   ‚ö†Ô∏è Cannot calculate payment: loan_amount={loan_amount}, monthly_rate={monthly_rate}")

            # =================================================================
            # üìà RISK ASSESSMENT RATIOS - T√≠nh to√°n c√°c t·ª∑ l·ªá ƒë√°nh gi√° r·ªßi ro
            # =================================================================

            # 4Ô∏è‚É£ NEW DTI RATIO (Projected Debt-to-Income after new loan)
            # M·ª•c ƒë√≠ch: ƒê√°nh gi√° kh·∫£ nƒÉng tr·∫£ n·ª£ sau khi c√≥ kho·∫£n vay m·ªõi
            # Ti√™u chu·∫©n ng√¢n h√†ng VN: ‚â§50% (SBVN), ‚â§40% (conservative)
            new_debt_ratio = (monthly_debt_payment + estimated_monthly_payment) / total_monthly_income if total_monthly_income > 0 else 0
            logger.info(f"   üö® New DTI ratio: {new_debt_ratio:.2%} (Regulatory limit: ‚â§50%, Bank limit: ‚â§40%)")

            # 5Ô∏è‚É£ LTV RATIO (Loan-to-Value Ratio)
            # M·ª•c ƒë√≠ch: ƒê√°nh gi√° r·ªßi ro t√†i s·∫£n ƒë·∫£m b·∫£o
            # Ti√™u chu·∫©n ng√¢n h√†ng: ‚â§70% (t·ªët), 70-80% (ch·∫•p nh·∫≠n), >80% (t·ª´ ch·ªëi)
            loan_to_value = loan_amount / collateral_value if collateral_value > 0 else 0
            logger.info(f"   üè† LTV ratio: {loan_to_value:.2%} (Standard: ‚â§70% good, >80% risky)")

            # =================================================================
            # üí° FINANCIAL CAPACITY ANALYSIS - Ph√¢n t√≠ch kh·∫£ nƒÉng t√†i ch√≠nh
            # =================================================================

            # 6Ô∏è‚É£ REMAINING INCOME (Thu nh·∫≠p c√≤n l·∫°i sau tr·∫£ n·ª£)
            # M·ª•c ƒë√≠ch: ƒê√°nh gi√° kh·∫£ nƒÉng chi ti√™u sinh ho·∫°t sau khi tr·∫£ n·ª£
            remaining_income = total_monthly_income - monthly_debt_payment - estimated_monthly_payment
            logger.info(f"   üíµ Remaining income: {remaining_income:,} VNƒê (for living expenses)")

            # 7Ô∏è‚É£ DEBT SERVICE COVERAGE (Kh·∫£ nƒÉng thanh to√°n n·ª£)
            # M·ª•c ƒë√≠ch: ƒêo l∆∞·ªùng m·ª©c ƒë·ªô an to√†n trong vi·ªác tr·∫£ n·ª£
            total_debt_service = monthly_debt_payment + estimated_monthly_payment
            debt_coverage = total_monthly_income / total_debt_service if total_debt_service > 0 else float('inf')
            logger.info(f"   üõ°Ô∏è Debt service coverage: {debt_coverage:.2f}x (>1.25x recommended)")

            # =================================================================
            # üéØ RISK ASSESSMENT SUMMARY - T√≥m t·∫Øt ƒë√°nh gi√° r·ªßi ro
            # =================================================================

            # ƒê√°nh gi√° m·ª©c ƒë·ªô r·ªßi ro d·ª±a tr√™n c√°c ch·ªâ s·ªë
            risk_indicators = []

            if new_debt_ratio > 0.5:  # >50%
                risk_indicators.append(f"High DTI: {new_debt_ratio:.1%}")
            elif new_debt_ratio > 0.4:  # 40-50%
                risk_indicators.append(f"Moderate DTI: {new_debt_ratio:.1%}")

            if loan_to_value > 0.8:  # >80%
                risk_indicators.append(f"High LTV: {loan_to_value:.1%}")
            elif loan_to_value > 0.7:  # 70-80%
                risk_indicators.append(f"Moderate LTV: {loan_to_value:.1%}")

            if remaining_income < 15_000_000:  # <15M VNƒê
                risk_indicators.append(f"Low remaining income: {remaining_income/1_000_000:.1f}M")

            if debt_coverage < 1.25:
                risk_indicators.append(f"Low debt coverage: {debt_coverage:.2f}x")

            # =================================================================
            # üìä COMPREHENSIVE LOGGING - Ghi log chi ti·∫øt
            # =================================================================

            logger.info(f"üìä [LOAN ASSESSMENT] {assessment_id}: Financial metrics calculation completed")
            logger.info(f"   üíµ Total monthly income: {total_monthly_income:,} VNƒê")
            logger.info(f"   üìà Current DTI ratio: {current_debt_ratio:.2%}")
            logger.info(f"   üö® Projected DTI ratio: {new_debt_ratio:.2%}")
            logger.info(f"   üè† Loan-to-Value ratio: {loan_to_value:.2%}")
            logger.info(f"   üí∞ Estimated monthly payment: {estimated_monthly_payment:,} VNƒê")
            logger.info(f"   üíµ Remaining income: {remaining_income:,} VNƒê")
            logger.info(f"   üõ°Ô∏è Debt service coverage: {debt_coverage:.2f}x")

            if risk_indicators:
                logger.warning(f"   ‚ö†Ô∏è Risk indicators: {', '.join(risk_indicators)}")
            else:
                logger.info(f"   ‚úÖ All financial ratios within acceptable ranges")

            # =================================================================
            # üéØ ASSESSMENT RECOMMENDATION - ƒê·ªÅ xu·∫•t s∆° b·ªô
            # =================================================================

            # ƒê∆∞a ra ƒë·ªÅ xu·∫•t s∆° b·ªô d·ª±a tr√™n c√°c ch·ªâ s·ªë t√†i ch√≠nh
            if new_debt_ratio <= 0.4 and loan_to_value <= 0.7 and remaining_income >= 15_000_000:
                preliminary_recommendation = "STRONG_APPROVAL"
                logger.info(f"   üü¢ Preliminary assessment: STRONG APPROVAL CANDIDATE")
            elif new_debt_ratio <= 0.5 and loan_to_value <= 0.8 and remaining_income >= 10_000_000:
                preliminary_recommendation = "CONDITIONAL_APPROVAL"
                logger.info(f"   üü° Preliminary assessment: CONDITIONAL APPROVAL CANDIDATE")
            else:
                preliminary_recommendation = "NEEDS_REVIEW"
                logger.info(f"   üî¥ Preliminary assessment: NEEDS DETAILED REVIEW")

            # Store all calculated metrics for later use
            financial_metrics = {
                'monthly_income': monthly_income,
                'other_income_amount': other_income_amount,
                'total_monthly_income': total_monthly_income,
                'monthly_debt_payment': monthly_debt_payment,
                'estimated_monthly_payment': estimated_monthly_payment,
                'current_debt_ratio': current_debt_ratio,
                'new_debt_ratio': new_debt_ratio,
                'loan_to_value': loan_to_value,
                'remaining_income': remaining_income,
                'debt_coverage': debt_coverage,
                'risk_indicators': risk_indicators,
                'preliminary_recommendation': preliminary_recommendation,
                'backend_interest_rate': backend_interest_rate,
                'loan_term_years': years
            }

            logger.info(f"‚úÖ [LOAN ASSESSMENT] {assessment_id}: All financial metrics stored successfully")

        except Exception as calc_error:
            logger.error(f"‚ùå [LOAN ASSESSMENT] {assessment_id}: Financial calculation error - {calc_error}")
            import traceback
            logger.error(f"üîç [LOAN ASSESSMENT] {assessment_id}: Calculation traceback - {traceback.format_exc()}")

            # ‚úÖ SAFE FALLBACK VALUES - Gi√° tr·ªã d·ª± ph√≤ng an to√†n
            estimated_monthly_payment = 0
            new_debt_ratio = 0
            loan_to_value = 0
            backend_interest_rate = 8.5
            total_monthly_income = getattr(request, 'monthlyIncome', 0) or 0
            current_debt_ratio = 0
            remaining_income = 0
            debt_coverage = 0

            # Store fallback metrics
            financial_metrics = {
                'monthly_income': getattr(request, 'monthlyIncome', 0) or 0,
                'other_income_amount': 0,
                'total_monthly_income': total_monthly_income,
                'monthly_debt_payment': 0,
                'estimated_monthly_payment': 0,
                'current_debt_ratio': 0,
                'new_debt_ratio': 0,
                'loan_to_value': 0,
                'remaining_income': 0,
                'debt_coverage': 0,
                'risk_indicators': ['Calculation Error'],
                'preliminary_recommendation': 'MANUAL_REVIEW_REQUIRED',
                'backend_interest_rate': 8.5,
                'loan_term_years': 15,
                'calculation_error': str(calc_error)
            }

            logger.warning(f"üîÑ [LOAN ASSESSMENT] {assessment_id}: Using fallback financial metrics due to calculation error")
        # ‚úÖ STEP 4: Build comprehensive assessment prompt
        try:
            # ‚úÖ CALCULATE AGE FROM BIRTH YEAR
            current_year = datetime.now().year
            birth_year = getattr(request, 'birthYear', None)
            if birth_year:
                age = current_year - birth_year
            elif request.idCardInfo and request.idCardInfo.dateOfBirth:
                age = current_year - request.idCardInfo.dateOfBirth.year
            else:
                age = "Ch∆∞a x√°c ƒë·ªãnh"

            # ‚úÖ GET PERSONAL INFO (Priority: personalInfo over idCardInfo) - ALL SAFE
            full_name = getattr(request, 'fullName', None) or (request.idCardInfo.fullName if request.idCardInfo else "Ch∆∞a cung c·∫•p")
            gender = getattr(request, 'gender', None) or (request.idCardInfo.gender if request.idCardInfo else "Ch∆∞a cung c·∫•p")
            marital_status = getattr(request, 'maritalStatus', None) or "Ch∆∞a cung c·∫•p"
            dependents = getattr(request, 'dependents', 0) or 0
            email = getattr(request, 'email', None) or "Ch∆∞a cung c·∫•p"

            # Phone with country code
            phone_country_code = getattr(request, 'phoneCountryCode', '+84')
            phone_number = getattr(request, 'phoneNumber', None)
            phone_display = f"{phone_country_code} {phone_number}" if phone_number else "Ch∆∞a cung c·∫•p"

            # ID Card info (optional)
            id_number = "Ch∆∞a cung c·∫•p"
            permanent_address = "Ch∆∞a cung c·∫•p"
            if request.idCardInfo:
                id_number = getattr(request.idCardInfo, 'idNumber', None) or "Ch∆∞a cung c·∫•p"
                permanent_address = getattr(request.idCardInfo, 'permanentAddress', None) or "Ch∆∞a cung c·∫•p"

            # ‚úÖ GET FINANCIAL INFO - ALL SAFE
            monthly_income = getattr(request, 'monthlyIncome', 0) or 0
            primary_income_source = getattr(request, 'primaryIncomeSource', None) or "Ch∆∞a cung c·∫•p"
            company_name = getattr(request, 'companyName', None) or "Ch∆∞a cung c·∫•p"
            job_title = getattr(request, 'jobTitle', None) or "Ch∆∞a cung c·∫•p"
            work_experience = getattr(request, 'workExperience', 0) or 0
            other_income = getattr(request, 'otherIncome', None) or "Kh√¥ng c√≥"
            other_income_amount = getattr(request, 'otherIncomeAmount', 0) or 0

            # Banking info - SAFE
            bank_name = getattr(request, 'bankName', None) or "Ch∆∞a cung c·∫•p"
            bank_account = getattr(request, 'bankAccount', None) or "Ch∆∞a cung c·∫•p"

            # Assets - SAFE
            total_assets = getattr(request, 'totalAssets', 0) or 0
            liquid_assets = getattr(request, 'liquidAssets', 0) or 0

            # ‚úÖ GET DEBT INFO - ALL SAFE
            has_existing_debt = getattr(request, 'hasExistingDebt', False) or False
            total_debt_amount = getattr(request, 'totalDebtAmount', 0) or 0
            monthly_debt_payment = getattr(request, 'monthlyDebtPayment', 0) or 0
            cic_credit_score_group = getattr(request, 'cicCreditScoreGroup', None) or "Ch∆∞a x√°c ƒë·ªãnh"
            credit_history = getattr(request, 'creditHistory', None) or "Ch∆∞a c√≥ th√¥ng tin"

            # ‚úÖ GET COLLATERAL INFO - ALL SAFE
            collateral_type = getattr(request, 'collateralType', 'B·∫•t ƒë·ªông s·∫£n')
            collateral_info = getattr(request, 'collateralInfo', 'Ch∆∞a c√≥ th√¥ng tin chi ti·∫øt')
            collateral_value = getattr(request, 'collateralValue', 0) or 0
            has_collateral_image = getattr(request, 'hasCollateralImage', False)

            # ‚úÖ GET BACKEND INTEREST RATE - SAFE
            backend_interest_rate = getattr(request, 'interestRate', 8.5)

            # ‚úÖ SAFE EXISTING LOANS FORMAT
            existing_loans = getattr(request, 'existingLoans', []) or []
            existing_loans_text = ""
            if existing_loans and len(existing_loans) > 0:
                for i, loan in enumerate(existing_loans, 1):
                    lender = getattr(loan, 'lender', None) or "Kh√¥ng r√µ"
                    amount = getattr(loan, 'amount', 0) or 0
                    payment = getattr(loan, 'monthlyPayment', 0) or 0
                    term = getattr(loan, 'remainingTerm', None) or "Kh√¥ng r√µ"
                    existing_loans_text += f"{i}. {lender}: {amount/1_000_000:.0f} tri·ªáu VNƒê (tr·∫£ {payment/1_000_000:.0f} tri·ªáu/th√°ng, c√≤n {term})\n"
            else:
                existing_loans_text = "Kh√¥ng c√≥ kho·∫£n n·ª£ n√†o"

            # ‚úÖ BUILD SAFE PROMPT WITH ALL FALLBACKS
            assessment_prompt = f"""B·∫°n l√† CHUY√äN GIA TH·∫®M ƒê·ªäNH T√çN D·ª§NG cao c·∫•p v·ªõi 15 nƒÉm kinh nghi·ªám t·∫°i c√°c ng√¢n h√†ng l·ªõn ·ªü Vi·ªát Nam (VietinBank, BIDV, Vietcombank). H√£y th·∫©m ƒë·ªãnh h·ªì s∆° vay v·ªën n√†y m·ªôt c√°ch chi ti·∫øt v√† chuy√™n nghi·ªáp theo ti√™u chu·∫©n ng√¢n h√†ng Vi·ªát Nam.

üéØ **NHI·ªÜM V·ª§ TH·∫®M ƒê·ªäNH:**

1. **PH√ÇN T√çCH KH·∫¢ NƒÇNG T√ÄI CH√çNH:**
   - ƒê√°nh gi√° t·ª∑ l·ªá thu nh·∫≠p/chi ph√≠ (DTI - Debt to Income)
   - Ph√¢n t√≠ch d√≤ng ti·ªÅn h√†ng th√°ng v√† kh·∫£ nƒÉng tr·∫£ n·ª£
   - ƒê√°nh gi√° ƒë·ªô ·ªïn ƒë·ªãnh thu nh·∫≠p v√† ngu·ªìn thu

2. **TH·∫®M ƒê·ªäNH T√ÄI S·∫¢N ƒê·∫¢M B·∫¢O:**
   - ƒê·ªãnh gi√° l·∫°i b·∫•t ƒë·ªông s·∫£n d·ª±a tr√™n m√¥ t·∫£ chi ti·∫øt t·ª´ kh√°ch h√†ng
   - ƒê√°nh gi√° t√≠nh thanh kho·∫£n v√† r·ªßi ro gi√° tr·ªã
   - Ph√¢n t√≠ch v·ªã tr√≠ ƒë·ªãa l√Ω v√† ti·ªán √≠ch d·ª±a tr√™n th√¥ng tin c√≥ s·∫µn

3. **ƒê√ÅNH GI√Å R·ª¶I RO T√çN D·ª§NG:**
   - Ph√¢n t√≠ch l·ªãch s·ª≠ t√≠n d·ª•ng v√† nh√≥m CIC
   - ƒê√°nh gi√° kh·∫£ nƒÉng tr·∫£ n·ª£ d·ª±a tr√™n thu nh·∫≠p
   - X√°c ƒë·ªãnh c√°c y·∫øu t·ªë r·ªßi ro ti·ªÅm ·∫©n

4. **KI·∫æN NGH·ªä QUY·∫æT ƒê·ªäNH:**
   - Ph√™ duy·ªát/t·ª´ ch·ªëi v·ªõi l√Ω do c·ª• th·ªÉ v√† chi ti·∫øt
   - ƒê·ªÅ xu·∫•t s·ªë ti·ªÅn cho vay ph√π h·ª£p (n·∫øu ph√™ duy·ªát)
   - ƒê·ªÅ xu·∫•t l√£i su·∫•t v√† ƒëi·ªÅu ki·ªán vay
   - C√°c y√™u c·∫ßu b·ªï sung (n·∫øu c√≥)

üìã **TH√îNG TIN H·ªí S∆† VAY:**

**A. TH√îNG TIN KHO·∫¢N VAY:**
- M√£ h·ªì s∆°: {getattr(request, 'applicationId', 'N/A')}
- S·ªë ti·ªÅn vay: {loan_amount/1_000_000_000:.1f} t·ª∑ VNƒê
- Lo·∫°i vay: {getattr(request, 'loanType', 'Ch∆∞a x√°c ƒë·ªãnh')}
- Th·ªùi h·∫°n: {loan_term}
- M·ª•c ƒë√≠ch: {getattr(request, 'loanPurpose', 'Ch∆∞a cung c·∫•p')}

**B. TH√îNG TIN C√Å NH√ÇN:**
- H·ªç t√™n: {full_name}
- Tu·ªïi: {age}
- Gi·ªõi t√≠nh: {gender}
- T√¨nh tr·∫°ng h√¥n nh√¢n: {marital_status}
- S·ªë ng∆∞·ªùi ph·ª• thu·ªôc: {dependents} ng∆∞·ªùi
- ƒêi·ªán tho·∫°i: {phone_display}
- Email: {email}

**Th√¥ng tin CCCD (n·∫øu c√≥):**
- S·ªë CCCD: {id_number}
- ƒê·ªãa ch·ªâ th∆∞·ªùng tr√∫: {permanent_address}

**C. TH√îNG TIN T√ÄI CH√çNH:**
- Thu nh·∫≠p ch√≠nh: {monthly_income/1_000_000:.0f} tri·ªáu VNƒê/th√°ng
- Ngu·ªìn thu nh·∫≠p: {primary_income_source}
- C√¥ng ty/Doanh nghi·ªáp: {company_name}
- Ch·ª©c v·ª•: {job_title}
- Kinh nghi·ªám l√†m vi·ªác: {work_experience} nƒÉm
- Thu nh·∫≠p kh√°c: {other_income_amount/1_000_000:.0f} tri·ªáu VNƒê/th√°ng ({other_income})

- T·ªïng t√†i s·∫£n: {total_assets/1_000_000_000:.1f} t·ª∑ VNƒê
- T√†i s·∫£n thanh kho·∫£n: {liquid_assets/1_000_000_000:.1f} t·ª∑ VNƒê
- Ng√¢n h√†ng ch√≠nh: {bank_name}
- S·ªë t√†i kho·∫£n: {bank_account}

**D. TH√îNG TIN N·ª¢ HI·ªÜN T·∫†I:**
- C√≥ n·ª£ hi·ªán t·∫°i: {'C√≥' if has_existing_debt else 'Kh√¥ng'}
- T·ªïng d∆∞ n·ª£: {total_debt_amount/1_000_000:.0f} tri·ªáu VNƒê
- Tr·∫£ n·ª£ h√†ng th√°ng: {monthly_debt_payment/1_000_000:.0f} tri·ªáu VNƒê
- T·ª∑ l·ªá n·ª£/thu nh·∫≠p hi·ªán t·∫°i: {current_debt_ratio:.1%}
- Nh√≥m t√≠n d·ª•ng CIC: Nh√≥m {cic_credit_score_group}
- L·ªãch s·ª≠ t√≠n d·ª•ng: {credit_history}

**Chi ti·∫øt c√°c kho·∫£n n·ª£ hi·ªán t·∫°i:**
{existing_loans_text}

**E. T√ÄI S·∫¢N ƒê·∫¢M B·∫¢O:**
- Lo·∫°i t√†i s·∫£n: {collateral_type}
- Gi√° tr·ªã kh√°ch h√†ng ∆∞·ªõc t√≠nh: {collateral_value/1_000_000_000:.1f} t·ª∑ VNƒê {"(c√≥ h√¨nh ·∫£nh ƒë√≠nh k√®m)" if has_collateral_image else "(ch∆∞a c√≥ h√¨nh ·∫£nh)"}
- T·ª∑ l·ªá cho vay/gi√° tr·ªã t√†i s·∫£n d·ª± ki·∫øn: {loan_to_value:.1%}

**M√¥ t·∫£ chi ti·∫øt t√†i s·∫£n t·ª´ kh√°ch h√†ng:**
{collateral_info}

üìä **CH·ªà S·ªê T√ÄI CH√çNH QUAN TR·ªåNG:**
- D·ª± ki·∫øn tr·∫£ n·ª£ m·ªõi h√†ng th√°ng: {estimated_monthly_payment/1_000_000:.1f} tri·ªáu VNƒê
- T·ª∑ l·ªá n·ª£/thu nh·∫≠p sau khi vay: {new_debt_ratio:.1%}
- Thu nh·∫≠p c√≤n l·∫°i sau tr·∫£ n·ª£: {(total_monthly_income - monthly_debt_payment - estimated_monthly_payment)/1_000_000:.1f} tri·ªáu VNƒê


‚ö†Ô∏è **Y√äU C·∫¶U QUAN TR·ªåNG:**
1. Ph√¢n t√≠ch t·ª´ng b∆∞·ªõc m·ªôt c√°ch logic v√† chi ti·∫øt
2. ƒê∆∞a ra quy·∫øt ƒë·ªãnh d·ª±a tr√™n ti√™u chu·∫©n ng√¢n h√†ng Vi·ªát Nam
3. Gi·∫£i th√≠ch r√µ r√†ng l√Ω do cho m·ªçi quy·∫øt ƒë·ªãnh
4. ƒê·ªÅ xu·∫•t c√°c ƒëi·ªÅu ki·ªán c·ª• th·ªÉ v√† kh·∫£ thi
5. **ƒê·ªãnh gi√° t√†i s·∫£n ƒë·∫£m b·∫£o d·ª±a CH√çNH X√ÅC tr√™n m√¥ t·∫£ t·ª´ kh√°ch h√†ng**

üéØ **TR√ÅCH NGHI·ªÜM TH·∫®M ƒê·ªäNH:**
Tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng JSON ch√≠nh x√°c (b·∫Øt bu·ªôc):

{{
  "status": "approved/rejected/needs_review",
  "confidence": 0.85,
  "creditScore": 750,
  "reasoning": "Ph√¢n t√≠ch chi ti·∫øt l√Ω do quy·∫øt ƒë·ªãnh v·ªõi √≠t nh·∫•t 200 t·ª´...",
  "riskFactors": ["R·ªßi ro c·ª• th·ªÉ 1", "R·ªßi ro c·ª• th·ªÉ 2", "R·ªßi ro c·ª• th·ªÉ 3"],
  "recommendations": ["Ki·∫øn ngh·ªã c·ª• th·ªÉ 1", "Ki·∫øn ngh·ªã c·ª• th·ªÉ 2"],
  "approvedAmount": 
  "interestRate": {backend_interest_rate},
  "loanTerm": 
  "monthlyPayment": {int(estimated_monthly_payment) if estimated_monthly_payment > 0 else None},
  "loanToValue": 
  "debtToIncome": 
  "conditions": 
  "collateralValuation": {{
    "estimatedValue": {collateral_value if collateral_value > 0 else "c·∫ßn ƒë·ªãnh gi√° d·ª±a tr√™n t√†i s·∫£n ƒë·∫£m b·∫£o"},
    "marketAnalysis": "Ph√¢n t√≠ch th·ªã tr∆∞·ªùng d·ª±a tr√™n m√¥ t·∫£ t√†i s·∫£n t·ª´ kh√°ch h√†ng",
    "liquidityRisk": "low/medium/high",
    "valuationMethod": "customer_description_analysis"
  }},
  "financialAnalysis": {{
    "totalMonthlyIncome": {total_monthly_income},
    "totalMonthlyDebt": {monthly_debt_payment},
    "newLoanPayment": {int(estimated_monthly_payment) if estimated_monthly_payment > 0 else 0},
    "remainingIncome": {max(0, total_monthly_income - monthly_debt_payment - estimated_monthly_payment)},
    "emergencyFund": {liquid_assets},
    "incomeStability": "high/medium/low",
    "assetLiquidity": "high/medium/low"
  }}
}}

H√ÉY TH·∫®M ƒê·ªäNH K·ª∏ L∆Ø·ª†NG V√Ä ƒê∆ØA RA QUY·∫æT ƒê·ªäNH CH√çNH X√ÅC THEO TI√äU CHU·∫®N NG√ÇN H√ÄNG VI·ªÜT NAM."""

            logger.info(f"üìù [LOAN ASSESSMENT] {assessment_id}: Safe assessment prompt prepared")
            logger.info(f"   üìÑ Prompt length: {len(assessment_prompt)} characters")

        except Exception as prompt_error:
            logger.error(f"‚ùå [LOAN ASSESSMENT] {assessment_id}: Prompt preparation error - {prompt_error}")
            return LoanAssessmentResponse(
                success=False,
                applicationId=getattr(request, 'applicationId', 'unknown'),
                error=f"Prompt preparation failed: {str(prompt_error)}",
                processingDetails={
                    "processingTime": time.time() - processing_start
                }
            )

        # ‚úÖ STEP 5: Call DeepSeek Reasoning API
        try:
            logger.info(f"ü§ñ [LOAN ASSESSMENT] {assessment_id}: Calling DeepSeek Reasoning API")

            # Prepare messages for DeepSeek
            messages = [
                {
                    "role": "system",
                    "content": "B·∫°n l√† chuy√™n gia th·∫©m ƒë·ªãnh t√≠n d·ª•ng chuy√™n nghi·ªáp. Ph·∫£n h·ªìi b·∫±ng JSON ch√≠nh x√°c v√† ph√¢n t√≠ch chi ti·∫øt."
                },
                {
                    "role": "user",
                    "content": assessment_prompt
                }
            ]

            # Call DeepSeek with reasoning
            reasoning_start = time.time()

            def sync_loan_assessment_non_stream():
                """Sync wrapper for loan assessment non-streaming"""
                try:
                    import asyncio
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)

                    # ‚úÖ USE DEDICATED LOAN ASSESSMENT NON-STREAMING METHOD
                    result = loop.run_until_complete(
                        ai_provider_manager.loan_assessment_completion_non_stream(messages, "deepseek")
                    )

                    loop.close()
                    return result

                except Exception as e:
                    logger.error(f"Sync loan assessment non-stream call error: {e}")
                    raise e

            # ‚úÖ GET COMPLETE RESPONSE NON-STREAMING (MORE RELIABLE)
            raw_response = sync_loan_assessment_non_stream()
            reasoning_duration = time.time() - reasoning_start

            logger.info(f"‚úÖ [LOAN ASSESSMENT] {assessment_id}: DeepSeek response received in {reasoning_duration:.2f}s")
            logger.info(f"üìÑ [LOAN ASSESSMENT] {assessment_id}: Response length: {len(raw_response)} chars")

        except Exception as api_error:
            logger.error(f"‚ùå [LOAN ASSESSMENT] {assessment_id}: DeepSeek API error - {api_error}")
            return LoanAssessmentResponse(
                success=False,
                applicationId=request.applicationId,
                error=f"DeepSeek API failed: {str(api_error)}",
                processingDetails={
                    "processingTime": time.time() - processing_start
                }
            )

        # ‚úÖ STEP 6: Parse assessment response
        try:
            logger.info(f"üîç [LOAN ASSESSMENT] {assessment_id}: Parsing DeepSeek response")

            # Extract JSON from response
            json_match = re.search(r'\{.*\}', raw_response, re.DOTALL)
            if json_match:
                assessment_data = json.loads(json_match.group())
            else:
                # Try to find JSON in the response
                lines = raw_response.split('\n')
                json_lines = []
                in_json = False
                for line in lines:
                    if '{' in line:
                        in_json = True
                    if in_json:
                        json_lines.append(line)
                    if '}' in line and in_json:
                        break

                if json_lines:
                    assessment_data = json.loads(''.join(json_lines))
                else:
                    raise Exception("No valid JSON found in response")

            # Validate required fields
            required_fields = ['status', 'confidence', 'reasoning']
            missing_fields = [field for field in required_fields if field not in assessment_data]

            if missing_fields:
                logger.warning(f"‚ö†Ô∏è [LOAN ASSESSMENT] {assessment_id}: Missing fields: {missing_fields}")

            logger.info(f"‚úÖ [LOAN ASSESSMENT] {assessment_id}: Assessment parsing successful")
            logger.info(f"üìä [LOAN ASSESSMENT] {assessment_id}: Status: {assessment_data.get('status', 'unknown')}")
            logger.info(f"üìä [LOAN ASSESSMENT] {assessment_id}: Confidence: {assessment_data.get('confidence', 0)}")

        except Exception as parse_error:
            logger.error(f"‚ùå [LOAN ASSESSMENT] {assessment_id}: Response parsing error - {parse_error}")
            logger.error(f"üìÑ [LOAN ASSESSMENT] {assessment_id}: Raw response preview: {raw_response[:500]}...")

            return LoanAssessmentResponse(
                success=False,
                applicationId=request.applicationId,
                error=f"Response parsing failed: {str(parse_error)}",
                processingDetails={
                    "processingTime": time.time() - processing_start,
                    "rawResponsePreview": raw_response[:200] + "..." if len(raw_response) > 200 else raw_response
                }
            )

        # ‚úÖ STEP 7: Save assessment log
        try:
            logger.info(f"üîç [LOAN ASSESSMENT] {assessment_id}: Starting log save process")

            # ‚úÖ CHECK VARIABLES EXISTENCE
            logger.info(f"üîç [LOAN ASSESSMENT] {assessment_id}: Variables check:")
            logger.info(f"   - applicant_name exists: {'applicant_name' in locals()}")
            logger.info(f"   - id_number exists: {'id_number' in locals()}")
            logger.info(f"   - age exists: {'age' in locals()}")
            logger.info(f"   - financial_metrics exists: {'financial_metrics' in locals()}")
            logger.info(f"   - assessment_data exists: {'assessment_data' in locals()}")
            logger.info(f"   - reasoning_duration exists: {'reasoning_duration' in locals()}")
            logger.info(f"   - raw_response exists: {'raw_response' in locals()}")
            applicant_info = {
                "name": applicant_name,  # Already safe from earlier
                "idNumber": id_number if 'id_number' in locals() else "N/A",
                "age": age if 'age' in locals() else "N/A"
            }
            assessment_log = {
                "assessmentId": assessment_id,
                "applicationId": request.applicationId,
                "timestamp": datetime.now().isoformat(),
                "applicant": applicant_info, 
                "loanRequest": {
                    "amount": request.loanAmount,
                    "type": request.loanType,
                    "term": request.loanTerm,
                    "purpose": request.loanPurpose
                },
                "financialMetrics": {
                    "totalMonthlyIncome": total_monthly_income,
                    "currentDebtRatio": current_debt_ratio,
                    "projectedDebtRatio": new_debt_ratio,
                    "loanToValue": loan_to_value,
                    "estimatedMonthlyPayment": estimated_monthly_payment
                },
                "assessmentResult": assessment_data,
                "processingDetails": {
                    "reasoningDuration": reasoning_duration,
                    "totalProcessingTime": time.time() - processing_start,
                    "modelUsed": "deepseek-reasoning"
                },
                "rawResponse": raw_response
            }
            # ‚úÖ CHECK DIRECTORY AND PERMISSIONS
            logger.info(f"üîç [LOAN ASSESSMENT] {assessment_id}: Directory check:")
            logger.info(f"   - RESULTS_DIR: {RESULTS_DIR}")
            logger.info(f"   - Directory exists: {os.path.exists(RESULTS_DIR)}")
            logger.info(f"   - Directory writable: {os.access(RESULTS_DIR, os.W_OK)}")
            # Save to file
            assessment_filename = f"loan_assessment_{assessment_id}_{request.applicationId}.json"
            assessment_filepath = os.path.join(RESULTS_DIR, assessment_filename)
            logger.info(f"üîç [LOAN ASSESSMENT] {assessment_id}: File info:")
            logger.info(f"   - Filename: {assessment_filename}")
            logger.info(f"   - Full path: {assessment_filepath}")

            with open(assessment_filepath, 'w', encoding='utf-8') as f:
                json.dump(assessment_log, f, ensure_ascii=False, indent=2)

            # ‚úÖ VERIFY FILE CREATION
            if os.path.exists(assessment_filepath):
                file_size = os.path.getsize(assessment_filepath)
                logger.info(f"‚úÖ [LOAN ASSESSMENT] {assessment_id}: Assessment log saved successfully")
                logger.info(f"üìÅ [LOAN ASSESSMENT] {assessment_id}: File: {assessment_filename}")
                logger.info(f"üìÅ [LOAN ASSESSMENT] {assessment_id}: Size: {file_size} bytes")
            else:
                logger.error(f"‚ùå [LOAN ASSESSMENT] {assessment_id}: File was not created!")

        except Exception as log_error:
            logger.error(f"‚ö†Ô∏è [LOAN ASSESSMENT] {assessment_id}: Log saving error - {log_error}")
            import traceback
            logger.error(f"‚ö†Ô∏è [LOAN ASSESSMENT] {assessment_id}: Traceback - {traceback.format_exc()}")

        # ‚úÖ STEP 8: Return successful response
        total_time = time.time() - processing_start

        processing_details = {
            "assessmentId": assessment_id,
            "processingTime": round(total_time, 2),
            "reasoningDuration": round(reasoning_duration, 2),
            "modelUsed": "deepseek-reasoning",
            "promptLength": len(assessment_prompt),
            "responseLength": len(raw_response),
            "financialMetrics": {
                "totalMonthlyIncome": total_monthly_income,
                "estimatedMonthlyPayment": int(estimated_monthly_payment),
                "projectedDebtRatio": round(new_debt_ratio, 3),
                "loanToValue": round(loan_to_value, 3)
            }
        }

        logger.info(f"üéâ [LOAN ASSESSMENT] {assessment_id}: Assessment completed successfully in {total_time:.2f}s")

        return LoanAssessmentResponse(
            success=True,
            applicationId=request.applicationId,
            assessmentId=assessment_id,
            status=assessment_data.get("status"),
            confidence=assessment_data.get("confidence"),
            creditScore=assessment_data.get("creditScore"),
            reasoning=assessment_data.get("reasoning"),
            riskFactors=assessment_data.get("riskFactors", []),
            recommendations=assessment_data.get("recommendations", []),
            approvedAmount=assessment_data.get("approvedAmount"),
            interestRate=assessment_data.get("interestRate"),
            monthlyPayment=assessment_data.get("monthlyPayment"),
            loanToValue=assessment_data.get("loanToValue"),
            debtToIncome=assessment_data.get("debtToIncome"),
            conditions=assessment_data.get("conditions", []),
            collateralValuation=assessment_data.get("collateralValuation"),
            financialAnalysis=assessment_data.get("financialAnalysis"),
            processingDetails=processing_details
        )

    except Exception as general_error:
        logger.error(f"‚ùå [LOAN ASSESSMENT] {assessment_id}: General error - {general_error}")
        import traceback
        logger.error(f"üîç [LOAN ASSESSMENT] {assessment_id}: Traceback - {traceback.format_exc()}")

        return LoanAssessmentResponse(
            success=False,
            applicationId=request.applicationId if hasattr(request, 'applicationId') else "unknown",
            error=f"Assessment failed: {str(general_error)}",
            processingDetails={
                "processingTime": time.time() - processing_start
            }
        )


# ===== DOCUMENT INGESTION WORKFLOW =====
# Added for Backend integration workflow

from pydantic import BaseModel
from typing import Optional, Dict, Any, List
import uuid
import redis
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
import httpx
from datetime import datetime
import time

# Document Ingestion Models
class DocumentIngestionRequest(BaseModel):
    task_id: str
    user_id: str
    document_id: str
    r2_path: str
    file_name: str
    content_type: str
    task_type: str = "ingest_document"

class DocumentIngestionResponse(BaseModel):
    success: bool
    task_id: str
    message: str
    processing_details: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class CallbackRequest(BaseModel):
    task_id: str
    user_id: str
    document_id: str
    status: str  # "COMPLETED" | "FAILED" | "PROCESSING"
    error_message: Optional[str] = None
    processing_details: Optional[Dict[str, Any]] = None

# Redis Queue Manager
class DocumentQueueManager:
    def __init__(self):
        redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
        self.redis_client = redis.Redis.from_url(redis_url, decode_responses=True)
        
    async def push_ingestion_task(self, task_data: Dict[str, Any]) -> bool:
        """Push task to ingestion queue"""
        try:
            await asyncio.get_event_loop().run_in_executor(
                None, 
                self.redis_client.lpush, 
                "document_ingestion_queue", 
                json.dumps(task_data)
            )
            return True
        except Exception as e:
            logger.error(f"‚ùå Failed to push task to queue: {e}")
            return False
    
    async def get_ingestion_task(self) -> Optional[Dict[str, Any]]:
        """Get task from ingestion queue"""
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                None, 
                self.redis_client.brpop, 
                "document_ingestion_queue", 
                10  # timeout 10s
            )
            if result:
                return json.loads(result[1])
            return None
        except Exception as e:
            logger.error(f"‚ùå Failed to get task from queue: {e}")
            return None

# R2 Document Processor
class R2DocumentProcessor:
    def __init__(self):
        # R2 Client
        try:
            import boto3
            from botocore.config import Config
            
            self.r2_client = boto3.client(
                's3',
                endpoint_url=os.getenv("R2_ENDPOINT"),
                aws_access_key_id=os.getenv("R2_ACCESS_KEY_ID"),
                aws_secret_access_key=os.getenv("R2_SECRET_ACCESS_KEY"),
                config=Config(signature_version='s3v4'),
                region_name='auto'
            )
            self.bucket_name = os.getenv("R2_BUCKET_NAME", "studynotes")
            
        except ImportError:
            logger.error("‚ùå boto3 not installed. Please install: pip install boto3")
            self.r2_client = None
        
        # Qdrant Client
        try:
            from qdrant_client import QdrantClient
            from qdrant_client.models import VectorParams, Distance, PointStruct
            
            self.qdrant_client = QdrantClient(
                url=os.getenv("QDRANT_URL"),
                api_key=os.getenv("QDRANT_API_KEY")
            )
        except ImportError:
            logger.error("‚ùå qdrant-client not installed")
            self.qdrant_client = None
        
        # Embedding Model
        try:
            from sentence_transformers import SentenceTransformer
            self.embedder = SentenceTransformer("all-MiniLM-L6-v2")
        except ImportError:
            logger.error("‚ùå sentence-transformers not installed")
            self.embedder = None
    
    async def download_from_r2(self, r2_path: str) -> bytes:
        """Download file from R2"""
        if not self.r2_client:
            raise Exception("R2 client not available")
            
        try:
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.r2_client.get_object(Bucket=self.bucket_name, Key=r2_path)
            )
            return response['Body'].read()
        except Exception as e:
            logger.error(f"‚ùå R2 download failed: {e}")
            raise
    
    def chunk_document(self, content: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """Chunk document into smaller pieces"""
        chunks = []
        start = 0
        
        while start < len(content):
            end = start + chunk_size
            chunk = content[start:end]
            
            if end < len(content):
                # Find good breaking point
                last_sentence = chunk.rfind('. ')
                last_exclamation = chunk.rfind('! ')
                last_question = chunk.rfind('? ')
                last_newline = chunk.rfind('\n\n')
                
                break_points = [p for p in [last_sentence, last_exclamation, last_question, last_newline] if p > start + chunk_size // 2]
                
                if break_points:
                    break_point = max(break_points)
                    chunk = content[start:break_point + 2]
                    start = break_point + 2 - overlap
                else:
                    start = end - overlap
            else:
                start = len(content)
                
            cleaned_chunk = chunk.strip()
            if cleaned_chunk and len(cleaned_chunk) > 50:
                chunks.append(cleaned_chunk)
        
        return chunks
    
    async def store_in_qdrant(self, collection_name: str, chunks: List[str], user_id: str, document_id: str, filename: str) -> int:
        """Store chunks in Qdrant"""
        if not self.qdrant_client or not self.embedder:
            raise Exception("Qdrant client or embedder not available")
            
        try:
            from qdrant_client.models import VectorParams, Distance, PointStruct
            
            # Create collection if not exists
            try:
                await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.qdrant_client.create_collection(
                        collection_name=collection_name,
                        vectors_config=VectorParams(size=384, distance=Distance.COSINE)
                    )
                )
                logger.info(f"üìö Created collection: {collection_name}")
            except Exception as e:
                if "already exists" in str(e).lower():
                    logger.info(f"üìö Collection {collection_name} already exists")
                else:
                    raise e
            
            # Generate embeddings and points
            points = []
            for i, chunk in enumerate(chunks):
                # Generate embedding
                embedding = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.embedder.encode(chunk).tolist()
                )
                
                point = PointStruct(
                    id=str(uuid.uuid4()),
                    vector=embedding,
                    payload={
                        "user_id": user_id,
                        "document_id": document_id,
                        "filename": filename,
                        "chunk_index": i,
                        "content": chunk,
                        "word_count": len(chunk.split()),
                        "char_count": len(chunk),
                        "timestamp": datetime.now().isoformat(),
                        "chunk_id": f"{document_id}_chunk_{i:03d}"
                    }
                )
                points.append(point)
            
            # Upsert to Qdrant
            await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.qdrant_client.upsert(collection_name=collection_name, points=points)
            )
            
            logger.info(f"üíæ Stored {len(points)} chunks in Qdrant collection '{collection_name}'")
            return len(points)
            
        except Exception as e:
            logger.error(f"‚ùå Qdrant storage failed: {e}")
            raise

# Initialize components
queue_manager = DocumentQueueManager()
document_processor = R2DocumentProcessor()

async def send_callback_to_backend(task_id: str, user_id: str, document_id: str, status: str, 
                                 processing_details: Dict = None, error_message: str = None):
    """Send callback to backend when processing is complete"""
    try:
        backend_callback_url = os.getenv("BACKEND_CALLBACK_URL", "http://localhost:3000/api/documents/ingestion-callback")
        
        callback_data = {
            "task_id": task_id,
            "user_id": user_id,
            "document_id": document_id,
            "status": status,
            "timestamp": datetime.now().isoformat(),
            "processing_details": processing_details,
            "error_message": error_message
        }
        
        timeout = httpx.Timeout(10.0, connect=5.0)
        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.post(
                backend_callback_url,
                json=callback_data
            )
            
            if response.status_code == 200:
                logger.info(f"‚úÖ [CALLBACK] Sent to backend: {task_id}")
            else:
                logger.warning(f"‚ö†Ô∏è [CALLBACK] Backend returned {response.status_code}")
                
    except Exception as e:
        logger.error(f"‚ùå [CALLBACK] Failed to send callback: {e}")

# Document Worker
class DocumentWorker:
    def __init__(self):
        self.queue_manager = queue_manager
        self.processor = document_processor
        self.running = False
        self.executor = ThreadPoolExecutor(max_workers=2)
    
    async def start(self):
        """Start the document processing worker"""
        self.running = True
        logger.info("üîÑ [WORKER] Document ingestion worker started")
        
        while self.running:
            try:
                # Get task from queue
                task_data = await self.queue_manager.get_ingestion_task()
                
                if task_data:
                    logger.info(f"üì• [WORKER] Processing task: {task_data.get('task_id')}")
                    
                    # Process the task
                    await self.process_document_task(task_data)
                else:
                    # No task available, wait a bit
                    await asyncio.sleep(5)
                    
            except Exception as e:
                logger.error(f"‚ùå [WORKER] Error: {e}")
                await asyncio.sleep(10)  # Wait longer on error
    
    async def process_document_task(self, task_data: Dict[str, Any]):
        """Process a single document ingestion task"""
        task_id = task_data.get('task_id')
        user_id = task_data.get('user_id')
        document_id = task_data.get('document_id')
        r2_path = task_data.get('r2_path')
        filename = task_data.get('file_name')
        content_type = task_data.get('content_type')
        
        processing_start = time.time()
        
        try:
            logger.info(f"üì• [INGESTION] {task_id}: Starting document ingestion")
            logger.info(f"üë§ [INGESTION] {task_id}: User: {user_id}")
            logger.info(f"üìÑ [INGESTION] {task_id}: Document: {document_id}")
            logger.info(f"üìÅ [INGESTION] {task_id}: R2 Path: {r2_path}")
            
            # Step 1: Download file from R2
            logger.info(f"‚¨áÔ∏è [INGESTION] {task_id}: Downloading from R2...")
            file_content = await self.processor.download_from_r2(r2_path)
            logger.info(f"‚úÖ [INGESTION] {task_id}: Downloaded {len(file_content)} bytes")
            
            # Step 2: Extract text based on content type
            logger.info(f"üìù [INGESTION] {task_id}: Extracting text...")
            
            if content_type == "application/pdf":
                import base64
                file_base64 = base64.b64encode(file_content).decode('utf-8')
                extracted_text = extract_text_from_pdf(file_base64)
            elif content_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                import base64
                file_base64 = base64.b64encode(file_content).decode('utf-8')
                extracted_text = extract_text_from_docx(file_base64)
            elif content_type.startswith("text/"):
                extracted_text = file_content.decode('utf-8')
            else:
                raise Exception(f"Unsupported content type: {content_type}")
            
            if not extracted_text or len(extracted_text.strip()) < 10:
                raise Exception("No text content extracted from document")
            
            logger.info(f"‚úÖ [INGESTION] {task_id}: Extracted {len(extracted_text)} characters")
            
            # Step 3: Chunk document
            logger.info(f"üî™ [INGESTION] {task_id}: Chunking document...")
            chunks = self.processor.chunk_document(extracted_text)
            logger.info(f"‚úÖ [INGESTION] {task_id}: Created {len(chunks)} chunks")
            
            # Step 4: Store in Qdrant
            logger.info(f"üíæ [INGESTION] {task_id}: Storing in Qdrant...")
            collection_name = f"user_{user_id}_documents"
            stored_chunks = await self.processor.store_in_qdrant(
                collection_name, chunks, user_id, document_id, filename
            )
            
            # Step 5: Send success callback
            processing_time = time.time() - processing_start
            processing_details = {
                "chunks_created": len(chunks),
                "chunks_stored": stored_chunks,
                "text_length": len(extracted_text),
                "processing_time": processing_time,
                "collection_name": collection_name
            }
            
            await send_callback_to_backend(
                task_id=task_id,
                user_id=user_id,
                document_id=document_id,
                status="COMPLETED",
                processing_details=processing_details
            )
            
            logger.info(f"üéâ [INGESTION] {task_id}: Completed successfully in {processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"‚ùå [INGESTION] {task_id}: Failed - {e}")
            
            # Send failure callback
            await send_callback_to_backend(
                task_id=task_id,
                user_id=user_id,
                document_id=document_id,
                status="FAILED",
                error_message=str(e)
            )
    
    def stop(self):
        """Stop the worker"""
        self.running = False
        logger.info("üõë [WORKER] Document ingestion worker stopped")

# Initialize worker
document_worker = DocumentWorker()

# Start the worker in a separate thread
@app.on_event("startup")
async def startup_event():
    # Start the document worker
    loop = asyncio.get_event_loop()
    loop.create_task(document_worker.start())
    
    # Run the scheduler in the background
    loop.run_in_executor(None, run_scheduler)

@app.on_event("shutdown")
async def shutdown_event():
    # Stop the document worker
    document_worker.stop()
    logger.info("AI Service is shutting down...")
