#!/usr/bin/env python3
"""
Test script Ä‘á»ƒ kiá»ƒm tra káº¿t ná»‘i R2 Storage
"""

import os
import asyncio
import logging
from datetime import datetime
from pathlib import Path

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables
try:
    from dotenv import load_dotenv
    load_dotenv('development.env')
    logger.info("âœ… Loaded development.env")
except ImportError:
    logger.warning("âš ï¸ python-dotenv not installed")

def check_r2_config():
    """Kiá»ƒm tra R2 configuration"""
    print("ğŸ” Checking R2 configuration...")
    
    required_vars = [
        'R2_ACCOUNT_ID',
        'R2_ACCESS_KEY_ID', 
        'R2_SECRET_ACCESS_KEY',
        'R2_BUCKET_NAME',
        'R2_ENDPOINT'
    ]
    
    config = {}
    missing_vars = []
    
    for var in required_vars:
        value = os.getenv(var)
        if value:
            config[var] = value
            if 'SECRET' in var:
                print(f"âœ… {var}: {value[:20]}...")
            else:
                print(f"âœ… {var}: {value}")
        else:
            missing_vars.append(var)
            print(f"âŒ {var}: NOT SET")
    
    if missing_vars:
        print(f"\nâš ï¸ Missing environment variables: {missing_vars}")
        return None
    
    print("\nâœ… All R2 environment variables are set!")
    return config

async def test_basic_r2_connection():
    """Test basic R2 connection using boto3"""
    print("\nğŸ” Testing basic R2 connection...")
    
    try:
        import boto3
        from botocore.exceptions import ClientError, NoCredentialsError
        from botocore.config import Config
        
        # Get R2 config
        endpoint = os.getenv('R2_ENDPOINT')
        access_key = os.getenv('R2_ACCESS_KEY_ID')
        secret_key = os.getenv('R2_SECRET_ACCESS_KEY')
        bucket_name = os.getenv('R2_BUCKET_NAME')
        
        print(f"ğŸ“ Endpoint: {endpoint}")
        print(f"ğŸª£ Bucket: {bucket_name}")
        print(f"ğŸ”‘ Access Key: {access_key[:20]}...")
        
        # Create S3 client for R2
        s3_client = boto3.client(
            's3',
            endpoint_url=endpoint,
            aws_access_key_id=access_key,
            aws_secret_access_key=secret_key,
            config=Config(
                signature_version='s3v4',
                s3={'addressing_style': 'path'}
            ),
            region_name='auto'
        )
        
        print("â³ Testing connection...")
        
        # Test 1: List buckets
        try:
            response = s3_client.list_buckets()
            buckets = [bucket['Name'] for bucket in response.get('Buckets', [])]
            print(f"âœ… Connection successful! Found buckets: {buckets}")
            
            if bucket_name in buckets:
                print(f"âœ… Target bucket '{bucket_name}' exists")
            else:
                print(f"âš ï¸ Target bucket '{bucket_name}' not found in: {buckets}")
                
        except Exception as e:
            print(f"âŒ List buckets failed: {e}")
            return False
        
        # Test 2: Check bucket access
        try:
            response = s3_client.head_bucket(Bucket=bucket_name)
            print(f"âœ… Bucket '{bucket_name}' is accessible")
        except ClientError as e:
            error_code = e.response['Error']['Code']
            if error_code == '404':
                print(f"âŒ Bucket '{bucket_name}' not found")
            elif error_code == '403':
                print(f"âŒ Access denied to bucket '{bucket_name}'")
            else:
                print(f"âŒ Bucket access error: {e}")
            return False
        
        # Test 3: List objects in bucket
        try:
            response = s3_client.list_objects_v2(Bucket=bucket_name, MaxKeys=10)
            object_count = response.get('KeyCount', 0)
            print(f"âœ… Bucket contains {object_count} objects")
            
            if object_count > 0:
                print("ğŸ“„ Sample objects:")
                for obj in response.get('Contents', [])[:5]:
                    print(f"   - {obj['Key']} ({obj['Size']} bytes)")
            
        except Exception as e:
            print(f"âš ï¸ List objects failed: {e}")
        
        return True
        
    except ImportError:
        print("âŒ boto3 not installed. Please run: pip install boto3")
        return False
    except Exception as e:
        print(f"âŒ Connection test failed: {e}")
        return False

async def test_file_operations():
    """Test file upload/download operations"""
    print("\nğŸ” Testing file operations...")
    
    try:
        import boto3
        from botocore.config import Config
        
        # Initialize client
        s3_client = boto3.client(
            's3',
            endpoint_url=os.getenv('R2_ENDPOINT'),
            aws_access_key_id=os.getenv('R2_ACCESS_KEY_ID'),
            aws_secret_access_key=os.getenv('R2_SECRET_ACCESS_KEY'),
            config=Config(
                signature_version='s3v4',
                s3={'addressing_style': 'path'}
            ),
            region_name='auto'
        )
        
        bucket_name = os.getenv('R2_BUCKET_NAME')
        
        # Test upload
        print("ğŸ“¤ Testing file upload...")
        test_content = f"""# R2 Storage Test

This is a test file uploaded to R2 at {datetime.now().isoformat()}.

## Test Information
- Bucket: {bucket_name}
- Endpoint: {os.getenv('R2_ENDPOINT')}
- Upload time: {datetime.now()}

## Content
This file is used to test:
1. File upload to R2
2. File download from R2  
3. File listing
4. File deletion

Generated by AI Chatbot RAG system.
""".encode('utf-8')
        
        test_key = f"test/upload_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        try:
            s3_client.put_object(
                Bucket=bucket_name,
                Key=test_key,
                Body=test_content,
                ContentType='text/markdown'
            )
            print(f"âœ… Upload successful: {test_key}")
            
            # Generate public URL
            public_url = f"{os.getenv('R2_ENDPOINT')}/{bucket_name}/{test_key}"
            print(f"ğŸ”— Public URL: {public_url}")
            
        except Exception as e:
            print(f"âŒ Upload failed: {e}")
            return False
        
        # Test download
        print("ğŸ“¥ Testing file download...")
        try:
            response = s3_client.get_object(Bucket=bucket_name, Key=test_key)
            downloaded_content = response['Body'].read()
            downloaded_text = downloaded_content.decode('utf-8')
            
            print(f"âœ… Download successful ({len(downloaded_content)} bytes)")
            print("ğŸ“„ Content preview:")
            print(downloaded_text[:200] + "..." if len(downloaded_text) > 200 else downloaded_text)
            
        except Exception as e:
            print(f"âŒ Download failed: {e}")
            return False
        
        # Test presigned URL
        print("ğŸ”— Testing presigned URL generation...")
        try:
            presigned_url = s3_client.generate_presigned_url(
                'get_object',
                Params={'Bucket': bucket_name, 'Key': test_key},
                ExpiresIn=3600
            )
            print(f"âœ… Presigned URL generated (expires in 1 hour)")
            print(f"   URL: {presigned_url[:80]}...")
            
        except Exception as e:
            print(f"âŒ Presigned URL generation failed: {e}")
        
        # Test upload presigned URL
        print("ğŸ“¤ Testing upload presigned URL...")
        try:
            upload_key = f"test/presigned_upload_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            upload_url = s3_client.generate_presigned_url(
                'put_object',
                Params={
                    'Bucket': bucket_name, 
                    'Key': upload_key,
                    'ContentType': 'text/plain'
                },
                ExpiresIn=3600
            )
            print(f"âœ… Upload presigned URL generated")
            print(f"   URL: {upload_url[:80]}...")
            
        except Exception as e:
            print(f"âŒ Upload presigned URL generation failed: {e}")
        
        # Test file listing with prefix
        print("ğŸ“ Testing file listing...")
        try:
            response = s3_client.list_objects_v2(
                Bucket=bucket_name,
                Prefix="test/",
                MaxKeys=10
            )
            
            test_files = response.get('Contents', [])
            print(f"âœ… Found {len(test_files)} test files:")
            for file in test_files:
                print(f"   ğŸ“„ {file['Key']} ({file['Size']} bytes, {file['LastModified']})")
                
        except Exception as e:
            print(f"âŒ File listing failed: {e}")
        
        # Cleanup - delete test file
        print("ğŸ§¹ Cleaning up test file...")
        try:
            s3_client.delete_object(Bucket=bucket_name, Key=test_key)
            print(f"âœ… Test file deleted: {test_key}")
        except Exception as e:
            print(f"âš ï¸ Cleanup failed: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ File operations test failed: {e}")
        return False

async def test_document_workflow():
    """Test typical document upload workflow"""
    print("\nğŸ” Testing document workflow...")
    
    try:
        import boto3
        from botocore.config import Config
        
        # Initialize client
        s3_client = boto3.client(
            's3',
            endpoint_url=os.getenv('R2_ENDPOINT'),
            aws_access_key_id=os.getenv('R2_ACCESS_KEY_ID'),
            aws_secret_access_key=os.getenv('R2_SECRET_ACCESS_KEY'),
            config=Config(
                signature_version='s3v4',
                s3={'addressing_style': 'path'}
            ),
            region_name='auto'
        )
        
        bucket_name = os.getenv('R2_BUCKET_NAME')
        
        # Simulate document upload workflow
        user_id = "user_123"
        document_id = f"doc_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Create document structure
        documents = [
            {
                'filename': 'research_paper.pdf',
                'content': b'%PDF-1.4\nFake PDF content for testing...' * 100,
                'content_type': 'application/pdf'
            },
            {
                'filename': 'notes.txt', 
                'content': 'This is a text document with important notes.\n\nKey points:\n- Point 1\n- Point 2\n- Point 3'.encode('utf-8'),
                'content_type': 'text/plain'
            },
            {
                'filename': 'presentation.pptx',
                'content': b'PK\x03\x04' + b'Fake PowerPoint content...' * 50,
                'content_type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'
            }
        ]
        
        uploaded_files = []
        
        for doc in documents:
            # Create organized file path
            file_key = f"users/{user_id}/documents/{document_id}/{doc['filename']}"
            
            try:
                s3_client.put_object(
                    Bucket=bucket_name,
                    Key=file_key,
                    Body=doc['content'],
                    ContentType=doc['content_type'],
                    Metadata={
                        'user_id': user_id,
                        'document_id': document_id,
                        'upload_timestamp': datetime.now().isoformat(),
                        'original_filename': doc['filename']
                    }
                )
                
                uploaded_files.append(file_key)
                print(f"âœ… Uploaded: {doc['filename']} -> {file_key}")
                
            except Exception as e:
                print(f"âŒ Upload failed for {doc['filename']}: {e}")
        
        # List user documents
        print(f"\nğŸ“ Listing documents for user {user_id}...")
        try:
            response = s3_client.list_objects_v2(
                Bucket=bucket_name,
                Prefix=f"users/{user_id}/documents/",
                MaxKeys=50
            )
            
            user_files = response.get('Contents', [])
            print(f"âœ… User has {len(user_files)} files:")
            
            for file in user_files:
                # Get metadata
                try:
                    obj_response = s3_client.head_object(Bucket=bucket_name, Key=file['Key'])
                    metadata = obj_response.get('Metadata', {})
                    print(f"   ğŸ“„ {file['Key']} ({file['Size']} bytes)")
                    if metadata:
                        print(f"      ğŸ“Š Metadata: {metadata}")
                except Exception:
                    print(f"   ğŸ“„ {file['Key']} ({file['Size']} bytes)")
                    
        except Exception as e:
            print(f"âŒ List user documents failed: {e}")
        
        # Test download specific document
        if uploaded_files:
            test_file = uploaded_files[0]
            print(f"\nğŸ“¥ Testing download of: {test_file}")
            try:
                response = s3_client.get_object(Bucket=bucket_name, Key=test_file)
                content = response['Body'].read()
                metadata = response.get('Metadata', {})
                
                print(f"âœ… Download successful ({len(content)} bytes)")
                print(f"ğŸ“Š Metadata: {metadata}")
                
            except Exception as e:
                print(f"âŒ Download failed: {e}")
        
        # Cleanup
        print("\nğŸ§¹ Cleaning up test documents...")
        for file_key in uploaded_files:
            try:
                s3_client.delete_object(Bucket=bucket_name, Key=file_key)
                print(f"âœ… Deleted: {file_key}")
            except Exception as e:
                print(f"âš ï¸ Delete failed for {file_key}: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Document workflow test failed: {e}")
        return False

async def main():
    """Main test function"""
    print("ğŸš€ R2 Storage Connection Test")
    print("=" * 60)
    
    # Check configuration
    config = check_r2_config()
    if not config:
        print("\nâŒ Configuration check failed!")
        return
    
    # Test basic connection
    print("\n" + "="*60)
    connection_ok = await test_basic_r2_connection()
    if not connection_ok:
        print("\nâŒ Basic connection test failed!")
        return
    
    # Test file operations
    print("\n" + "="*60)
    file_ops_ok = await test_file_operations()
    if not file_ops_ok:
        print("\nâŒ File operations test failed!")
        return
    
    # Test document workflow
    print("\n" + "="*60)
    workflow_ok = await test_document_workflow()
    if not workflow_ok:
        print("\nâŒ Document workflow test failed!")
        return
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ALL R2 TESTS PASSED!")
    print("âœ… Configuration: OK")
    print("âœ… Connection: OK")
    print("âœ… File Upload: OK")
    print("âœ… File Download: OK")
    print("âœ… File Listing: OK")
    print("âœ… Presigned URLs: OK")
    print("âœ… Document Workflow: OK")
    print("\nğŸš€ R2 Storage is ready for production use!")

if __name__ == "__main__":
    asyncio.run(main())
