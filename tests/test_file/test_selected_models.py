#!/usr/bin/env python3
"""
Test Specific AI Models - Test v·ªõi model c·ª• th·ªÉ theo y√™u c·∫ßu
"""

import os
import asyncio
import sys

sys.path.append("src")

# Load environment variables
with open(".env", "r") as f:
    for line in f:
        if line.strip() and not line.startswith("#") and "=" in line:
            key, value = line.strip().split("=", 1)
            value = value.strip("\"'")
            os.environ[key] = value

# Model configuration theo y√™u c·∫ßu
SELECTED_MODELS = {
    "openai": [
        {
            "id": "chatgpt_4o_latest",
            "name": "ChatGPT-4o Latest",
            "model": "chatgpt-4o-latest",
            "category": "latest",
        },
        {
            "id": "gpt_4o_realtime",
            "name": "GPT-4o Realtime Preview",
            "model": "gpt-4o-realtime-preview-2025-06-03",
            "category": "realtime",
        },
        {
            "id": "gpt_4o_search",
            "name": "GPT-4o Search Preview",
            "model": "gpt-4o-search-preview",
            "category": "search",
        },
    ],
    "gemini": [
        # {
        #     "id": "gemini_flash_image",
        #     "name": "Gemini 2.5 Flash Image Preview",
        #     "model": "gemini-2.5-flash-image-preview",
        #     "category": "image"
        # },
        {
            "id": "gemini_flash",
            "name": "Gemini 2.5 Flash",
            "model": "gemini-2.5-flash",
            "category": "fast",
        },
        {
            "id": "gemini_pro",
            "name": "Gemini 2.5 Pro",
            "model": "gemini-2.5-pro",
            "category": "advanced",
        },
    ],
    "cerebras": [
        {
            "id": "qwen_235b_instruct",
            "name": "Qwen 235B Instruct",
            "model": "qwen-3-235b-a22b-instruct-2507",
            "category": "instruct",
        },
        {
            "id": "qwen_235b_thinking",
            "name": "Qwen 235B Thinking",
            "model": "qwen-3-235b-a22b-thinking-2507",
            "category": "thinking",
        },
        {
            "id": "qwen_480b_coder",
            "name": "Qwen 480B Coder",
            "model": "qwen-3-coder-480b",
            "category": "coding",
        },
        {
            "id": "qwen_32b",
            "name": "Qwen 32B",
            "model": "qwen-3-32b",
            "category": "general",
        },
        {
            "id": "llama_70b",
            "name": "Llama 3.3 70B",
            "model": "llama-3.3-70b",
            "category": "general",
        },
        {
            "id": "llama_8b",
            "name": "Llama 3.1 8B",
            "model": "llama3.1-8b",
            "category": "lightweight",
        },
    ],
    "deepseek": [
        {
            "id": "deepseek_chat",
            "name": "DeepSeek Chat",
            "model": "deepseek-chat",
            "category": "general",
        },
        {
            "id": "deepseek_reasoner",
            "name": "DeepSeek Reasoner",
            "model": "deepseek-reasoner",
            "category": "reasoning",
        },
    ],
}

# C√¢u h·ªèi test v·ªÅ V≈©ng T√†u
TEST_QUESTION = """B·∫°n c√≥ th·ªÉ gi·ªõi thi·ªáu cho t√¥i 5 ƒë·ªãa ƒëi·ªÉm du l·ªãch n·ªïi ti·∫øng ·ªü V≈©ng T√†u kh√¥ng?
H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, m·ªói ƒë·ªãa ƒëi·ªÉm ch·ªâ c·∫ßn 1-2 c√¢u m√¥ t·∫£."""


async def test_openai_models():
    """Test OpenAI models"""
    print("üîç Testing OpenAI Models...")

    try:
        import openai

        client = openai.AsyncOpenAI(api_key=os.getenv("CHATGPT_API_KEY"))

        results = []
        for model_info in SELECTED_MODELS["openai"]:
            try:
                print(f"  üìù Testing {model_info['name']} ({model_info['model']})...")

                response = await client.chat.completions.create(
                    model=model_info["model"],
                    messages=[{"role": "user", "content": TEST_QUESTION}],
                    max_tokens=300,
                    temperature=0.7,
                )

                answer = response.choices[0].message.content.strip()
                print(f"    ‚úÖ Success: {answer[:100]}...")
                results.append(
                    {
                        "provider": "openai",
                        "model": model_info,
                        "status": "success",
                        "answer": answer,
                    }
                )

            except Exception as e:
                print(f"    ‚ùå Failed: {e}")
                results.append(
                    {
                        "provider": "openai",
                        "model": model_info,
                        "status": "failed",
                        "error": str(e),
                    }
                )

        return results

    except Exception as e:
        print(f"  ‚ùå OpenAI setup error: {e}")
        return []


async def test_deepseek_models():
    """Test DeepSeek models"""
    print("\nüîç Testing DeepSeek Models...")

    try:
        import openai

        client = openai.AsyncOpenAI(
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url="https://api.deepseek.com/v1",
        )

        results = []
        for model_info in SELECTED_MODELS["deepseek"]:
            try:
                print(f"  üìù Testing {model_info['name']} ({model_info['model']})...")

                response = await client.chat.completions.create(
                    model=model_info["model"],
                    messages=[{"role": "user", "content": TEST_QUESTION}],
                    max_tokens=300,
                    temperature=0.7,
                )

                answer = response.choices[0].message.content.strip()
                print(f"    ‚úÖ Success: {answer[:100]}...")
                results.append(
                    {
                        "provider": "deepseek",
                        "model": model_info,
                        "status": "success",
                        "answer": answer,
                    }
                )

            except Exception as e:
                print(f"    ‚ùå Failed: {e}")
                results.append(
                    {
                        "provider": "deepseek",
                        "model": model_info,
                        "status": "failed",
                        "error": str(e),
                    }
                )

        return results

    except Exception as e:
        print(f"  ‚ùå DeepSeek setup error: {e}")
        return []


async def test_cerebras_models():
    """Test Cerebras models"""
    print("\nüîç Testing Cerebras Models...")

    try:
        from cerebras.cloud.sdk import Cerebras

        client = Cerebras(api_key=os.getenv("CEREBRAS_API_KEY"))

        results = []
        for model_info in SELECTED_MODELS["cerebras"]:
            try:
                print(f"  üìù Testing {model_info['name']} ({model_info['model']})...")

                response = client.chat.completions.create(
                    model=model_info["model"],
                    messages=[{"role": "user", "content": TEST_QUESTION}],
                    max_tokens=300,
                    temperature=0.7,
                )

                answer = response.choices[0].message.content.strip()
                print(f"    ‚úÖ Success: {answer[:100]}...")
                results.append(
                    {
                        "provider": "cerebras",
                        "model": model_info,
                        "status": "success",
                        "answer": answer,
                    }
                )

            except Exception as e:
                print(f"    ‚ùå Failed: {e}")
                results.append(
                    {
                        "provider": "cerebras",
                        "model": model_info,
                        "status": "failed",
                        "error": str(e),
                    }
                )

        return results

    except Exception as e:
        print(f"  ‚ùå Cerebras setup error: {e}")
        return []


async def test_gemini_models():
    """Test Gemini models"""
    print("\nüîç Testing Gemini Models...")

    try:
        import google.generativeai as genai

        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

        results = []
        for model_info in SELECTED_MODELS["gemini"]:
            try:
                print(f"  üìù Testing {model_info['name']} ({model_info['model']})...")

                model = genai.GenerativeModel(model_info["model"])
                response = model.generate_content(
                    TEST_QUESTION,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.7,
                        max_output_tokens=2000,
                    ),
                )

                # X·ª≠ l√Ω response ph·ª©c t·∫°p t·ª´ Gemini v·ªõi try-catch ri√™ng bi·ªát
                answer = None

                # Th·ª≠ c√°ch 1: text tr·ª±c ti·∫øp
                try:
                    answer = response.text.strip()
                    print(f"    ‚úÖ Method 1 (text): Success")
                except Exception as e1:
                    print(f"    ‚ùå Method 1 (text): {e1}")

                    # Th·ª≠ c√°ch 2: t·ª´ parts
                    try:
                        if len(response.parts) > 0:
                            answer = "".join(
                                part.text for part in response.parts
                            ).strip()
                            print(
                                f"    ‚úÖ Method 2 (parts): Success - Length: {len(answer)}"
                            )
                        else:
                            print(f"    ‚ö†Ô∏è Method 2: parts is empty, trying method 3")
                            raise Exception("Empty parts, try method 3")

                    except Exception as e2:
                        print(f"    ‚ùå Method 2 (parts): {e2}")

                        # Th·ª≠ c√°ch 3: t·ª´ candidates
                        try:
                            parts = response.candidates[0].content.parts
                            if len(parts) > 0:
                                answer = "".join(part.text for part in parts).strip()
                                print(
                                    f"    ‚úÖ Method 3 (candidates): Success - Length: {len(answer)}"
                                )
                            else:
                                print(f"    ‚ö†Ô∏è Method 3: candidate parts also empty")
                                print(
                                    f"    üîç Debug: finish_reason = {response.candidates[0].finish_reason}"
                                )
                                answer = (
                                    "Empty response from Gemini (finish_reason issue)"
                                )
                        except Exception as e3:
                            print(f"    ‚ùå Method 3 (candidates): {e3}")
                            answer = "Failed to parse Gemini response"
                            answer = str(response)
                            print(f"    ‚ö†Ô∏è Using str(response)")

                if answer and answer.strip():
                    print(f"    ‚úÖ Final Success: {len(answer)} characters")
                else:
                    print(f"    ‚ö†Ô∏è Empty response detected, using fallback")
                    answer = "No response content found"
                results.append(
                    {
                        "provider": "gemini",
                        "model": model_info,
                        "status": "success",
                        "answer": answer,
                    }
                )

            except Exception as e:
                print(f"    ‚ùå Failed: {e}")
                results.append(
                    {
                        "provider": "gemini",
                        "model": model_info,
                        "status": "failed",
                        "error": str(e),
                    }
                )

        return results

    except Exception as e:
        print(f"  ‚ùå Gemini setup error: {e}")
        return []


async def main():
    """Main function"""
    print("üöÄ Testing Selected AI Models with V≈©ng T√†u Tourism Question")
    print("=" * 80)
    print(f"Question: {TEST_QUESTION}")
    print("=" * 80)

    # Test all providers
    all_results = []

    # openai_results = await test_openai_models()
    # all_results.extend(openai_results)

    # deepseek_results = await test_deepseek_models()
    # all_results.extend(deepseek_results)

    # cerebras_results = await test_cerebras_models()
    # all_results.extend(cerebras_results)

    gemini_results = await test_gemini_models()
    all_results.extend(gemini_results)

    # Summary
    print("\n" + "=" * 80)
    print("üìä SUMMARY")
    print("=" * 80)

    successful_models = [r for r in all_results if r["status"] == "success"]
    failed_models = [r for r in all_results if r["status"] == "failed"]

    print(f"‚úÖ Successful models: {len(successful_models)}")
    print(f"‚ùå Failed models: {len(failed_models)}")

    if successful_models:
        print("\nüéØ WORKING MODELS FOR FRONTEND:")
        print("-" * 60)
        for result in successful_models:
            model_info = result["model"]
            print(f"Provider: {result['provider'].upper()}")
            print(f"  ID: {model_info['id']}")
            print(f"  Name: {model_info['name']}")
            print(f"  Model: {model_info['model']}")
            print(f"  Category: {model_info['category']}")
            print(f"  Answer preview: {result['answer'][:150]}...")
            print()

    if failed_models:
        print("\n‚ùå FAILED MODELS:")
        print("-" * 60)
        for result in failed_models:
            model_info = result["model"]
            print(f"Provider: {result['provider'].upper()}")
            print(f"  Model: {model_info['name']} ({model_info['model']})")
            print(f"  Error: {result['error']}")
            print()

    # Generate final model list for documentation
    print("\nüîß FINAL MODEL LIST FOR API DOCUMENTATION:")
    print("=" * 80)
    provider_groups = {}
    for result in successful_models:
        provider = result["provider"]
        if provider not in provider_groups:
            provider_groups[provider] = []
        provider_groups[provider].append(result["model"])

    for provider, models in provider_groups.items():
        print(f"\n{provider.upper()}:")
        for model in models:
            print(f"  - {model['id']}: {model['name']} ({model['model']})")


if __name__ == "__main__":
    asyncio.run(main())
