#!/usr/bin/env python3
"""
Test Enhanced Token Analysis Implementation
"""
import asyncio
import json
import pytest
from unittest.mock import AsyncMock, patch
from src.services.unified_chat_service import UnifiedChatService
from src.models.unified_models import ChatRequest, ChatIntent, UserSource


@pytest.mark.asyncio
async def test_enhanced_token_analysis_structure():
    """Test that token analysis has proper structure with input/output/total breakdown"""

    # Mock request with longer content for better token analysis
    request = ChatRequest(
        message="Cho t√¥i bi·∫øt v·ªÅ c√°c g√≥i l√£i su·∫•t vay mua nh√† hi·ªán t·∫°i c·ªßa ng√¢n h√†ng? T√¥i mu·ªën vay 2 t·ª∑ trong 20 nƒÉm.",
        company_id="comp_test",
        conversation_id="conv_test_tokens",
        session_id="sess_test_tokens",
        user_id="user_test_tokens",
        source=UserSource.CHATDEMO,
        message_id="msg_test_tokens",
    )

    # Mock AI response with thinking
    mock_ai_response = """```json
{
    "thinking": {
        "intent": "ASK_COMPANY_INFORMATION",
        "persona": "Chuy√™n vi√™n t∆∞ v·∫•n t√†i ch√≠nh",
        "reasoning": "Kh√°ch h√†ng h·ªèi v·ªÅ l√£i su·∫•t vay mua nh√† v·ªõi s·ªë ti·ªÅn c·ª• th·ªÉ 2 t·ª∑ trong 20 nƒÉm. ƒê√¢y l√† intent ASK_COMPANY_INFORMATION v√¨ h·ªç c·∫ßn th√¥ng tin ch√≠nh th·ª©c v·ªÅ s·∫£n ph·∫©m vay c·ªßa ng√¢n h√†ng."
    },
    "final_answer": "C·∫£m ∆°n b·∫°n ƒë√£ quan t√¢m ƒë·∫øn d·ªãch v·ª• vay mua nh√† c·ªßa ch√∫ng t√¥i! Hi·ªán t·∫°i ng√¢n h√†ng ƒëang c√≥ c√°c g√≥i l√£i su·∫•t vay mua nh√† r·∫•t h·∫•p d·∫´n:\n\nüè† **G√ìI VAY MUA NH√Ä ∆ØU ƒê√ÉI 2025**\n- L√£i su·∫•t c·ªë ƒë·ªãnh 12 th√°ng ƒë·∫ßu: 6.5%/nƒÉm\n- L√£i su·∫•t t·ª´ th√°ng 13 tr·ªü ƒëi: 8.2%/nƒÉm\n- Th·ªùi h·∫°n vay: T·ªëi ƒëa 25 nƒÉm\n- M·ª©c vay: L√™n ƒë·∫øn 85% gi√° tr·ªã t√†i s·∫£n\n\nV·ªõi kho·∫£n vay 2 t·ª∑ trong 20 nƒÉm:\nüí∞ S·ªë ti·ªÅn tr·∫£ h√†ng th√°ng d·ª± ki·∫øn: ~18-20 tri·ªáu\nüìã H·ªì s∆° c·∫ßn thi·∫øt: CMND/CCCD, s·ªï h·ªô kh·∫©u, gi·∫•y t·ªù thu nh·∫≠p\n\nB·∫°n c√≥ mu·ªën t√¥i t∆∞ v·∫•n th√™m v·ªÅ th·ªß t·ª•c ho·∫∑c l·ªãch h·∫πn g·∫∑p chuy√™n vi√™n kh√¥ng?"
}
```"""

    with patch(
        "src.services.unified_chat_service.UnifiedChatService._get_ai_response_with_retry"
    ) as mock_ai, patch(
        "src.services.unified_chat_service.UnifiedChatService._send_webhook_if_needed"
    ) as mock_webhook:

        mock_ai.return_value = mock_ai_response
        mock_webhook.return_value = True

        chat_service = UnifiedChatService()

        response = await chat_service.process_chat_request(request)

        # Verify response structure
        assert response.status == "success"
        assert "lastAiResponse" in response.data

        last_ai_response = response.data["lastAiResponse"]
        metadata = last_ai_response["metadata"]

        print("\nüîç Enhanced Token Analysis Structure:")
        print(f"Metadata keys: {list(metadata.keys())}")

        # Verify enhanced token structure
        assert "tokens" in metadata
        tokens = metadata["tokens"]

        print(f"Token structure: {tokens}")

        # Check if it's enhanced structure (dict) or legacy (int)
        if isinstance(tokens, dict):
            print("‚úÖ Enhanced token structure detected!")
            assert "input" in tokens
            assert "output" in tokens
            assert "total" in tokens
            assert tokens["total"] == tokens["input"] + tokens["output"]

            # Verify character count
            assert "characterCount" in metadata
            char_count = metadata["characterCount"]
            assert "input" in char_count
            assert "output" in char_count
            assert "total" in char_count
            assert char_count["total"] == char_count["input"] + char_count["output"]

            print(f"‚úÖ Input tokens: {tokens['input']}")
            print(f"‚úÖ Output tokens: {tokens['output']}")
            print(f"‚úÖ Total tokens: {tokens['total']}")
            print(f"‚úÖ Input chars: {char_count['input']}")
            print(f"‚úÖ Output chars: {char_count['output']}")
            print(f"‚úÖ Total chars: {char_count['total']}")

        else:
            print("‚ö†Ô∏è Legacy token structure detected - will be converted in webhook")
            assert isinstance(tokens, int)

        # Verify thinking data is included
        assert "thinking" in metadata
        assert metadata["thinking"] is not None

        # Verify intent extraction
        assert (
            metadata["intent"] == "information"
        )  # ASK_COMPANY_INFORMATION -> information

        print("‚úÖ All token analysis checks passed!")


@pytest.mark.asyncio
async def test_webhook_token_analysis_conversion():
    """Test webhook converts legacy token format to enhanced format"""

    from src.services.webhook_service import WebhookService

    # Mock legacy lastAiResponse with simple token count
    legacy_ai_response = {
        "content": "Test AI response content",
        "timestamp": "2025-01-26T10:30:00.000Z",
        "messageId": "msg_test",
        "metadata": {
            "intent": "information",
            "tokens": 150,  # Legacy simple count
            "thinking": "Test thinking content",
        },
    }

    webhook_service = WebhookService()

    with patch.object(webhook_service, "send_conversation_event") as mock_send:
        mock_send.return_value = True

        result = await webhook_service.notify_conversation_updated(
            company_id="comp_test",
            conversation_id="conv_test",
            status="ACTIVE",
            message_count=1,
            last_ai_response=legacy_ai_response,
            channel="chatdemo",
            intent="information",
        )

        # Verify webhook was called
        assert result is True
        assert mock_send.called

        # Get the data sent to webhook
        call_args = mock_send.call_args
        webhook_data = call_args[0][2]  # Third argument is the data

        print("\nüîç Webhook Data Structure:")
        print(f"Webhook data keys: {list(webhook_data.keys())}")

        if "lastAiResponse" in webhook_data:
            ai_response = webhook_data["lastAiResponse"]
            tokens = ai_response["metadata"]["tokens"]

            print(f"Webhook tokens structure: {tokens}")

            # Should be converted to enhanced structure
            if isinstance(tokens, dict):
                print("‚úÖ Webhook converted legacy tokens to enhanced structure!")
                assert "input" in tokens
                assert "output" in tokens
                assert "total" in tokens
            else:
                print("‚ö†Ô∏è Webhook kept legacy token structure")

        print("‚úÖ Webhook token conversion test passed!")


if __name__ == "__main__":
    print("üß™ Testing Enhanced Token Analysis Implementation...")

    # Run the tests
    asyncio.run(test_enhanced_token_analysis_structure())
    asyncio.run(test_webhook_token_analysis_conversion())

    print("\n‚úÖ All enhanced token analysis tests completed!")
